This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/
  settings.local.json
.opencode/
  command/
    tm_review.md
    tm_update.md
.taskmaster/
  docs/
    prd-init.md
    prd-part2.md
  reports/
    task-complexity-report.json
  tasks/
    tasks.json
  config.json
  state.json
.zed/
  settings.json
backend/
  DATA/
    scenes.db
  database.py
  genesis_renderer.py
  llm_interpreter.py
  main.py
  requirements.txt
  scene_converter.py
src/
  Main.elm
  PhysicsRenderer.js
  Route.elm
  Video.elm
  VideoGallery.elm
.dockerignore
.gitignore
deploy.sh
DEPLOYMENT.md
docker-compose.yml
Dockerfile
elm.json
fly.toml
FLYIO_DEPLOYMENT.md
GENESIS_USAGE.md
index.html
package.json
README.md
SETUP_SUMMARY.md
test_scene.json
vite.config.js
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "WebFetch(domain:genesis-world.readthedocs.io)",
      "WebSearch",
      "WebFetch(domain:github.com)"
    ],
    "deny": [],
    "ask": []
  }
}
</file>

<file path="backend/database.py">
"""Database models and operations for storing generated scenes."""
import sqlite3
import json
import os
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any
from contextlib import contextmanager

# Get data directory from environment variable, default to ./DATA
DATA_DIR = Path(os.getenv("DATA", "./DATA"))
DATA_DIR.mkdir(exist_ok=True)

DB_PATH = DATA_DIR / "scenes.db"

def init_db():
    """Initialize the database with required tables."""
    with get_db() as conn:
        conn.execute("""
            CREATE TABLE IF NOT EXISTS generated_scenes (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                prompt TEXT NOT NULL,
                scene_data TEXT NOT NULL,
                model TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                metadata TEXT
            )
        """)

        conn.execute("""
            CREATE TABLE IF NOT EXISTS generated_videos (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                prompt TEXT NOT NULL,
                video_url TEXT NOT NULL,
                model_id TEXT NOT NULL,
                parameters TEXT NOT NULL,
                status TEXT DEFAULT 'completed',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                collection TEXT,
                metadata TEXT
            )
        """)

        conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_created_at
            ON generated_scenes(created_at DESC)
        """)

        conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_model
            ON generated_scenes(model)
        """)

        conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_videos_created_at
            ON generated_videos(created_at DESC)
        """)

        conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_videos_model
            ON generated_videos(model_id)
        """)

        conn.commit()

@contextmanager
def get_db():
    """Context manager for database connections."""
    conn = sqlite3.connect(str(DB_PATH))
    conn.row_factory = sqlite3.Row
    try:
        yield conn
    finally:
        conn.close()

def save_generated_scene(
    prompt: str,
    scene_data: dict,
    model: str,
    metadata: Optional[dict] = None
) -> int:
    """Save a generated scene to the database."""
    with get_db() as conn:
        cursor = conn.execute(
            """
            INSERT INTO generated_scenes (prompt, scene_data, model, metadata)
            VALUES (?, ?, ?, ?)
            """,
            (
                prompt,
                json.dumps(scene_data),
                model,
                json.dumps(metadata) if metadata else None
            )
        )
        conn.commit()
        return cursor.lastrowid

def get_scene_by_id(scene_id: int) -> Optional[Dict[str, Any]]:
    """Retrieve a specific scene by ID."""
    with get_db() as conn:
        row = conn.execute(
            "SELECT * FROM generated_scenes WHERE id = ?",
            (scene_id,)
        ).fetchone()

        if row:
            return {
                "id": row["id"],
                "prompt": row["prompt"],
                "scene_data": json.loads(row["scene_data"]),
                "model": row["model"],
                "created_at": row["created_at"],
                "metadata": json.loads(row["metadata"]) if row["metadata"] else None
            }
    return None

def list_scenes(
    limit: int = 50,
    offset: int = 0,
    model: Optional[str] = None
) -> List[Dict[str, Any]]:
    """List generated scenes with pagination and optional model filter."""
    query = "SELECT * FROM generated_scenes"
    params = []

    if model:
        query += " WHERE model = ?"
        params.append(model)

    query += " ORDER BY created_at DESC LIMIT ? OFFSET ?"
    params.extend([limit, offset])

    with get_db() as conn:
        rows = conn.execute(query, params).fetchall()

        return [
            {
                "id": row["id"],
                "prompt": row["prompt"],
                "scene_data": json.loads(row["scene_data"]),
                "model": row["model"],
                "created_at": row["created_at"],
                "metadata": json.loads(row["metadata"]) if row["metadata"] else None
            }
            for row in rows
        ]

def get_scene_count(model: Optional[str] = None) -> int:
    """Get total count of scenes, optionally filtered by model."""
    query = "SELECT COUNT(*) as count FROM generated_scenes"
    params = []

    if model:
        query += " WHERE model = ?"
        params.append(model)

    with get_db() as conn:
        row = conn.execute(query, params).fetchone()
        return row["count"]

def get_models_list() -> List[str]:
    """Get list of unique models that have generated scenes."""
    with get_db() as conn:
        rows = conn.execute(
            "SELECT DISTINCT model FROM generated_scenes ORDER BY model"
        ).fetchall()
        return [row["model"] for row in rows]

def delete_scene(scene_id: int) -> bool:
    """Delete a scene by ID."""
    with get_db() as conn:
        cursor = conn.execute(
            "DELETE FROM generated_scenes WHERE id = ?",
            (scene_id,)
        )
        conn.commit()
        return cursor.rowcount > 0

def save_generated_video(
    prompt: str,
    video_url: str,
    model_id: str,
    parameters: dict,
    collection: Optional[str] = None,
    metadata: Optional[dict] = None
) -> int:
    """Save a generated video to the database."""
    with get_db() as conn:
        cursor = conn.execute(
            """
            INSERT INTO generated_videos (prompt, video_url, model_id, parameters, collection, metadata)
            VALUES (?, ?, ?, ?, ?, ?)
            """,
            (
                prompt,
                video_url,
                model_id,
                json.dumps(parameters),
                collection,
                json.dumps(metadata) if metadata else None
            )
        )
        conn.commit()
        return cursor.lastrowid

def get_video_by_id(video_id: int) -> Optional[Dict[str, Any]]:
    """Retrieve a specific video by ID."""
    with get_db() as conn:
        row = conn.execute(
            "SELECT * FROM generated_videos WHERE id = ?",
            (video_id,)
        ).fetchone()

        if row:
            return {
                "id": row["id"],
                "prompt": row["prompt"],
                "video_url": row["video_url"],
                "model_id": row["model_id"],
                "parameters": json.loads(row["parameters"]),
                "status": row["status"],
                "created_at": row["created_at"],
                "collection": row["collection"],
                "metadata": json.loads(row["metadata"]) if row["metadata"] else None
            }
    return None

def list_videos(
    limit: int = 50,
    offset: int = 0,
    model_id: Optional[str] = None,
    collection: Optional[str] = None
) -> List[Dict[str, Any]]:
    """List generated videos with pagination and optional filters."""
    query = "SELECT * FROM generated_videos WHERE 1=1"
    params = []

    if model_id:
        query += " AND model_id = ?"
        params.append(model_id)

    if collection:
        query += " AND collection = ?"
        params.append(collection)

    query += " ORDER BY created_at DESC LIMIT ? OFFSET ?"
    params.extend([limit, offset])

    with get_db() as conn:
        rows = conn.execute(query, params).fetchall()

        return [
            {
                "id": row["id"],
                "prompt": row["prompt"],
                "video_url": row["video_url"],
                "model_id": row["model_id"],
                "parameters": json.loads(row["parameters"]),
                "status": row["status"],
                "created_at": row["created_at"],
                "collection": row["collection"],
                "metadata": json.loads(row["metadata"]) if row["metadata"] else None
            }
            for row in rows
        ]

def delete_video(video_id: int) -> bool:
    """Delete a video by ID."""
    with get_db() as conn:
        cursor = conn.execute(
            "DELETE FROM generated_videos WHERE id = ?",
            (video_id,)
        )
        conn.commit()
        return cursor.rowcount > 0

# Initialize database on import
init_db()
</file>

<file path="backend/genesis_renderer.py">
"""
Genesis Photorealistic Renderer

Main renderer that orchestrates:
1. LLM semantic augmentation
2. Scene conversion to Genesis
3. Ray-traced rendering
4. Video export
"""

import os
import time
from typing import Dict, List, Optional, Tuple
from pathlib import Path

import genesis as gs
from .llm_interpreter import get_interpreter
from .scene_converter import SceneConverter


class RenderQuality:
    """Predefined quality presets for rendering"""

    DRAFT = {
        "spp": 64,
        "description": "Fast preview (30 sec/frame)",
        "tracing_depth": 16
    }

    HIGH = {
        "spp": 256,
        "description": "Production quality (2 min/frame)",
        "tracing_depth": 32
    }

    ULTRA = {
        "spp": 512,
        "description": "Maximum quality (4 min/frame)",
        "tracing_depth": 48
    }


class GenesisRenderer:
    """Photorealistic renderer using Genesis ray-tracer"""

    def __init__(
        self,
        quality: str = "high",
        output_dir: str = "./backend/DATA/genesis_videos"
    ):
        """
        Initialize Genesis renderer

        Args:
            quality: "draft", "high", or "ultra"
            output_dir: Directory to save rendered videos
        """

        self.quality = self._get_quality_preset(quality)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.scene = None
        self.camera = None
        self.converter = None
        self.llm_interpreter = get_interpreter()

    def _get_quality_preset(self, quality: str) -> Dict:
        """Get quality preset by name"""
        presets = {
            "draft": RenderQuality.DRAFT,
            "high": RenderQuality.HIGH,
            "ultra": RenderQuality.ULTRA
        }
        return presets.get(quality.lower(), RenderQuality.HIGH)

    async def render_scene(
        self,
        scene_data: Dict,
        duration: float = 5.0,
        fps: int = 60,
        resolution: Tuple[int, int] = (1920, 1080),
        camera_config: Optional[Dict] = None,
        scene_context: Optional[str] = None
    ) -> str:
        """
        Render a complete scene to video

        Args:
            scene_data: JSON scene data with objects
            duration: Video duration in seconds
            fps: Frames per second
            resolution: (width, height) tuple
            camera_config: Optional camera position/settings
            scene_context: Optional overall scene description for LLM context

        Returns:
            Path to rendered video file
        """

        print(f"ðŸŽ¬ Starting Genesis render (Quality: {self.quality['description']})")
        start_time = time.time()

        # Step 1: Augment objects with LLM
        print("ðŸ¤– Augmenting scene with LLM...")
        augmented_objects = await self.llm_interpreter.augment_scene(
            scene_data.get("objects", []),
            scene_context=scene_context
        )
        scene_data["objects"] = augmented_objects

        # Step 2: Create Genesis scene with ray-tracer
        print("ðŸŒ Creating Genesis scene...")
        self._create_scene()

        # Step 3: Convert JSON to Genesis entities
        print("ðŸ“¦ Converting objects to Genesis entities...")
        self.converter = SceneConverter(self.scene)
        self.converter.add_ground_plane()
        self.converter.convert_scene(scene_data)

        # Step 4: Setup camera
        print("ðŸ“¸ Setting up camera...")
        self._setup_camera(resolution, camera_config)

        # Step 5: Build scene
        print("ðŸ”¨ Building scene...")
        self.scene.build()

        # Step 6: Render frames
        print(f"ðŸŽ¥ Rendering {int(duration * fps)} frames...")
        output_path = await self._render_video(duration, fps)

        elapsed = time.time() - start_time
        print(f"âœ… Rendering complete in {elapsed:.1f}s: {output_path}")

        return output_path

    def _create_scene(self):
        """Create Genesis scene with ray-tracer backend"""

        # Configure lighting (3-point lighting setup)
        lights = [
            {
                "pos": (10.0, 20.0, 10.0),
                "color": (1.0, 0.95, 0.9),  # Warm key light
                "intensity": 15.0,
                "radius": 6.0
            },
            {
                "pos": (-10.0, 10.0, -10.0),
                "color": (0.8, 0.9, 1.0),  # Cool fill light
                "intensity": 5.0,
                "radius": 4.0
            },
            {
                "pos": (0.0, 5.0, -15.0),
                "color": (1.0, 1.0, 1.0),  # Back light
                "intensity": 8.0,
                "radius": 3.0
            }
        ]

        # Create scene with ray-tracer
        self.scene = gs.Scene(
            renderer=gs.renderers.RayTracer(
                spp=self.quality["spp"],
                tracing_depth=self.quality["tracing_depth"],
                lights=lights,
                env_radius=1000.0,
                rr_depth=0,
                rr_threshold=0.95,
                logging_level="warning"
            ),
            show_viewer=False,  # No GUI for batch rendering
            sim_options=gs.options.SimOptions(
                dt=1/60,  # 60Hz simulation
                gravity=(0, -9.81, 0)
            )
        )

    def _setup_camera(
        self,
        resolution: Tuple[int, int],
        camera_config: Optional[Dict] = None
    ):
        """Setup camera with photorealistic settings"""

        # Default camera config
        config = camera_config or {}

        pos = config.get("position", (8, 6, 8))
        lookat = config.get("lookat", (0, 2, 0))
        fov = config.get("fov", 40)
        aperture = config.get("aperture", 2.8)

        self.camera = self.scene.add_camera(
            model="thinlens",  # Enable depth-of-field
            spp=self.quality["spp"],
            aperture=aperture,
            focus_dist=None,  # Auto-compute from pos/lookat
            denoise=True,  # Enable AI denoising
            res=resolution,
            fov=fov,
            pos=pos,
            lookat=lookat
        )

    async def _render_video(
        self,
        duration: float,
        fps: int
    ) -> str:
        """Render simulation to video"""

        # Generate unique output filename
        timestamp = int(time.time())
        output_filename = f"genesis_render_{timestamp}.mp4"
        output_path = str(self.output_dir / output_filename)

        # Start recording
        self.camera.start_recording()

        # Simulate and render frames
        num_frames = int(duration * fps)
        for frame_idx in range(num_frames):
            # Progress indicator
            if frame_idx % 10 == 0:
                progress = (frame_idx / num_frames) * 100
                print(f"  Progress: {progress:.1f}% ({frame_idx}/{num_frames} frames)")

            # Step physics simulation
            self.scene.step()

            # Optional: Update camera pose for dynamic shots
            # self.camera.set_pose(pos=..., lookat=...)

            # Render frame (automatically captured by recorder)
            self.camera.render(
                rgb=True,
                antialiasing=True
            )

        # Stop recording and export video
        self.camera.stop_recording(
            save_to_filename=output_path,
            fps=fps
        )

        return output_path

    def cleanup(self):
        """Clean up Genesis resources"""
        if self.scene:
            # Genesis handles cleanup automatically, but we can reset references
            self.scene = None
            self.camera = None
            self.converter = None


# Factory function for easy creation
def create_renderer(
    quality: str = "high",
    output_dir: str = "./backend/DATA/genesis_videos"
) -> GenesisRenderer:
    """
    Create a Genesis renderer with specified quality

    Args:
        quality: "draft", "high", or "ultra"
        output_dir: Output directory for videos

    Returns:
        GenesisRenderer instance
    """
    return GenesisRenderer(quality=quality, output_dir=output_dir)
</file>

<file path="backend/llm_interpreter.py">
"""
LLM Interpreter for Semantic Scene Augmentation

Takes simple geometry + text descriptions and uses an LLM to generate
detailed Genesis properties (materials, colors, scales, etc.)
"""

import os
import json
from typing import Dict, List, Optional
from anthropic import Anthropic
from pydantic import BaseModel


class GenesisProperties(BaseModel):
    """Enhanced properties for Genesis rendering"""
    # Visual properties
    color: tuple[float, float, float]  # RGB 0-1
    metallic: float  # 0-1
    roughness: float  # 0-1
    opacity: float = 1.0
    emissive: tuple[float, float, float] = (0.0, 0.0, 0.0)

    # Geometry adjustments
    scale_multiplier: tuple[float, float, float] = (1.0, 1.0, 1.0)
    suggested_dimensions: Optional[Dict[str, float]] = None  # Real-world dimensions

    # Additional details
    add_details: List[str] = []  # e.g., ["wheels", "windows", "headlights"]
    material_type: str = "generic"  # "metal", "plastic", "glass", "wood", etc.

    # Contextual info
    object_category: str = "unknown"  # "vehicle", "furniture", "building", etc.
    reasoning: str = ""  # LLM's reasoning for choices


class LLMInterpreter:
    """Interprets text descriptions and generates Genesis properties"""

    def __init__(self):
        self.client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        self.model = "claude-sonnet-4-5-20250929"

    async def augment_object(
        self,
        shape: str,
        base_dimensions: Dict[str, float],
        description: str,
        context: Optional[str] = None
    ) -> GenesisProperties:
        """
        Augment a simple shape with semantic properties based on description

        Args:
            shape: Base shape type ("Box", "Sphere", "Cylinder", "Capsule")
            base_dimensions: Current dimensions (e.g., {"x": 2.0, "y": 1.0, "z": 4.0})
            description: User's text description (e.g., "blue corvette")
            context: Optional scene context for better interpretation

        Returns:
            GenesisProperties with enhanced rendering properties
        """

        prompt = self._build_augmentation_prompt(
            shape, base_dimensions, description, context
        )

        response = self.client.messages.create(
            model=self.model,
            max_tokens=2000,
            temperature=0.3,  # Lower for consistency
            messages=[{"role": "user", "content": prompt}]
        )

        # Parse LLM response
        response_text = response.content[0].text
        properties = self._parse_llm_response(response_text)

        return properties

    def _build_augmentation_prompt(
        self,
        shape: str,
        base_dimensions: Dict[str, float],
        description: str,
        context: Optional[str] = None
    ) -> str:
        """Build the prompt for the LLM"""

        return f"""You are helping create a photorealistic 3D scene. A user has placed a simple {shape} shape and wants it rendered as: "{description}"

Base shape dimensions:
{json.dumps(base_dimensions, indent=2)}

Your task: Generate PBR (Physically Based Rendering) properties to make this shape look like the described object.

Respond with a JSON object containing:
{{
  "color": [R, G, B],  // RGB values 0.0-1.0
  "metallic": 0.0-1.0,  // 0=non-metallic, 1=fully metallic
  "roughness": 0.0-1.0,  // 0=mirror smooth, 1=rough/matte
  "opacity": 0.0-1.0,    // 1=opaque, 0=transparent
  "emissive": [R, G, B], // Self-illumination (usually [0,0,0])
  "scale_multiplier": [x, y, z],  // Adjust proportions (1.0 = no change)
  "suggested_dimensions": {{"length": X, "width": Y, "height": Z}},  // Real-world meters
  "add_details": ["detail1", "detail2"],  // Visual details to emphasize
  "material_type": "metal|plastic|glass|wood|fabric|concrete|ceramic",
  "object_category": "vehicle|furniture|building|nature|electronics|sports",
  "reasoning": "Brief explanation of your choices"
}}

Examples for reference:

"blue corvette" on a Box:
{{
  "color": [0.0, 0.27, 0.67],  // Deep blue
  "metallic": 0.9,  // Car paint is metallic
  "roughness": 0.2,  // Glossy finish
  "scale_multiplier": [1.0, 0.65, 2.25],  // Car proportions (wider, lower, longer)
  "suggested_dimensions": {{"length": 4.5, "width": 1.8, "height": 1.3}},
  "add_details": ["wheels", "windows", "headlights", "spoiler"],
  "material_type": "metal",
  "object_category": "vehicle",
  "reasoning": "Corvette is a sports car with metallic blue paint, low profile, and distinctive aerodynamic shape"
}}

"light pole" on a Cylinder:
{{
  "color": [0.5, 0.5, 0.52],  // Galvanized steel gray
  "metallic": 0.7,  // Metal pole
  "roughness": 0.6,  // Weathered metal
  "scale_multiplier": [1.0, 6.0, 1.0],  // Tall and thin
  "suggested_dimensions": {{"diameter": 0.25, "height": 8.0}},
  "add_details": ["light_bulb", "base_plate", "electrical_box"],
  "material_type": "metal",
  "object_category": "building",
  "reasoning": "Street light poles are typically 8m tall, galvanized steel, with weathered finish"
}}

"wooden coffee table" on a Box:
{{
  "color": [0.55, 0.35, 0.2],  // Walnut brown
  "metallic": 0.0,  // Wood is non-metallic
  "roughness": 0.4,  // Polished wood
  "scale_multiplier": [1.2, 0.4, 0.8],  // Table proportions
  "suggested_dimensions": {{"length": 1.2, "width": 0.6, "height": 0.45}},
  "add_details": ["wood_grain", "table_legs", "surface_reflection"],
  "material_type": "wood",
  "object_category": "furniture",
  "reasoning": "Coffee tables are low, wide, with polished wood finish showing natural grain"
}}

Now generate properties for: "{description}"
Shape: {shape}
Current dimensions: {json.dumps(base_dimensions)}
{f"Scene context: {context}" if context else ""}

Respond with ONLY the JSON object, no other text.
"""

    def _parse_llm_response(self, response: str) -> GenesisProperties:
        """Parse LLM JSON response into GenesisProperties"""

        try:
            # Extract JSON from response (handle markdown code blocks)
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                json_str = response.split("```")[1].split("```")[0].strip()
            else:
                json_str = response.strip()

            data = json.loads(json_str)

            # Convert to GenesisProperties
            return GenesisProperties(
                color=tuple(data["color"]),
                metallic=data["metallic"],
                roughness=data["roughness"],
                opacity=data.get("opacity", 1.0),
                emissive=tuple(data.get("emissive", [0.0, 0.0, 0.0])),
                scale_multiplier=tuple(data.get("scale_multiplier", [1.0, 1.0, 1.0])),
                suggested_dimensions=data.get("suggested_dimensions"),
                add_details=data.get("add_details", []),
                material_type=data.get("material_type", "generic"),
                object_category=data.get("object_category", "unknown"),
                reasoning=data.get("reasoning", "")
            )

        except Exception as e:
            print(f"Error parsing LLM response: {e}")
            print(f"Response was: {response}")

            # Return default properties on error
            return GenesisProperties(
                color=(0.7, 0.7, 0.7),
                metallic=0.2,
                roughness=0.7,
                reasoning=f"Failed to parse LLM response: {e}"
            )

    async def augment_scene(
        self,
        scene_objects: List[Dict],
        scene_context: Optional[str] = None
    ) -> List[Dict]:
        """
        Augment all objects in a scene

        Args:
            scene_objects: List of objects with shape, dimensions, and description
            scene_context: Overall scene description for context

        Returns:
            Objects with added 'genesis_properties' field
        """

        augmented_objects = []

        for obj in scene_objects:
            # Skip if no description provided
            if not obj.get("description"):
                augmented_objects.append(obj)
                continue

            # Extract dimensions from object
            shape = obj.get("visualProperties", {}).get("shape", "Box")
            scale = obj.get("transform", {}).get("scale", {"x": 1, "y": 1, "z": 1})

            # Get augmented properties
            properties = await self.augment_object(
                shape=shape,
                base_dimensions=scale,
                description=obj["description"],
                context=scene_context
            )

            # Add to object
            obj["genesis_properties"] = properties.model_dump()
            augmented_objects.append(obj)

        return augmented_objects


# Singleton instance
_interpreter = None

def get_interpreter() -> LLMInterpreter:
    """Get or create the LLM interpreter singleton"""
    global _interpreter
    if _interpreter is None:
        _interpreter = LLMInterpreter()
    return _interpreter
</file>

<file path="backend/scene_converter.py">
"""
Scene Converter: JSON Scene Data â†’ Genesis Entities

Converts the frontend JSON scene format to Genesis scene objects
with LLM-augmented properties
"""

import genesis as gs
from typing import Dict, List, Optional, Tuple


class SceneConverter:
    """Converts JSON scene data to Genesis entities"""

    def __init__(self, scene: gs.Scene):
        self.scene = scene
        self.entities = []

    def convert_scene(self, scene_data: Dict) -> List:
        """
        Convert full scene data to Genesis entities

        Args:
            scene_data: JSON scene data with objects list

        Returns:
            List of created Genesis entities
        """

        objects = scene_data.get("objects", [])

        for obj_data in objects:
            entity = self.convert_object(obj_data)
            if entity:
                self.entities.append(entity)

        return self.entities

    def convert_object(self, obj_data: Dict) -> Optional[gs.Entity]:
        """
        Convert a single object to a Genesis entity

        Args:
            obj_data: Object data with transform, physics, visual, and genesis properties

        Returns:
            Genesis Entity or None if conversion fails
        """

        try:
            # Extract base properties
            transform = obj_data.get("transform", {})
            physics = obj_data.get("physicsProperties", {})
            visual = obj_data.get("visualProperties", {})
            genesis_props = obj_data.get("genesis_properties", {})

            # Create morph (geometry)
            morph = self._create_morph(visual, genesis_props)
            if not morph:
                return None

            # Create material (physics)
            material = self._create_material(physics)

            # Create surface (visual/PBR)
            surface = self._create_surface(visual, genesis_props)

            # Get position and rotation
            position = self._get_position(transform)
            rotation = self._get_rotation(transform)

            # Add entity to scene
            entity = self.scene.add_entity(
                morph=morph,
                material=material,
                surface=surface,
                pos=position,
                quat=rotation
            )

            return entity

        except Exception as e:
            print(f"Error converting object: {e}")
            print(f"Object data: {obj_data}")
            return None

    def _create_morph(
        self,
        visual: Dict,
        genesis_props: Dict
    ) -> Optional[gs.Morph]:
        """Create Genesis morph (geometry) from visual properties"""

        shape = visual.get("shape", "Box")

        # Get scale (use LLM-suggested dimensions if available, else use base scale)
        if genesis_props and genesis_props.get("suggested_dimensions"):
            dims = genesis_props["suggested_dimensions"]

            if shape == "Box":
                size = [
                    dims.get("length", dims.get("width", 1.0)),
                    dims.get("width", dims.get("length", 1.0)),
                    dims.get("height", 1.0)
                ]
            elif shape == "Sphere":
                size = dims.get("radius", dims.get("diameter", 1.0) / 2)
            elif shape == "Cylinder":
                size = {
                    "radius": dims.get("radius", dims.get("diameter", 1.0) / 2),
                    "height": dims.get("height", 1.0)
                }
            elif shape == "Capsule":
                size = {
                    "radius": dims.get("radius", dims.get("diameter", 1.0) / 2),
                    "length": dims.get("length", dims.get("height", 1.0))
                }
        else:
            # Use base scale with optional multiplier
            scale = visual.get("scale", {"x": 1, "y": 1, "z": 1})
            multiplier = genesis_props.get("scale_multiplier", [1.0, 1.0, 1.0]) if genesis_props else [1.0, 1.0, 1.0]

            if shape == "Box":
                size = [
                    scale.get("x", 1.0) * multiplier[0],
                    scale.get("y", 1.0) * multiplier[1],
                    scale.get("z", 1.0) * multiplier[2]
                ]
            elif shape == "Sphere":
                size = scale.get("x", 1.0) * multiplier[0] / 2  # radius
            elif shape == "Cylinder":
                size = {
                    "radius": scale.get("x", 1.0) * multiplier[0] / 2,
                    "height": scale.get("y", 1.0) * multiplier[1]
                }
            elif shape == "Capsule":
                size = {
                    "radius": scale.get("x", 1.0) * multiplier[0] / 2,
                    "length": scale.get("y", 1.0) * multiplier[1]
                }

        # Create appropriate morph
        if shape == "Box":
            return gs.morphs.Box(size=size)
        elif shape == "Sphere":
            return gs.morphs.Sphere(radius=size)
        elif shape == "Cylinder":
            return gs.morphs.Cylinder(radius=size["radius"], height=size["height"])
        elif shape == "Capsule":
            return gs.morphs.Capsule(radius=size["radius"], length=size["length"])
        else:
            print(f"Unsupported shape: {shape}, defaulting to Box")
            return gs.morphs.Box(size=[1.0, 1.0, 1.0])

    def _create_material(self, physics: Dict) -> gs.Material:
        """Create Genesis material (physics properties) from physics data"""

        return gs.materials.Rigid(
            rho=physics.get("mass", 1.0) * 1000,  # Convert to kg/mÂ³
            friction=physics.get("friction", 0.5),
            restitution=physics.get("restitution", 0.3)
        )

    def _create_surface(
        self,
        visual: Dict,
        genesis_props: Dict
    ) -> gs.Surface:
        """Create Genesis surface (PBR visual properties)"""

        # If we have LLM-augmented properties, use them
        if genesis_props:
            color = tuple(genesis_props.get("color", [0.7, 0.7, 0.7]))
            metallic = genesis_props.get("metallic", 0.2)
            roughness = genesis_props.get("roughness", 0.7)
            opacity = genesis_props.get("opacity", 1.0)
            emissive = tuple(genesis_props.get("emissive", [0.0, 0.0, 0.0]))
        else:
            # Fall back to basic visual properties
            color = self._hex_to_rgb(visual.get("color", "#B0B0B0"))
            metallic = 0.2
            roughness = 0.7
            opacity = 1.0
            emissive = (0.0, 0.0, 0.0)

        return gs.surfaces.Surface(
            color=color,
            metallic=metallic,
            roughness=roughness,
            opacity=opacity,
            emissive=emissive,
            smooth=True,  # Enable smooth shading
            double_sided=False
        )

    def _get_position(self, transform: Dict) -> Tuple[float, float, float]:
        """Extract position from transform"""
        pos = transform.get("position", {"x": 0, "y": 0, "z": 0})
        return (pos.get("x", 0), pos.get("y", 0), pos.get("z", 0))

    def _get_rotation(self, transform: Dict) -> Optional[Tuple[float, float, float, float]]:
        """Extract rotation quaternion from transform"""
        rot = transform.get("rotation")
        if rot and all(k in rot for k in ["x", "y", "z", "w"]):
            return (rot["x"], rot["y"], rot["z"], rot["w"])
        return None  # Use default rotation

    def _hex_to_rgb(self, hex_color: str) -> Tuple[float, float, float]:
        """Convert hex color to RGB tuple (0-1 range)"""
        hex_color = hex_color.lstrip('#')

        if len(hex_color) == 6:
            r = int(hex_color[0:2], 16) / 255.0
            g = int(hex_color[2:4], 16) / 255.0
            b = int(hex_color[4:6], 16) / 255.0
            return (r, g, b)
        else:
            return (0.7, 0.7, 0.7)  # Default gray

    def add_ground_plane(
        self,
        size: float = 50.0,
        height: float = 0.0,
        color: Tuple[float, float, float] = (0.3, 0.3, 0.3)
    ):
        """Add a ground plane to the scene"""

        ground = self.scene.add_entity(
            morph=gs.morphs.Plane(),
            material=gs.materials.Rigid(rho=1000, friction=0.8),
            surface=gs.surfaces.Surface(
                color=color,
                roughness=0.9,
                metallic=0.0
            ),
            pos=(0, height, 0)
        )

        self.entities.append(ground)
        return ground
</file>

<file path="src/Route.elm">
module Route exposing (Route(..), fromUrl, toHref)

import Url exposing (Url)
import Url.Parser as Parser exposing (Parser, oneOf, s)


type Route
    = Physics
    | Videos
    | Gallery


parser : Parser (Route -> a) a
parser =
    oneOf
        [ Parser.map Physics Parser.top
        , Parser.map Physics (s "physics")
        , Parser.map Videos (s "videos")
        , Parser.map Gallery (s "gallery")
        ]


fromUrl : Url -> Maybe Route
fromUrl url =
    Parser.parse parser url


toHref : Route -> String
toHref route =
    case route of
        Physics ->
            "/physics"

        Videos ->
            "/videos"

        Gallery ->
            "/gallery"
</file>

<file path="src/Video.elm">
module Video exposing (Model, Msg, init, update, view)

import Html exposing (..)
import Html.Attributes exposing (..)
import Html.Events exposing (..)
import Http
import Json.Decode as Decode
import Json.Encode as Encode


-- MODEL


type alias Model =
    { models : List VideoModel
    , selectedModel : Maybe VideoModel
    , parameters : List Parameter
    , isGenerating : Bool
    , outputVideo : Maybe String
    , error : Maybe String
    , searchQuery : String
    , selectedCollection : String
    , requiredFields : List String
    }


type alias Parameter =
    { key : String
    , value : String
    , paramType : String
    , enum : Maybe (List String)
    , description : Maybe String
    , default : Maybe String
    , minimum : Maybe Float
    , maximum : Maybe Float
    , format : Maybe String
    }


type alias VideoModel =
    { id : String
    , name : String
    , description : String
    , inputSchema : Maybe Decode.Value
    }


init : ( Model, Cmd Msg )
init =
    ( { models = []
      , selectedModel = Nothing
      , parameters = []
      , isGenerating = False
      , outputVideo = Nothing
      , error = Nothing
      , searchQuery = ""
      , selectedCollection = "text-to-video"
      , requiredFields = []
      }
    , fetchModels "text-to-video"
    )


-- UPDATE


type Msg
    = NoOp
    | FetchModels
    | SelectCollection String
    | ModelsFetched (Result Http.Error (List VideoModel))
    | SelectModel String
    | SchemaFetched String (Result Http.Error { schema : Decode.Value, required : List String })
    | UpdateParameter String String
    | UpdateSearch String
    | GenerateVideo
    | VideoGenerated (Result Http.Error { output : List String, status : String })


update : Msg -> Model -> ( Model, Cmd Msg )
update msg model =
    case msg of
        NoOp ->
            ( model, Cmd.none )

        FetchModels ->
            ( model, fetchModels model.selectedCollection )

        SelectCollection collection ->
            ( { model | selectedCollection = collection, selectedModel = Nothing, outputVideo = Nothing, requiredFields = [] }
            , fetchModels collection
            )

        ModelsFetched result ->
            case result of
                Ok models ->
                    ( { model | models = models, error = Nothing }, Cmd.none )

                Err error ->
                    ( { model | models = demoModels, error = Just ("Failed to fetch models: " ++ httpErrorToString error) }, Cmd.none )

        SelectModel modelId ->
            let
                selected =
                    model.models
                        |> List.filter (\m -> m.id == modelId)
                        |> List.head
            in
            case selected of
                Just selectedModel ->
                    ( { model | selectedModel = selected, outputVideo = Nothing, error = Nothing, parameters = [] }
                    , fetchModelSchema selectedModel.id
                    )

                Nothing ->
                    ( model, Cmd.none )

        SchemaFetched modelId result ->
            case result of
                Ok { schema, required } ->
                    let
                        params =
                            case Decode.decodeValue (Decode.keyValuePairs Decode.value) schema of
                                Ok properties ->
                                    List.map (parseParameter) properties

                                Err _ ->
                                    [ Parameter "prompt" "" "string" Nothing Nothing Nothing Nothing Nothing Nothing ]
                    in
                    ( { model | parameters = params, requiredFields = required }, Cmd.none )

                Err _ ->
                    -- Fallback to default prompt parameter
                    ( { model | parameters = [ Parameter "prompt" "" "string" Nothing Nothing Nothing Nothing Nothing Nothing ], requiredFields = [ "prompt" ] }, Cmd.none )

        UpdateParameter key value ->
            let
                updatedParams =
                    updateParameterInList key value model.parameters
            in
            ( { model | parameters = updatedParams }, Cmd.none )

        UpdateSearch query ->
            ( { model | searchQuery = query }, Cmd.none )

        GenerateVideo ->
            case model.selectedModel of
                Just selectedModel ->
                    ( { model | isGenerating = True, error = Nothing }
                    , generateVideo selectedModel.id model.parameters model.selectedCollection
                    )

                Nothing ->
                    ( { model | error = Just "No model selected" }, Cmd.none )

        VideoGenerated result ->
            case result of
                Ok response ->
                    let
                        urls = response.output
                    in
                    ( { model
                        | isGenerating = False
                        , outputVideo = List.head urls
                        , error = Nothing
                      }
                    , Cmd.none
                    )

                Err error ->
                    ( { model | isGenerating = False, error = Just (httpErrorToString error) }, Cmd.none )


-- HELPER FUNCTIONS


parseParameter : ( String, Decode.Value ) -> Parameter
parseParameter ( key, value ) =
    let
        paramType =
            Decode.decodeValue (Decode.at [ "type" ] Decode.string) value
                |> Result.withDefault "string"

        enumValues =
            Decode.decodeValue (Decode.at [ "enum" ] (Decode.list Decode.string)) value
                |> Result.toMaybe

        description =
            Decode.decodeValue (Decode.at [ "description" ] Decode.string) value
                |> Result.toMaybe

        default =
            Decode.decodeValue (Decode.field "default" Decode.value) value
                |> Result.toMaybe
                |> Maybe.andThen
                    (\v ->
                        case Decode.decodeValue Decode.string v of
                            Ok s ->
                                Just s

                            Err _ ->
                                case Decode.decodeValue Decode.float v of
                                    Ok f ->
                                        Just (String.fromFloat f)

                                    Err _ ->
                                        case Decode.decodeValue Decode.int v of
                                            Ok i ->
                                                Just (String.fromInt i)

                                            Err _ ->
                                                Nothing
                    )

        minimum =
            Decode.decodeValue (Decode.field "minimum" Decode.float) value
                |> Result.toMaybe

        maximum =
            Decode.decodeValue (Decode.field "maximum" Decode.float) value
                |> Result.toMaybe

        format =
            Decode.decodeValue (Decode.field "format" Decode.string) value
                |> Result.toMaybe

        initialValue =
            Maybe.withDefault "" default
    in
    Parameter key initialValue paramType enumValues description default minimum maximum format


getDefaultParameters : VideoModel -> List Parameter
getDefaultParameters model =
    case model.inputSchema of
        Just schema ->
            case Decode.decodeValue (Decode.keyValuePairs Decode.value) schema of
                Ok properties ->
                    List.map parseParameter properties

                Err _ ->
                    [ Parameter "prompt" "" "string" Nothing Nothing Nothing Nothing Nothing Nothing ]

        Nothing ->
            [ Parameter "prompt" "" "string" Nothing Nothing Nothing Nothing Nothing Nothing ]


updateParameterInList : String -> String -> List Parameter -> List Parameter
updateParameterInList key value params =
    List.map
        (\param ->
            if param.key == key then
                { param | value = value }

            else
                param
        )
        params


-- VIEW


view : Model -> Html Msg
view model =
    div [ class "video-page" ]
        [ h1 [] [ text "Video Models Explorer" ]
        , div [ class "collection-buttons" ]
            [ button
                [ onClick (SelectCollection "text-to-video")
                , class (if model.selectedCollection == "text-to-video" then "collection-button active" else "collection-button")
                ]
                [ text "Text to Video" ]
            , button
                [ onClick (SelectCollection "image-to-video")
                , class (if model.selectedCollection == "image-to-video" then "collection-button active" else "collection-button")
                ]
                [ text "Image to Video" ]
            ]
        , div [ class "search-section" ]
            [ input
                [ type_ "text"
                , placeholder "Search video models..."
                , value model.searchQuery
                , onInput UpdateSearch
                ]
                []
            , button [ onClick FetchModels, disabled (model.models /= []) ] [ text (if model.models == [] then "Loading..." else "Refresh Models") ]
            ]
        , if List.isEmpty model.models then
            div [ class "loading-text" ] [ text "Loading models..." ]
          else
            div [ class "models-list" ]
                (model.models
                    |> List.filter (\m -> String.contains (String.toLower model.searchQuery) (String.toLower m.name))
                    |> List.map viewModelOption
                )
        , case model.selectedModel of
            Just selected ->
                div [ class "selected-model" ]
                    [ h2 [] [ text selected.name ]
                    , p [] [ text selected.description ]
                    , div [ class "parameters-form-grid" ]
                        (List.map (viewParameter model.isGenerating model.requiredFields) model.parameters)
                    , button
                        [ onClick GenerateVideo
                        , disabled (hasEmptyRequiredParameters model.parameters model.requiredFields || model.isGenerating)
                        , class "generate-button"
                        ]
                        [ text (if model.isGenerating then "Generating..." else "Generate Video") ]
                    ]

            Nothing ->
                if not (List.isEmpty model.models) then
                    div [] [ text "Select a model from the list above" ]
                else
                    text ""
        , case model.outputVideo of
            Just url ->
                div [ class "video-output" ]
                    [ video [ src url, controls True, attribute "width" "100%" ] [] ]

            Nothing ->
                text ""
        , case model.error of
            Just err ->
                div [ class "error" ] [ text err ]

            Nothing ->
                text ""
        ]


viewModelOption : VideoModel -> Html Msg
viewModelOption model =
    div [ class "model-option", onClick (SelectModel model.id) ]
        [ h3 [] [ text model.name ]
        , p [] [ text model.description ]
        ]


viewParameter : Bool -> List String -> Parameter -> Html Msg
viewParameter isDisabled requiredFields param =
    let
        isRequired =
            List.member param.key requiredFields

        labelText =
            formatParameterName param.key ++ (if isRequired then " *" else "")

        rangeText =
            case ( param.minimum, param.maximum ) of
                ( Just min, Just max ) ->
                    " (" ++ String.fromFloat min ++ " - " ++ String.fromFloat max ++ ")"

                ( Just min, Nothing ) ->
                    " (min: " ++ String.fromFloat min ++ ")"

                ( Nothing, Just max ) ->
                    " (max: " ++ String.fromFloat max ++ ")"

                ( Nothing, Nothing ) ->
                    ""

        fullDescription =
            case param.description of
                Just desc ->
                    desc ++ rangeText

                Nothing ->
                    if rangeText /= "" then
                        String.trim rangeText

                    else
                        ""

        defaultHint =
            case param.default of
                Just def ->
                    if fullDescription /= "" then
                        fullDescription ++ " (default: " ++ def ++ ")"

                    else
                        "default: " ++ def

                Nothing ->
                    fullDescription

        isImageField =
            param.format == Just "uri" || String.contains "image" (String.toLower param.key)
    in
    div [ class "parameter-field" ]
        [ label [ class "parameter-label" ]
            [ text labelText
            , if defaultHint /= "" then
                span [ class "parameter-hint" ] [ text (" â€” " ++ defaultHint) ]

              else
                text ""
            ]
        , case param.enum of
            Just options ->
                select
                    [ onInput (UpdateParameter param.key)
                    , disabled isDisabled
                    , class "parameter-select"
                    , Html.Attributes.value param.value
                    ]
                    (option [ Html.Attributes.value "" ] [ text "-- Select --" ]
                        :: List.map (\opt -> option [ Html.Attributes.value opt ] [ text opt ]) options
                    )

            Nothing ->
                if isImageField then
                    div [ class "image-upload-container" ]
                        [ input
                            [ type_ "file"
                            , Html.Attributes.accept "image/*"
                            , disabled isDisabled
                            , class "parameter-file-input"
                            , Html.Attributes.id ("file-" ++ param.key)
                            ]
                            []
                        , input
                            [ type_ "text"
                            , placeholder "Or enter image URL..."
                            , Html.Attributes.value param.value
                            , onInput (UpdateParameter param.key)
                            , disabled isDisabled
                            , class "parameter-input"
                            ]
                            []
                        ]

                else if param.key == "prompt" then
                    textarea
                        [ placeholder (Maybe.withDefault "Enter prompt..." param.default)
                        , Html.Attributes.value param.value
                        , onInput (UpdateParameter param.key)
                        , disabled isDisabled
                        , class "parameter-input parameter-textarea"
                        ]
                        []

                else
                    input
                        [ type_ (if param.paramType == "number" || param.paramType == "integer" then "number" else "text")
                        , placeholder (Maybe.withDefault ("Enter " ++ param.key ++ "...") param.default)
                        , Html.Attributes.value param.value
                        , onInput (UpdateParameter param.key)
                        , disabled isDisabled
                        , class "parameter-input"
                        ]
                        []
        ]


formatParameterName : String -> String
formatParameterName name =
    name
        |> String.split "_"
        |> List.map capitalize
        |> String.join " "


capitalize : String -> String
capitalize str =
    case String.uncons str of
        Just ( first, rest ) ->
            String.fromChar (Char.toUpper first) ++ rest

        Nothing ->
            str


hasEmptyRequiredParameters : List Parameter -> List String -> Bool
hasEmptyRequiredParameters params requiredFields =
    List.any
        (\param ->
            List.member param.key requiredFields && String.isEmpty (String.trim param.value)
        )
        params


-- HTTP


fetchModels : String -> Cmd Msg
fetchModels collection =
    Http.get
        { url = "http://127.0.0.1:8000/api/video-models?collection=" ++ collection
        , expect = Http.expectJson ModelsFetched (Decode.field "models" (Decode.list videoModelDecoder))
        }


fetchModelSchema : String -> Cmd Msg
fetchModelSchema modelId =
    let
        -- Split modelId into owner/name
        parts =
            String.split "/" modelId

        url =
            case parts of
                [ owner, name ] ->
                    "http://127.0.0.1:8000/api/video-models/" ++ owner ++ "/" ++ name ++ "/schema"

                _ ->
                    ""
    in
    if String.isEmpty url then
        Cmd.none

    else
        Http.get
            { url = url
            , expect = Http.expectJson (SchemaFetched modelId) schemaResponseDecoder
            }


schemaResponseDecoder : Decode.Decoder { schema : Decode.Value, required : List String }
schemaResponseDecoder =
    Decode.map2 (\s r -> { schema = s, required = r })
        (Decode.field "input_schema" Decode.value)
        (Decode.oneOf
            [ Decode.field "required" (Decode.list Decode.string)
            , Decode.succeed []
            ]
        )


generateVideo : String -> List Parameter -> String -> Cmd Msg
generateVideo modelId parameters collection =
    let
        encodeParameterValue : Parameter -> Maybe ( String, Encode.Value )
        encodeParameterValue param =
            if String.isEmpty (String.trim param.value) then
                Nothing
            else
                let
                    encoded =
                        case param.paramType of
                            "integer" ->
                                case String.toInt param.value of
                                    Just i ->
                                        Encode.int i
                                    Nothing ->
                                        Encode.string param.value

                            "number" ->
                                case String.toFloat param.value of
                                    Just f ->
                                        Encode.float f
                                    Nothing ->
                                        Encode.string param.value

                            "boolean" ->
                                case String.toLower param.value of
                                    "true" ->
                                        Encode.bool True
                                    "false" ->
                                        Encode.bool False
                                    _ ->
                                        Encode.string param.value

                            _ ->
                                Encode.string param.value
                in
                Just ( param.key, encoded )

        inputObject =
            Encode.object (List.filterMap encodeParameterValue parameters)
    in
    Http.post
        { url = "http://127.0.0.1:8000/api/run-video-model"
        , body =
            Http.jsonBody
                (Encode.object
                    [ ( "model_id", Encode.string modelId )
                    , ( "input", inputObject )
                    , ( "collection", Encode.string collection )
                    ]
                )
        , expect = Http.expectJson VideoGenerated videoResponseDecoder
        }

videoResponseDecoder : Decode.Decoder { output : List String, status : String }
videoResponseDecoder =
    Decode.map2 (\o s -> { output = o, status = s })
        (Decode.field "output" (Decode.list Decode.string))
        (Decode.field "status" Decode.string)


videoModelDecoder : Decode.Decoder VideoModel
videoModelDecoder =
    Decode.map4 VideoModel
        (Decode.field "id" Decode.string)
        (Decode.field "name" Decode.string)
        (Decode.oneOf
            [ Decode.field "description" Decode.string
            , Decode.succeed "No description available"
            ]
        )
        (Decode.maybe (Decode.field "input_schema" Decode.value))


httpErrorToString : Http.Error -> String
httpErrorToString error =
    case error of
        Http.BadUrl url ->
            "Bad URL: " ++ url

        Http.Timeout ->
            "Request timed out"

        Http.NetworkError ->
            "Network error"

        Http.BadStatus status ->
            "Server error: " ++ String.fromInt status

        Http.BadBody body ->
            "Invalid response: " ++ body


-- Demo models for fallback
demoModels : List VideoModel
demoModels =
    [ VideoModel "demo/text-to-video" "Demo Text-to-Video" "Generates a demo video from text prompt" Nothing
    , VideoModel "demo/image-to-video" "Demo Image-to-Video" "Generates a demo video from image and prompt" Nothing
    ]
</file>

<file path="src/VideoGallery.elm">
module VideoGallery exposing (Model, Msg, init, update, view)

import Html exposing (..)
import Html.Attributes exposing (..)
import Html.Events exposing (..)
import Http
import Json.Decode as Decode


-- MODEL


type alias Model =
    { videos : List VideoRecord
    , loading : Bool
    , error : Maybe String
    , selectedVideo : Maybe VideoRecord
    }


type alias VideoRecord =
    { id : Int
    , prompt : String
    , videoUrl : String
    , modelId : String
    , createdAt : String
    , collection : Maybe String
    }


init : ( Model, Cmd Msg )
init =
    ( { videos = []
      , loading = True
      , error = Nothing
      , selectedVideo = Nothing
      }
    , fetchVideos
    )


-- UPDATE


type Msg
    = NoOp
    | FetchVideos
    | VideosFetched (Result Http.Error (List VideoRecord))
    | SelectVideo VideoRecord
    | CloseVideo


update : Msg -> Model -> ( Model, Cmd Msg )
update msg model =
    case msg of
        NoOp ->
            ( model, Cmd.none )

        FetchVideos ->
            ( { model | loading = True }, fetchVideos )

        VideosFetched result ->
            case result of
                Ok videos ->
                    ( { model | videos = videos, loading = False, error = Nothing }, Cmd.none )

                Err error ->
                    ( { model | loading = False, error = Just (httpErrorToString error) }, Cmd.none )

        SelectVideo video ->
            ( { model | selectedVideo = Just video }, Cmd.none )

        CloseVideo ->
            ( { model | selectedVideo = Nothing }, Cmd.none )


-- VIEW


view : Model -> Html Msg
view model =
    div [ class "video-gallery-page" ]
        [ h1 [] [ text "Generated Videos" ]
        , button [ onClick FetchVideos, disabled model.loading, class "refresh-button" ]
            [ text (if model.loading then "Loading..." else "Refresh") ]
        , case model.error of
            Just err ->
                div [ class "error" ] [ text err ]

            Nothing ->
                text ""
        , if model.loading && List.isEmpty model.videos then
            div [ class "loading-text" ] [ text "Loading videos..." ]

          else if List.isEmpty model.videos then
            div [ class "empty-state" ] [ text "No videos generated yet. Go to the Video Models page to generate some!" ]

          else
            div [ class "videos-grid" ]
                (List.map viewVideoCard model.videos)
        , case model.selectedVideo of
            Just video ->
                viewVideoModal video

            Nothing ->
                text ""
        ]


viewVideoCard : VideoRecord -> Html Msg
viewVideoCard video =
    div [ class "video-card", onClick (SelectVideo video) ]
        [ div [ class "video-thumbnail" ]
            [ video_ [ src video.videoUrl, attribute "preload" "metadata" ] [] ]
        , div [ class "video-card-info" ]
            [ div [ class "video-prompt" ] [ text video.prompt ]
            , div [ class "video-meta" ]
                [ span [ class "video-model" ] [ text video.modelId ]
                , span [ class "video-date" ] [ text (formatDate video.createdAt) ]
                ]
            ]
        ]


viewVideoModal : VideoRecord -> Html Msg
viewVideoModal video =
    div [ class "modal-overlay", onClick CloseVideo ]
        [ div [ class "modal-content", onClick (\_ -> NoOp) ]
            [ button [ class "modal-close", onClick CloseVideo ] [ text "Ã—" ]
            , h2 [] [ text "Generated Video" ]
            , video_ [ src video.videoUrl, controls True, attribute "width" "100%", class "modal-video" ] []
            , div [ class "modal-details" ]
                [ div [ class "detail-row" ]
                    [ strong [] [ text "Prompt: " ]
                    , text video.prompt
                    ]
                , div [ class "detail-row" ]
                    [ strong [] [ text "Model: " ]
                    , text video.modelId
                    ]
                , case video.collection of
                    Just coll ->
                        div [ class "detail-row" ]
                            [ strong [] [ text "Collection: " ]
                            , text coll
                            ]

                    Nothing ->
                        text ""
                , div [ class "detail-row" ]
                    [ strong [] [ text "Created: " ]
                    , text video.createdAt
                    ]
                ]
            ]
        ]


formatDate : String -> String
formatDate dateStr =
    -- Simple formatter - just show the date part
    String.left 19 dateStr


-- HTTP


fetchVideos : Cmd Msg
fetchVideos =
    Http.get
        { url = "http://127.0.0.1:8000/api/videos?limit=50"
        , expect = Http.expectJson VideosFetched (Decode.field "videos" (Decode.list videoDecoder))
        }


videoDecoder : Decode.Decoder VideoRecord
videoDecoder =
    Decode.map6 VideoRecord
        (Decode.field "id" Decode.int)
        (Decode.field "prompt" Decode.string)
        (Decode.field "video_url" Decode.string)
        (Decode.field "model_id" Decode.string)
        (Decode.field "created_at" Decode.string)
        (Decode.maybe (Decode.field "collection" Decode.string))


httpErrorToString : Http.Error -> String
httpErrorToString error =
    case error of
        Http.BadUrl url ->
            "Bad URL: " ++ url

        Http.Timeout ->
            "Request timed out"

        Http.NetworkError ->
            "Network error"

        Http.BadStatus status ->
            "Server error: " ++ String.fromInt status

        Http.BadBody body ->
            "Invalid response: " ++ body
</file>

<file path=".dockerignore">
node_modules
venv
__pycache__
*.pyc
.env
.git
.gitignore
elm-stuff
dist
data
DATA
.vscode
.idea
*.log
*.md
!FLYIO_DEPLOYMENT.md
fly.toml
docker-compose.yml
.dockerignore
README.md
SETUP_SUMMARY.md
DEPLOYMENT.md
</file>

<file path="deploy.sh">
#!/bin/bash
# Deployment script for Fly.io

set -e

echo "ðŸš€ Physics Simulator - Fly.io Deployment"
echo "========================================"
echo ""

# Check if fly CLI is installed
if ! command -v fly &> /dev/null; then
    echo "âŒ Fly.io CLI not found. Please install it first:"
    echo "   curl -L https://fly.io/install.sh | sh"
    exit 1
fi

# Check if logged in
if ! fly auth whoami &> /dev/null; then
    echo "âŒ Not logged in to Fly.io. Please run:"
    echo "   fly auth login"
    exit 1
fi

echo "âœ… Fly.io CLI found and authenticated"
echo ""

# Check if app exists
if fly status &> /dev/null; then
    echo "ðŸ“¦ App exists, proceeding with deployment..."
else
    echo "ðŸ†• App doesn't exist yet."
    echo ""
    read -p "Do you want to create a new app? (y/n) " -n 1 -r
    echo ""
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        fly launch --no-deploy
        echo ""
        echo "âš ï¸  Don't forget to:"
        echo "   1. Create a volume: fly volumes create physics_data --region <your-region> --size 1"
        echo "   2. Set secrets: fly secrets set REPLICATE_API_KEY=your_key_here"
        echo "   3. Run this script again to deploy"
        exit 0
    else
        echo "Deployment cancelled."
        exit 0
    fi
fi

# Check if volume exists
echo ""
echo "ðŸ“‚ Checking for persistent volume..."
if fly volumes list | grep -q "physics_data"; then
    echo "âœ… Volume 'physics_data' found"
else
    echo "âš ï¸  Volume 'physics_data' not found!"
    echo ""
    read -p "Create volume now? (y/n) " -n 1 -r
    echo ""
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        APP_REGION=$(fly status --json | grep -o '"Region":"[^"]*"' | cut -d'"' -f4 | head -1)
        if [ -z "$APP_REGION" ]; then
            APP_REGION="iad"
            echo "âš ï¸  Couldn't detect region, using default: $APP_REGION"
        fi
        echo "Creating volume in region: $APP_REGION"
        fly volumes create physics_data --region "$APP_REGION" --size 1
    else
        echo "âŒ Volume required for deployment. Exiting."
        exit 1
    fi
fi

# Deploy
echo ""
echo "ðŸš€ Deploying to Fly.io..."
fly deploy

echo ""
echo "âœ… Deployment complete!"
echo ""
echo "ðŸŒ Your app is available at: https://$(fly status --json | grep -o '"Hostname":"[^"]*"' | cut -d'"' -f4).fly.dev"
echo ""
echo "ðŸ“Š View logs: fly logs"
echo "ðŸ“ˆ Check status: fly status"
echo "ðŸ”“ SSH access: fly ssh console"
echo ""
</file>

<file path="DEPLOYMENT.md">
# Deployment Guide

## Quick Start with Docker

1. **Set up environment variables**:
   ```bash
   export REPLICATE_AI_KEY=your_key_here
   ```

2. **Build and run**:
   ```bash
   docker-compose up -d
   ```

The API will be available at `http://localhost:8000`

## Environment Variables

- `REPLICATE_AI_KEY` - Required for AI scene generation
- `DATA` - Directory for SQLite database (default: `./DATA`)

## Data Storage

Generated scenes are stored in SQLite database at `$DATA/scenes.db`

### Database Schema

```sql
CREATE TABLE generated_scenes (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    prompt TEXT NOT NULL,
    scene_data TEXT NOT NULL,
    model TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    metadata TEXT
);
```

## API Endpoints

### Scene Generation
- `POST /api/generate` - Generate new scene from prompt
- `POST /api/refine` - Refine existing scene

### Scene History
- `GET /api/scenes` - List all generated scenes
  - Query params: `limit` (default: 50), `offset` (default: 0), `model` (optional filter)
- `GET /api/scenes/{id}` - Get specific scene by ID
- `DELETE /api/scenes/{id}` - Delete scene by ID
- `GET /api/models` - List all models that have generated scenes

### Video Models
- `GET /api/video-models` - List available video generation models
- `POST /api/run-video-model` - Run video generation model

## Production Deployment

### Using Docker

```bash
# Build image
docker build -t physics-simulator .

# Run with persistent data
docker run -d \
  -p 8000:8000 \
  -e REPLICATE_AI_KEY=$REPLICATE_AI_KEY \
  -v $(pwd)/data:/data \
  physics-simulator
```

### Without Docker

```bash
cd backend
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt

# Set environment
export DATA=/path/to/data/directory
export REPLICATE_AI_KEY=your_key_here

# Run
python main.py
```

## Frontend Development

The frontend is built with Elm, Three.js, and Vite.

```bash
npm install
npm run dev  # Development server on port 5173
npm run build  # Production build
```

## Health Check

Check if the API is running:
```bash
curl http://localhost:8000/
```

Expected response:
```json
{
  "message": "Physics Simulator API",
  "status": "running"
}
```

## Backup

To backup your data:
```bash
cp $DATA/scenes.db $DATA/scenes.db.backup
```

## Monitoring

View logs:
```bash
docker-compose logs -f api
```
</file>

<file path="docker-compose.yml">
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - REPLICATE_AI_KEY=${REPLICATE_AI_KEY}
      - DATA=/data
    volumes:
      - ./data:/data
    restart: unless-stopped
</file>

<file path="Dockerfile">
# Multi-stage build for production deployment
FROM node:18-slim AS frontend-builder

WORKDIR /app

# Install CA certificates and Elm compiler
RUN apt-get update && \
    apt-get install -y ca-certificates && \
    rm -rf /var/lib/apt/lists/* && \
    npm install -g elm@latest-0.19.1

# Copy elm.json first to leverage layer caching for Elm packages
COPY elm.json ./

# Pre-fetch Elm packages to verify dependencies
RUN elm make --help || true

# Copy frontend files
COPY package*.json ./
COPY vite.config.js ./
COPY index.html ./
COPY src/ ./src/

# Install dependencies and build frontend
RUN npm ci
RUN npm run build

# Python backend stage
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# Copy backend requirements
COPY backend/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy backend code
COPY backend/ ./backend/

# Copy database.py to root for imports (main.py uses "from database import")
COPY backend/database.py ./database.py

# Copy built frontend from builder stage
COPY --from=frontend-builder /app/dist ./static

# Create data directory
RUN mkdir -p /data
ENV DATA=/data

# Set port for Fly.io
ENV PORT=8080
EXPOSE 8080

# Set PYTHONPATH so database.py can be imported
ENV PYTHONPATH=/app

# Run the application
CMD ["python", "-m", "uvicorn", "backend.main:app", "--host", "0.0.0.0", "--port", "8080"]
</file>

<file path="fly.toml">
# fly.toml app configuration file generated for gauntlet-video-server on 2025-11-14T15:11:03-06:00
#
# See https://fly.io/docs/reference/configuration/ for information about how to use this file.
#

app = 'gauntlet-video-server'
primary_region = 'dfw'

[build]

[env]
  DATA = '/data'
  PORT = '8080'

[[mounts]]
  source = 'physics_data'
  destination = '/data'

[http_service]
  internal_port = 8080
  force_https = true
  auto_stop_machines = 'stop'
  auto_start_machines = true
  min_machines_running = 0
  processes = ['app']

  [[http_service.checks]]
    interval = '30s'
    timeout = '5s'
    grace_period = '10s'
    method = 'GET'
    path = '/health'

[[vm]]
  memory = '2gb'
  cpu_kind = 'shared'
  cpus = 1
</file>

<file path="FLYIO_DEPLOYMENT.md">
# Fly.io Deployment Guide

This guide walks you through deploying the Physics Simulator application to Fly.io with persistent storage for your data.

## Prerequisites

1. Install the Fly.io CLI:
   ```bash
   curl -L https://fly.io/install.sh | sh
   ```

2. Login to Fly.io:
   ```bash
   fly auth login
   ```

3. Make sure you have your environment variables ready:
   - `REPLICATE_API_KEY` (optional, for AI scene generation)

## Initial Setup

### 1. Create the Fly.io Application

```bash
fly launch --no-deploy
```

This will:
- Create a new Fly.io app
- Generate a `fly.toml` configuration file (already provided)
- Set up your app in the Fly.io dashboard

When prompted:
- Choose an app name (or use the generated one)
- Select a region closest to your users
- Don't deploy yet - we need to set up the volume first

### 2. Create Persistent Volume

Create a volume for persistent data storage:

```bash
fly volumes create physics_data --region iad --size 1
```

Adjust:
- `--region` to match your app's region
- `--size` for the volume size in GB (1 GB minimum)

### 3. Set Environment Variables

Set your secrets/environment variables:

```bash
# Required: Set your Replicate API key for AI features
fly secrets set REPLICATE_API_KEY=your_replicate_api_key_here

# Optional: Set other environment variables
fly secrets set DATABASE_PATH=/data/scenes.db
```

### 4. Deploy

Deploy your application:

```bash
fly deploy
```

This will:
- Build the Docker image (including both frontend and backend)
- Push it to Fly.io
- Start your application
- Mount the persistent volume at `/data`

## Accessing Your Application

After deployment:

```bash
# Open in browser
fly open

# View logs
fly logs

# Check status
fly status

# SSH into the machine
fly ssh console
```

Your app will be available at: `https://your-app-name.fly.dev`

## Application Structure

### Production Mode

In production (on Fly.io):
- Frontend: Built Vite/Elm app served by FastAPI as static files
- Backend: FastAPI running on port 8080
- Data: Stored in `/data` (persistent volume)
- Database: SQLite at `/data/scenes.db`

### Development Mode

Locally:
- Frontend: Vite dev server on port 5173
- Backend: FastAPI on port 8000
- Data: Stored in `backend/DATA/`

## Scaling

### Vertical Scaling (More Resources)

Edit `fly.toml` to increase resources:

```toml
[[vm]]
  cpu_kind = "shared"
  cpus = 2           # Increase CPUs
  memory_mb = 512    # Increase RAM
```

Then deploy:
```bash
fly deploy
```

### Volume Size

To increase volume size:

```bash
fly volumes list  # Get volume ID
fly volumes extend <volume-id> --size 10  # Increase to 10GB
```

## Managing Data

### Backup Volume

```bash
# Create a snapshot
fly volumes snapshots create <volume-id>

# List snapshots
fly volumes snapshots list <volume-id>
```

### Access Data

```bash
# SSH into the machine
fly ssh console

# Navigate to data directory
cd /data
ls -la
```

## Configuration

### Port Configuration

The app runs on port 8080 (set via `PORT` env var in `fly.toml`).

### Health Checks

Fly.io monitors the `/health` endpoint:
- Interval: 30 seconds
- Timeout: 5 seconds
- Grace period: 10 seconds

### Auto-scaling

The app is configured to:
- Auto-stop when idle (saves costs)
- Auto-start on requests
- Minimum 0 machines running (cost-effective for low traffic)

Adjust in `fly.toml`:
```toml
[http_service]
  auto_stop_machines = true
  auto_start_machines = true
  min_machines_running = 0  # Change to 1 for always-on
```

## Troubleshooting

### Check Logs

```bash
fly logs
```

### Check Health

```bash
fly checks list
```

### SSH Debug

```bash
fly ssh console
# Check if static files exist
ls -la /app/static
# Check data directory
ls -la /data
# Check running processes
ps aux
```

### Volume Issues

```bash
# List volumes
fly volumes list

# Check volume status
fly volumes show <volume-id>

# Destroy and recreate (WARNING: loses data)
fly volumes destroy <volume-id>
fly volumes create physics_data --region iad --size 1
```

### Rebuild and Deploy

```bash
# Force a fresh build
fly deploy --no-cache
```

## Cost Optimization

For a low-traffic app:
- Use `min_machines_running = 0` (auto-stop when idle)
- Use "shared" CPU kind
- Start with 256MB RAM
- 1GB volume is usually sufficient

Estimated cost: ~$5-10/month for small apps with auto-scaling.

## Updating the Application

1. Make your code changes locally
2. Test locally
3. Deploy:
   ```bash
   fly deploy
   ```

Fly.io will:
- Build new Docker image
- Deploy with zero-downtime
- Keep your data intact (persistent volume)

## DNS and Custom Domain

To use a custom domain:

```bash
fly certs add yourdomain.com
fly certs show yourdomain.com
```

Add the DNS records shown by the command to your domain provider.

## Monitoring

View metrics in the Fly.io dashboard:
- https://fly.io/apps/your-app-name

Or use the CLI:
```bash
fly dashboard
```

## Support

- Fly.io Docs: https://fly.io/docs/
- Fly.io Community: https://community.fly.io/
- This app's issues: https://github.com/your-repo/issues
</file>

<file path="GENESIS_USAGE.md">
# Genesis Photorealistic Rendering - Usage Guide

## Overview

Your simulation platform now supports **LLM-augmented photorealistic rendering** with Genesis! This hybrid architecture lets you:

1. **Interactively edit scenes** with simple shapes (boxes, spheres, cylinders) using Rapier.js in the browser
2. **Describe objects semantically** with text (e.g., "blue corvette", "light pole")
3. **Render photorealistic videos** using Genesis ray-tracer with LLM property augmentation
4. **Optionally stylize** the output further with Veo 3.1

## Architecture

```
Browser (Rapier.js + Three.js)
   â†“ User manipulates simple shapes + adds descriptions
Scene JSON with Text Annotations
   â†“
Backend LLM (Claude Sonnet 4.5)
   â†’ Interprets descriptions
   â†’ Generates PBR properties (materials, colors, scales)
   â†“
Genesis Ray-Tracer
   â†’ Applies augmented properties
   â†’ Renders photorealistic video
   â†“
MP4 Output â†’ (Optional: Veo 3.1 stylization)
```

## Quick Start

### 1. Install Dependencies

```bash
cd backend
pip install -r requirements.txt
```

This installs:
- `genesis-world==0.3.7` - Physics simulation and ray-tracing
- `anthropic>=0.18.0` - LLM for semantic augmentation

**Requirements:**
- Python 3.9+
- CUDA-capable GPU (for ray-tracing)
- Ubuntu 22.04 recommended (or Docker)

### 2. Set Environment Variables

Create/update `.env` file:

```bash
ANTHROPIC_API_KEY=your_claude_api_key_here
REPLICATE_API_KEY=your_replicate_key_here  # Optional, for Veo
```

### 3. Start the Backend

```bash
cd backend
python main.py
```

The server starts on `http://127.0.0.1:8000`

### 4. Use the Frontend

```bash
npm run dev
```

Open `http://localhost:5173`

## Workflow Example

### Step 1: Create Simple Scene

1. Open the browser UI
2. Add objects (boxes, spheres, cylinders)
3. Position and scale them with Rapier.js physics controls
4. Test physics interactions in real-time

### Step 2: Add Semantic Descriptions

For each object, add a text description in the **"Description (for Genesis)"** field:

**Examples:**
- Box â†’ `"blue corvette sports car"`
- Cylinder â†’ `"street light pole"`
- Sphere â†’ `"soccer ball with black and white pattern"`
- Box â†’ `"wooden coffee table with dark walnut finish"`
- Cylinder â†’ `"fire hydrant, red and weathered"`

**Tips:**
- Be specific about colors, materials, and details
- Include texture info ("metallic", "glossy", "weathered")
- Mention key features ("car wheels", "light bulb", "wood grain")

### Step 3: Render with Genesis

Call the Genesis API endpoint:

```typescript
// Frontend example (add to your Elm port or JS)
const renderWithGenesis = async (scene) => {
  const response = await fetch('http://127.0.0.1:8000/api/genesis/render', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      scene: scene,
      duration: 5.0,        // Video duration in seconds
      fps: 60,              // Frames per second
      resolution: [1920, 1080],
      quality: 'high',      // 'draft', 'high', or 'ultra'
      scene_context: 'urban street scene at sunset'  // Optional context
    })
  });

  const result = await response.json();
  console.log('Video ready:', result.video_url);

  // Display video
  videoElement.src = result.video_url;
};
```

Or use curl:

```bash
curl -X POST http://127.0.0.1:8000/api/genesis/render \
  -H "Content-Type: application/json" \
  -d @scene.json
```

### Step 4: Results

You'll receive:

```json
{
  "success": true,
  "video_path": "./backend/DATA/genesis_videos/genesis_render_1234567890.mp4",
  "video_url": "/data/genesis_videos/genesis_render_1234567890.mp4",
  "quality": "high",
  "duration": 5.0,
  "fps": 60
}
```

The video will show:
- Your physics simulation with accurate motion
- Photorealistic appearance based on text descriptions
- Professional lighting and materials

## Quality Presets

### Draft (SPP=64)
- **Use case:** Quick previews
- **Speed:** ~30 seconds/frame
- **Quality:** Some noise, but fast

### High (SPP=256) â­ Default
- **Use case:** Production rendering
- **Speed:** ~2 minutes/frame
- **Quality:** Clean, photorealistic

### Ultra (SPP=512)
- **Use case:** Final export, maximum quality
- **Speed:** ~4 minutes/frame
- **Quality:** Pristine, minimal noise

**Example:**

```json
{
  "quality": "draft"  // For quick iteration
}
```

## LLM Semantic Augmentation

The LLM interpreter (Claude Sonnet 4.5) generates:

### PBR Properties
- **Color:** RGB values based on description
- **Metallic:** 0.0 (wood) to 1.0 (chrome)
- **Roughness:** 0.0 (mirror) to 1.0 (concrete)
- **Opacity:** For glass, transparent materials

### Geometry Adjustments
- **Scale multipliers:** Adjust proportions (e.g., car is wider, lower, longer)
- **Suggested dimensions:** Real-world sizes in meters

### Material Types
- Metal (car body, light pole)
- Plastic (chairs, toys)
- Wood (furniture)
- Glass (windows, bottles)
- Fabric (upholstery)
- Concrete (building materials)

### Detail Annotations
The LLM notes what details to emphasize:
- Car: "wheels", "headlights", "spoiler"
- Light pole: "light_bulb", "base_plate", "electrical_box"
- Table: "wood_grain", "table_legs", "surface_reflection"

## Advanced Configuration

### Custom Camera Settings

```json
{
  "camera_config": {
    "position": [8, 6, 8],     // Camera position in 3D
    "lookat": [0, 2, 0],        // Look-at target
    "fov": 40,                  // Field of view (degrees)
    "aperture": 2.8             // Depth-of-field (lower = more blur)
  }
}
```

### Scene Context

Provide overall scene description for better LLM interpretation:

```json
{
  "scene_context": "urban street scene at golden hour with warm lighting"
}
```

This helps the LLM understand:
- Lighting conditions
- Environment (indoor/outdoor)
- Time of day
- Weather effects

## File Structure

```
backend/
â”œâ”€â”€ genesis_renderer.py      # Main renderer orchestration
â”œâ”€â”€ llm_interpreter.py        # Claude-powered semantic augmentation
â”œâ”€â”€ scene_converter.py        # JSON â†’ Genesis entity conversion
â”œâ”€â”€ main.py                   # API endpoint: /api/genesis/render
â””â”€â”€ DATA/
    â””â”€â”€ genesis_videos/       # Rendered output videos

src/
â””â”€â”€ Main.elm                  # Frontend with description fields
```

## API Reference

### POST /api/genesis/render

**Request Body:**

```typescript
{
  scene: {
    objects: {
      [objectId: string]: {
        id: string
        transform: { position, rotation, scale }
        physicsProperties: { mass, friction, restitution }
        visualProperties: { color, shape }
        description?: string  // â­ New field for semantic rendering
      }
    },
    selectedObject?: string
  },
  duration?: number = 5.0
  fps?: number = 60
  resolution?: [number, number] = [1920, 1080]
  quality?: 'draft' | 'high' | 'ultra' = 'high'
  camera_config?: {
    position?: [number, number, number]
    lookat?: [number, number, number]
    fov?: number
    aperture?: number
  }
  scene_context?: string
}
```

**Response:**

```typescript
{
  success: boolean
  video_path: string
  video_url: string
  quality: string
  duration: number
  fps: number
}
```

## Performance Tips

### 1. Use Draft for Iteration

During scene setup:
```json
{"quality": "draft"}  // Fast feedback loop
```

For final export:
```json
{"quality": "high"}   // Clean output
```

### 2. Batch Multiple Scenes

Genesis supports parallel rendering across multiple GPUs. Consider batch processing:

```bash
for scene in scenes/*.json; do
  curl -X POST ... -d @$scene &
done
wait
```

### 3. Optimize Video Length

- **Short clips (3-5s):** Better for iteration
- **Long videos (30s+):** Use lower FPS (30fps) or draft quality

### 4. GPU Memory

Ray-tracing is memory-intensive:
- **8GB VRAM:** Draft/High quality OK
- **16GB+ VRAM:** Ultra quality supported
- **Low memory?** Reduce resolution to 1280x720

## Troubleshooting

### Error: "Genesis not available"

```
pip install genesis-world==0.3.7
```

Check CUDA installation:
```bash
nvcc --version
nvidia-smi
```

### Error: "ANTHROPIC_API_KEY not set"

Update `.env`:
```bash
ANTHROPIC_API_KEY=sk-ant-...
```

### Slow Rendering

- Use `quality: "draft"` for testing
- Reduce `duration` to 3 seconds
- Lower `resolution` to [1280, 720]

### Objects Look Wrong

- Improve text descriptions (more specific)
- Add `scene_context` for environment understanding
- Check that base shapes match object type (cylinder for pole, box for car)

## Integration with Veo 3.1

Genesis output can be fed into Veo for further stylization:

```
Genesis (Photorealistic Reference)
   â†’ Physically accurate motion
   â†’ Realistic lighting and materials
   â†“
Veo 3.1 (Stylization)
   â†’ Artistic style transfer
   â†’ Prompt-based modifications
   â†’ Final output
```

**Workflow:**
1. Render with Genesis: `/api/genesis/render`
2. Use Genesis video as motion reference for Veo
3. Apply Veo prompt: "make it look like a Pixar animation"

## Example Scenes

### Car Chase Scene

```json
{
  "objects": {
    "car1": {
      "id": "car1",
      "transform": { "position": {"x": 0, "y": 1, "z": 0}, ... },
      "visualProperties": { "color": "#0047AB", "shape": "Box" },
      "description": "blue corvette sports car with chrome wheels"
    },
    "pole": {
      "id": "pole",
      "transform": { "position": {"x": 5, "y": 4, "z": 0}, ... },
      "visualProperties": { "color": "#808080", "shape": "Cylinder" },
      "description": "street light pole, galvanized steel, weathered"
    }
  }
}
```

### Living Room

```json
{
  "objects": {
    "table": {
      "visualProperties": { "color": "#8B4513", "shape": "Box" },
      "description": "wooden coffee table, walnut finish, polished"
    },
    "lamp": {
      "visualProperties": { "color": "#FFD700", "shape": "Sphere" },
      "description": "table lamp with warm yellow light bulb"
    }
  }
}
```

## Future Enhancements

- **Batch rendering UI:** Queue multiple scenes
- **Live preview:** Real-time ray-tracing preview (lower quality)
- **Asset library:** Pre-built objects (cars, furniture, etc.)
- **Genesis generative framework:** When released, add prompt-to-scene

## Support

For issues:
1. Check [Genesis documentation](https://genesis-world.readthedocs.io/)
2. Verify CUDA and GPU drivers
3. Test with simple scenes first (1-2 objects)
4. Review backend logs for detailed errors

---

**Happy rendering! ðŸŽ¬âœ¨**
</file>

<file path="SETUP_SUMMARY.md">
# Physics Simulator Deployment Setup

## What Was Added

### 1. Database Layer (`backend/database.py`)
- SQLite database for storing generated scenes
- Tables: `generated_scenes` with fields:
  - `id`: Primary key
  - `prompt`: User's text prompt
  - `scene_data`: JSON of the generated scene
  - `model`: AI model used (e.g., "claude-3.5-sonnet")
  - `created_at`: Timestamp
  - `metadata`: Additional metadata (source, etc.)
- Functions for CRUD operations:
  - `save_generated_scene()` - Save new scene
  - `get_scene_by_id()` - Retrieve specific scene
  - `list_scenes()` - List with pagination and filtering
  - `get_scene_count()` - Count total scenes
  - `get_models_list()` - List unique models
  - `delete_scene()` - Delete by ID

### 2. Updated API Endpoints (`backend/main.py`)
**New endpoints added:**
- `GET /api/scenes` - List all generated scenes (supports pagination & model filter)
- `GET /api/scenes/{id}` - Get specific scene by ID
- `DELETE /api/scenes/{id}` - Delete scene
- `GET /api/models` - List all models that have generated scenes

**Modified:**
- `POST /api/generate` - Now saves to database and returns `_id`

### 3. Docker Setup
- **Dockerfile** - Ready for production deployment
  - Python 3.11 base image
  - Installs dependencies
  - Creates `/data` volume for database
  - Exposes port 8000

- **docker-compose.yml** - Easy local deployment
  - Maps port 8000
  - Mounts `./data` for persistence
  - Passes `REPLICATE_API_KEY` from environment

- **.dockerignore** - Excludes unnecessary files from build

### 4. Documentation
- **DEPLOYMENT.md** - Complete deployment guide with:
  - Docker quick start
  - Environment variables
  - API endpoints documentation
  - Production deployment instructions
  - Health checks and monitoring

## Environment Variables

```bash
DATA=/data                    # Directory for SQLite database
REPLICATE_AI_KEY=your_key    # Replicate API token
```

## Quick Start

```bash
# Set API key
export REPLICATE_AI_KEY=r8_...

# Using Docker Compose
docker-compose up -d

# Or without Docker
cd backend
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
export DATA=./DATA
python main.py
```

## API Usage Examples

### Generate and Store Scene
```bash
curl -X POST http://localhost:8000/api/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "A car driving on a road"}'
```

Response includes `_id` field with database ID.

### List Stored Scenes
```bash
# All scenes
curl http://localhost:8000/api/scenes

# With pagination
curl "http://localhost:8000/api/scenes?limit=10&offset=0"

# Filter by model
curl "http://localhost:8000/api/scenes?model=claude-3.5-sonnet"
```

### Get Specific Scene
```bash
curl http://localhost:8000/api/scenes/1
```

### Delete Scene
```bash
curl -X DELETE http://localhost:8000/api/scenes/1
```

### List Models
```bash
curl http://localhost:8000/api/models
```

## Database Location

- Development: `./DATA/scenes.db` (or `$DATA/scenes.db`)
- Docker: `/data/scenes.db` (mounted to `./data` on host)

## Next Steps for UI

To add scene browsing to the UI, you would need to:

1. Add a new Elm page/module for browsing scene history
2. Create API calls to `/api/scenes` endpoint
3. Display scenes in a list/grid with:
   - Thumbnail/preview
   - Prompt text
   - Model name
   - Timestamp
   - Load/Delete buttons
4. Add filtering by model and date range
5. Implement pagination controls

Example UI features:
- Scene library/gallery view
- Search and filter scenes
- Click to load scene into simulator
- View scene details (prompt, model, timestamp)
- Delete unwanted scenes
- Export scene data

## Files Modified/Created

**Created:**
- `backend/database.py` - Database operations
- `Dockerfile` - Docker build configuration
- `docker-compose.yml` - Docker Compose setup
- `.dockerignore` - Docker build exclusions
- `DEPLOYMENT.md` - Deployment documentation
- `SETUP_SUMMARY.md` - This file

**Modified:**
- `backend/main.py` - Added database imports, scene history endpoints, save on generate
- Fixed replicate library compatibility (commented out, using HTTP API directly)

## Current Status

âœ… Backend API ready for deployment
âœ… Database layer implemented
âœ… Docker configuration complete
âœ… Documentation written
âœ… Scene storage on generation working
âœ… History API endpoints functional

â³ UI for browsing scenes (not yet implemented)
â³ Frontend deployment setup (needs Vite build config)
</file>

<file path="test_scene.json">
{
  "scene": {
    "objects": {
      "car1": {
        "id": "car1",
        "transform": {
          "position": {"x": 0, "y": 1, "z": 0},
          "rotation": {"x": 0, "y": 0, "z": 0},
          "scale": {"x": 2, "y": 1, "z": 4}
        },
        "physicsProperties": {
          "mass": 1500.0,
          "friction": 0.7,
          "restitution": 0.1
        },
        "visualProperties": {
          "color": "#0047AB",
          "shape": "Box"
        },
        "description": "blue corvette sports car with glossy metallic paint"
      },
      "pole1": {
        "id": "pole1",
        "transform": {
          "position": {"x": 5, "y": 4, "z": 0},
          "rotation": {"x": 0, "y": 0, "z": 0},
          "scale": {"x": 0.5, "y": 8, "z": 0.5}
        },
        "physicsProperties": {
          "mass": 100.0,
          "friction": 0.6,
          "restitution": 0.2
        },
        "visualProperties": {
          "color": "#808080",
          "shape": "Cylinder"
        },
        "description": "street light pole, galvanized steel, weathered finish"
      },
      "ball1": {
        "id": "ball1",
        "transform": {
          "position": {"x": -3, "y": 2, "z": 0},
          "rotation": {"x": 0, "y": 0, "z": 0},
          "scale": {"x": 1, "y": 1, "z": 1}
        },
        "physicsProperties": {
          "mass": 0.45,
          "friction": 0.4,
          "restitution": 0.8
        },
        "visualProperties": {
          "color": "#FFFFFF",
          "shape": "Sphere"
        },
        "description": "soccer ball with traditional black and white pentagon pattern"
      }
    }
  },
  "duration": 3.0,
  "fps": 30,
  "resolution": [1280, 720],
  "quality": "draft",
  "scene_context": "outdoor scene at sunset with warm lighting"
}
</file>

<file path=".opencode/command/tm_review.md">
---
description: review and update taskmaster
agent: plan
model: grok-code-fast-1
---
review all tasks in task-master (.taskmaster/tasks/tasks.json, or use cli).
</file>

<file path=".opencode/command/tm_update.md">
---
description: review and update taskmaster
agent: build
model: grok-code-fast-1
---
review all tasks in task-master (.taskmaster/tasks/tasks.json, or use cli). Update status to match current state of implementation. Ask the user for any clarifying questions.
</file>

<file path=".taskmaster/docs/prd-init.md">
# Product Requirements Document: PhysicsPlayground

**Version:** 1.0  
**Last Updated:** November 12, 2025  
**Status:** Updated

---

## 1. Core Concept

**PhysicsPlayground** enables rapid iteration on physics simulations:
1. User describes scene in natural language
2. System generates validated 3D physics scene (<10s)
3. User manipulates objects visually (drag, rotate, scale)
4. User simulates, observes, resets
5. User refines via text or manual edits
6. Repeat instantly

**Goal:** Collapse "idea â†’ working physics" from 30 minutes to 30 seconds.

---

## 2. User Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRIMARY WORKFLOW                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. TEXT INPUT
   User: "Stack 3 red boxes with a blue sphere on top"
   â†“
2. AI GENERATION (3-8s)
   Claude â†’ Structured JSON â†’ Genesis validation
   â†“
3. SCENE RENDER
   Three.js displays scene, objects selectable
   â†“
4. MANUAL ADJUSTMENT
   - Click box â†’ Drag to new position
   - Press 'R' â†’ Rotate handle appears
   - Press 'S' â†’ Scale handle appears
   - Edit properties panel (mass, friction, etc.)
   â†“
5. SIMULATE
   Press Space â†’ Rapier physics runs at 60 FPS
   â†“
6. ITERATE
   Option A: Reset â†’ Adjust â†’ Simulate again
   Option B: Type refinement: "Make boxes heavier"
   â†“
   Loop back to step 4 or 2
```

---

## 3. Technical Architecture

### 3.1 System Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ BROWSER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚         Elm Application              â”‚            â”‚
â”‚  â”‚  - Pure state management             â”‚            â”‚
â”‚  â”‚  - UI rendering (HTML)               â”‚            â”‚
â”‚  â”‚  - Business logic                    â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜            â”‚
â”‚       â”‚ Ports                       â”‚ Ports          â”‚
â”‚       â–¼                             â–¼                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Three.js    â”‚â—€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Rapier Physics   â”‚       â”‚
â”‚  â”‚ (WebGL)     â”‚         â”‚ (WASM)           â”‚       â”‚
â”‚  â”‚             â”‚         â”‚                  â”‚       â”‚
â”‚  â”‚ - Rendering â”‚         â”‚ - Simulation     â”‚       â”‚
â”‚  â”‚ - Transform â”‚         â”‚ - Collision      â”‚       â”‚
â”‚  â”‚   Controls  â”‚         â”‚ - Forces         â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚       â”‚                             â”‚                â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                     â”‚ Events                         â”‚
â”‚                     â–¼                                â”‚
â”‚         Browser APIs (IndexedDB, LocalStorage)       â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTP/JSON
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   BACKEND SERVER                        â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚         FastAPI / Axum Server               â”‚      â”‚
â”‚  â”‚                                             â”‚      â”‚
â”‚  â”‚  POST /api/generate                         â”‚      â”‚
â”‚  â”‚  POST /api/refine                           â”‚      â”‚
â”‚  â”‚  POST /api/validate                         â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                â”‚                â”‚            â”‚
â”‚         â–¼                â–¼                â–¼            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Claude   â”‚    â”‚ Genesis  â”‚    â”‚ LMDB     â”‚        â”‚
â”‚  â”‚ API      â”‚    â”‚ Validatorâ”‚    â”‚ Cache    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                         â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚                          â”‚ SQLite   â”‚                  â”‚
â”‚                          â”‚ (scenes) â”‚                  â”‚
â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Frontend Architecture (Elm + Three.js)

**Philosophy:**
- **Elm owns state** - all application logic, no mutation
- **Three.js owns rendering** - via ports, treated as side effect
- **Clear boundary** - Elm sends commands, receives events

#### Elm Application Structure

```elm
-- src/Main.elm

module Main exposing (main)

import Browser
import Json.Decode as Decode
import Json.Encode as Encode

-- MODEL

type alias Model =
    { scene : Scene
    , uiState : UiState
    , simulationState : SimulationState
    }

type alias Scene =
    { objects : Dict ObjectId PhysicsObject
    , environment : Environment
    }

type alias PhysicsObject =
    { id : ObjectId
    , objectType : ObjectType
    , transform : Transform
    , physics : PhysicsProperties
    , visual : VisualProperties
    , selected : Bool
    }

type ObjectType
    = Box Vec3
    | Sphere Float
    | Cylinder Float Float
    | Capsule Float Float

type alias Transform =
    { position : Vec3
    , rotation : Quaternion  -- (x, y, z, w)
    , scale : Vec3
    }

type alias PhysicsProperties =
    { bodyType : BodyType
    , mass : Float
    , friction : Float
    , restitution : Float
    , linearVelocity : Vec3
    , angularVelocity : Vec3
    }

type BodyType
    = Dynamic
    | Static
    | Kinematic

type alias UiState =
    { textInput : String
    , selectedObjectId : Maybe ObjectId
    , transformMode : TransformMode
    , isGenerating : Bool
    , isPanelOpen : Bool
    }

type TransformMode
    = Translate
    | Rotate
    | Scale

type alias SimulationState =
    { isRunning : Bool
    , initialStates : Dict ObjectId Transform
    , currentFrame : Int
    }

-- UPDATE

type Msg
    = -- Text Input & Generation
      UpdateTextInput String
    | GenerateScene
    | SceneGenerated (Result Http.Error Scene)
    | RefineScene String
    | SceneRefined (Result Http.Error Scene)
    
    -- Object Selection & Manipulation
    | ObjectClicked ObjectId
    | ObjectTransformUpdated ObjectId Transform
    | DeselectAll
    
    -- Transform Controls
    | SetTransformMode TransformMode
    | ToggleTransformSpace  -- World vs Local
    
    -- Property Editing
    | UpdateObjectProperty ObjectId PropertyUpdate
    | ApplyPreset ObjectId Preset
    
    -- Simulation Control
    | ToggleSimulation
    | ResetSimulation
    | StepSimulation Int  -- Frame delta from Rapier
    
    -- Keyboard/UI
    | KeyPressed Key
    | TogglePanel
    
    -- Ports (from JS)
    | ThreeJsEvent ThreeJsEventData

type PropertyUpdate
    = SetMass Float
    | SetFriction Float
    | SetRestitution Float
    | SetColor String
    | SetBodyType BodyType

type alias ThreeJsEventData =
    { eventType : String
    , objectId : Maybe ObjectId
    , transform : Maybe Transform
    , raycastHit : Maybe Vec3
    }

update : Msg -> Model -> ( Model, Cmd Msg )
update msg model =
    case msg of
        UpdateTextInput text ->
            ( { model | uiState = updateTextInput text model.uiState }
            , Cmd.none
            )
        
        GenerateScene ->
            ( { model | uiState = setGenerating True model.uiState }
            , generateSceneRequest model.uiState.textInput
            )
        
        SceneGenerated (Ok scene) ->
            ( { model 
                | scene = scene
                , uiState = setGenerating False model.uiState
                , simulationState = initSimulationState scene
              }
            , Cmd.batch
                [ sendSceneToThreeJs scene
                , saveSceneToLocalStorage scene
                ]
            )
        
        ObjectClicked objectId ->
            let
                updatedScene = selectObject objectId model.scene
            in
            ( { model 
                | scene = updatedScene
                , uiState = setSelectedObject (Just objectId) model.uiState
              }
            , sendSelectionToThreeJs objectId
            )
        
        ObjectTransformUpdated objectId transform ->
            ( { model 
                | scene = updateObjectTransform objectId transform model.scene
              }
            , Cmd.none
            )
        
        SetTransformMode mode ->
            ( { model | uiState = setTransformMode mode model.uiState }
            , sendTransformModeToThreeJs mode
            )
        
        ToggleSimulation ->
            if model.simulationState.isRunning then
                ( { model | simulationState = pauseSimulation model.simulationState }
                , sendToPhysics (Encode.object [ ("command", Encode.string "pause") ])
                )
            else
                ( { model 
                    | simulationState = startSimulation model.simulationState model.scene
                  }
                , sendToPhysics (Encode.object [ ("command", Encode.string "start") ])
                )
        
        ResetSimulation ->
            let
                resetScene = restoreInitialStates model.scene model.simulationState.initialStates
            in
            ( { model 
                | scene = resetScene
                , simulationState = resetSimulationState model.simulationState
              }
            , Cmd.batch
                [ sendToPhysics (Encode.object [ ("command", Encode.string "reset") ])
                , sendSceneToThreeJs resetScene
                ]
            )
        
        StepSimulation frameDelta ->
            -- Physics engine has updated, sync state back to Elm
            ( model, Cmd.none )
        
        ThreeJsEvent eventData ->
            -- Handle events from Three.js (clicks, drags, etc)
            handleThreeJsEvent eventData model
        
        KeyPressed key ->
            handleKeyPress key model
        
        _ ->
            ( model, Cmd.none )

-- VIEW

view : Model -> Html Msg
view model =
    div [ class "app-container" ]
        [ viewHeader
        , div [ class "main-content" ]
            [ viewLeftPanel model
            , viewCanvas  -- Just a placeholder div, Three.js renders here
            , viewRightPanel model
            ]
        , viewBottomBar model
        ]

viewLeftPanel : Model -> Html Msg
viewLeftPanel model =
    aside [ class "left-panel" ]
        [ h2 [] [ text "Generate Scene" ]
        , textarea
            [ placeholder "Describe your physics scene..."
            , value model.uiState.textInput
            , onInput UpdateTextInput
            ]
            []
        , button
            [ onClick GenerateScene
            , disabled model.uiState.isGenerating
            ]
            [ text (if model.uiState.isGenerating then "Generating..." else "Generate") ]
        , hr [] []
        , h2 [] [ text "Refine Scene" ]
        , textarea [ placeholder "Modify with text..." ] []
        , button [ onClick (RefineScene "") ] [ text "Refine" ]
        ]

viewRightPanel : Model -> Html Msg
viewRightPanel model =
    aside [ class "right-panel" ]
        [ h2 [] [ text "Properties" ]
        , case model.uiState.selectedObjectId of
            Just objectId ->
                case Dict.get objectId model.scene.objects of
                    Just obj ->
                        viewObjectProperties obj
                    Nothing ->
                        text "Object not found"
            Nothing ->
                text "No object selected"
        ]

viewObjectProperties : PhysicsObject -> Html Msg
viewObjectProperties obj =
    div [ class "properties-panel" ]
        [ section []
            [ h3 [] [ text "Transform" ]
            , viewVec3Input "Position" obj.transform.position
            , viewQuaternionInput "Rotation" obj.transform.rotation
            , viewVec3Input "Scale" obj.transform.scale
            ]
        , section []
            [ h3 [] [ text "Physics" ]
            , viewSlider "Mass" obj.physics.mass 0.01 1000 (UpdateObjectProperty obj.id << SetMass)
            , viewSlider "Friction" obj.physics.friction 0 2 (UpdateObjectProperty obj.id << SetFriction)
            , viewSlider "Restitution" obj.physics.restitution 0 1 (UpdateObjectProperty obj.id << SetRestitution)
            , viewBodyTypeSelect obj.physics.bodyType
            ]
        , section []
            [ h3 [] [ text "Visual" ]
            , viewColorPicker obj.visual.color
            , viewSlider "Metalness" obj.visual.metallic 0 1 identity
            , viewSlider "Roughness" obj.visual.roughness 0 1 identity
            ]
        ]

viewBottomBar : Model -> Html Msg
viewBottomBar model =
    footer [ class "bottom-bar" ]
        [ button [ onClick ToggleSimulation ]
            [ text (if model.simulationState.isRunning then "â¸ Pause" else "â–¶ Play") ]
        , button [ onClick ResetSimulation ] [ text "â†» Reset" ]
        , div [ class "stats" ]
            [ text ("Objects: " ++ String.fromInt (Dict.size model.scene.objects))
            , text " | "
            , text ("Frame: " ++ String.fromInt model.simulationState.currentFrame)
            ]
        ]

-- PORTS

port sendSceneToThreeJs : Encode.Value -> Cmd msg
port sendSelectionToThreeJs : String -> Cmd msg
port sendTransformModeToThreeJs : String -> Cmd msg
port sendToPhysics : Encode.Value -> Cmd msg

port receiveFromThreeJs : (Decode.Value -> msg) -> Sub msg
port receiveFromPhysics : (Decode.Value -> msg) -> Sub msg

-- SUBSCRIPTIONS

subscriptions : Model -> Sub Msg
subscriptions model =
    Sub.batch
        [ receiveFromThreeJs (decodeThreeJsEvent >> ThreeJsEvent)
        , receiveFromPhysics (decodePhysicsUpdate >> StepSimulation)
        , Browser.Events.onKeyDown (Decode.map KeyPressed keyDecoder)
        ]

-- MAIN

main : Program () Model Msg
main =
    Browser.element
        { init = init
        , view = view
        , update = update
        , subscriptions = subscriptions
        }
```

#### JavaScript/Three.js Bridge

```javascript
// src/index.js (Vite entry point)

import { Elm } from './Main.elm'
import * as THREE from 'three'
import { OrbitControls } from 'three/examples/jsm/controls/OrbitControls'
import { TransformControls } from 'three/examples/jsm/controls/TransformControls'
import RAPIER from '@dimforge/rapier3d-compat'

// Initialize Elm
const app = Elm.Main.init({
  node: document.getElementById('app'),
  flags: null
})

// Three.js setup
class PhysicsRenderer {
  constructor(containerId) {
    this.container = document.getElementById(containerId)
    this.scene = new THREE.Scene()
    this.objects = new Map() // objectId -> {mesh, body, collider}
    
    this.setupRenderer()
    this.setupCamera()
    this.setupControls()
    this.setupLights()
    this.setupPhysics()
    
    this.animate()
  }
  
  async setupPhysics() {
    await RAPIER.init()
    this.world = new RAPIER.World({ x: 0, y: -9.81, z: 0 })
    this.isSimulating = false
  }
  
  setupRenderer() {
    this.renderer = new THREE.WebGLRenderer({ antialias: true })
    this.renderer.setSize(window.innerWidth, window.innerHeight)
    this.renderer.shadowMap.enabled = true
    this.container.appendChild(this.renderer.domElement)
  }
  
  setupCamera() {
    this.camera = new THREE.PerspectiveCamera(
      75, 
      window.innerWidth / window.innerHeight, 
      0.1, 
      1000
    )
    this.camera.position.set(5, 5, 5)
  }
  
  setupControls() {
    this.orbitControls = new OrbitControls(this.camera, this.renderer.domElement)
    
    this.transformControls = new TransformControls(this.camera, this.renderer.domElement)
    this.transformControls.addEventListener('dragging-changed', (event) => {
      this.orbitControls.enabled = !event.value
    })
    
    this.transformControls.addEventListener('objectChange', () => {
      if (this.transformControls.object) {
        const obj = this.transformControls.object
        app.ports.receiveFromThreeJs.send({
          type: 'transformUpdate',
          objectId: obj.userData.id,
          position: obj.position.toArray(),
          rotation: obj.quaternion.toArray(),
          scale: obj.scale.toArray()
        })
      }
    })
    
    this.scene.add(this.transformControls)
    
    // Raycasting for selection
    this.raycaster = new THREE.Raycaster()
    this.mouse = new THREE.Vector2()
    
    this.renderer.domElement.addEventListener('click', (e) => {
      this.mouse.x = (e.clientX / window.innerWidth) * 2 - 1
      this.mouse.y = -(e.clientY / window.innerHeight) * 2 + 1
      
      this.raycaster.setFromCamera(this.mouse, this.camera)
      const intersects = this.raycaster.intersectObjects(
        Array.from(this.objects.values()).map(o => o.mesh)
      )
      
      if (intersects.length > 0) {
        const objectId = intersects[0].object.userData.id
        app.ports.receiveFromThreeJs.send({
          type: 'objectClicked',
          objectId: objectId
        })
      }
    })
  }
  
  setupLights() {
    const ambientLight = new THREE.AmbientLight(0xffffff, 0.6)
    this.scene.add(ambientLight)
    
    const dirLight = new THREE.DirectionalLight(0xffffff, 0.8)
    dirLight.position.set(10, 20, 10)
    dirLight.castShadow = true
    this.scene.add(dirLight)
    
    // Ground
    const groundGeometry = new THREE.PlaneGeometry(100, 100)
    const groundMaterial = new THREE.MeshStandardMaterial({ color: 0x808080 })
    const ground = new THREE.Mesh(groundGeometry, groundMaterial)
    ground.rotation.x = -Math.PI / 2
    ground.receiveShadow = true
    this.scene.add(ground)
    
    // Physics ground
    const groundBodyDesc = RAPIER.RigidBodyDesc.fixed()
    const groundBody = this.world.createRigidBody(groundBodyDesc)
    const groundColliderDesc = RAPIER.ColliderDesc.cuboid(50, 0.1, 50)
    this.world.createCollider(groundColliderDesc, groundBody)
  }
  
  loadScene(sceneData) {
    // Clear existing
    this.clearScene()
    
    // Add objects from Elm
    for (const obj of sceneData.objects) {
      this.addObject(obj)
    }
  }
  
  addObject(objDesc) {
    // Create Three.js mesh
    let geometry
    switch(objDesc.type) {
      case 'Box':
        geometry = new THREE.BoxGeometry(...objDesc.size)
        break
      case 'Sphere':
        geometry = new THREE.SphereGeometry(objDesc.radius, 32, 32)
        break
      case 'Cylinder':
        geometry = new THREE.CylinderGeometry(objDesc.radius, objDesc.radius, objDesc.height, 32)
        break
    }
    
    const material = new THREE.MeshStandardMaterial({
      color: objDesc.visual.color,
      metalness: objDesc.visual.metallic,
      roughness: objDesc.visual.roughness
    })
    
    const mesh = new THREE.Mesh(geometry, material)
    mesh.position.set(...objDesc.transform.position)
    mesh.quaternion.set(...objDesc.transform.rotation)
    mesh.scale.set(...objDesc.transform.scale)
    mesh.castShadow = true
    mesh.receiveShadow = true
    mesh.userData.id = objDesc.id
    
    this.scene.add(mesh)
    
    // Create Rapier body
    const rigidBodyDesc = RAPIER.RigidBodyDesc.dynamic()
      .setTranslation(...objDesc.transform.position)
      .setRotation({
        w: objDesc.transform.rotation[3],
        x: objDesc.transform.rotation[0],
        y: objDesc.transform.rotation[1],
        z: objDesc.transform.rotation[2]
      })
    
    const body = this.world.createRigidBody(rigidBodyDesc)
    
    // Create collider
    let colliderDesc
    switch(objDesc.type) {
      case 'Box':
        colliderDesc = RAPIER.ColliderDesc.cuboid(
          objDesc.size[0] / 2,
          objDesc.size[1] / 2,
          objDesc.size[2] / 2
        )
        break
      case 'Sphere':
        colliderDesc = RAPIER.ColliderDesc.ball(objDesc.radius)
        break
    }
    
    colliderDesc
      .setMass(objDesc.physics.mass)
      .setRestitution(objDesc.physics.restitution)
      .setFriction(objDesc.physics.friction)
    
    const collider = this.world.createCollider(colliderDesc, body)
    
    this.objects.set(objDesc.id, { mesh, body, collider })
  }
  
  selectObject(objectId) {
    const obj = this.objects.get(objectId)
    if (obj) {
      this.transformControls.attach(obj.mesh)
    }
  }
  
  setTransformMode(mode) {
    const modeMap = {
      'Translate': 'translate',
      'Rotate': 'rotate',
      'Scale': 'scale'
    }
    this.transformControls.setMode(modeMap[mode])
  }
  
  startSimulation() {
    this.isSimulating = true
    this.transformControls.detach()
  }
  
  pauseSimulation() {
    this.isSimulating = false
  }
  
  resetSimulation() {
    this.isSimulating = false
    // Physics bodies will be reset by reloading scene from Elm
  }
  
  animate() {
    requestAnimationFrame(() => this.animate())
    
    if (this.isSimulating) {
      this.world.step()
      
      // Sync Three.js meshes with Rapier bodies
      for (const [id, obj] of this.objects) {
        const pos = obj.body.translation()
        const rot = obj.body.rotation()
        
        obj.mesh.position.set(pos.x, pos.y, pos.z)
        obj.mesh.quaternion.set(rot.x, rot.y, rot.z, rot.w)
      }
      
      // Send frame update to Elm
      app.ports.receiveFromPhysics.send({
        frame: this.currentFrame++
      })
    }
    
    this.orbitControls.update()
    this.renderer.render(this.scene, this.camera)
  }
  
  clearScene() {
    for (const [id, obj] of this.objects) {
      this.scene.remove(obj.mesh)
      this.world.removeRigidBody(obj.body)
    }
    this.objects.clear()
  }
}

// Initialize renderer
const renderer = new PhysicsRenderer('canvas-container')

// Connect Elm ports
app.ports.sendSceneToThreeJs.subscribe((sceneData) => {
  renderer.loadScene(sceneData)
})

app.ports.sendSelectionToThreeJs.subscribe((objectId) => {
  renderer.selectObject(objectId)
})

app.ports.sendTransformModeToThreeJs.subscribe((mode) => {
  renderer.setTransformMode(mode)
})

app.ports.sendToPhysics.subscribe((command) => {
  switch(command.command) {
    case 'start':
      renderer.startSimulation()
      break
    case 'pause':
      renderer.pauseSimulation()
      break
    case 'reset':
      renderer.resetSimulation()
      break
  }
})
```

#### Build Configuration (Vite)

```javascript
// vite.config.js

import { defineConfig } from 'vite'
import { plugin as elm } from 'vite-plugin-elm'

export default defineConfig({
  plugins: [
    elm({
      debug: true,
      optimize: process.env.NODE_ENV === 'production'
    })
  ],
  optimizeDeps: {
    exclude: ['@dimforge/rapier3d-compat']
  },
  server: {
    proxy: {
      '/api': {
        target: 'http://localhost:8000',
        changeOrigin: true
      }
    }
  }
})
```

```json
// package.json

{
  "name": "physics-playground",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "preview": "vite preview"
  },
  "dependencies": {
    "three": "^0.160.0",
    "@dimforge/rapier3d-compat": "^0.12.0"
  },
  "devDependencies": {
    "vite": "^5.0.0",
    "vite-plugin-elm": "^3.0.0",
    "elm": "^0.19.1"
  }
}
```

```html
<!-- index.html -->

<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>PhysicsPlayground</title>
  <style>
    body { 
      margin: 0; 
      overflow: hidden; 
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
    }
    
    .app-container {
      display: flex;
      flex-direction: column;
      height: 100vh;
    }
    
    .main-content {
      display: flex;
      flex: 1;
      overflow: hidden;
    }
    
    .left-panel, .right-panel {
      width: 300px;
      background: #1e293b;
      color: #f1f5f9;
      padding: 20px;
      overflow-y: auto;
    }
    
    #canvas-container {
      flex: 1;
      position: relative;
    }
    
    .bottom-bar {
      display: flex;
      gap: 10px;
      padding: 15px;
      background: #0f172a;
      color: #f1f5f9;
      align-items: center;
    }
    
    button {
      padding: 10px 20px;
      background: #3b82f6;
      color: white;
      border: none;
      border-radius: 6px;
      cursor: pointer;
      font-size: 14px;
    }
    
    button:hover {
      background: #2563eb;
    }
    
    button:disabled {
      background: #64748b;
      cursor: not-allowed;
    }
    
    textarea {
      width: 100%;
      min-height: 100px;
      padding: 10px;
      background: #334155;
      color: #f1f5f9;
      border: 1px solid #475569;
      border-radius: 6px;
      font-family: inherit;
      font-size: 14px;
      resize: vertical;
    }
    
    input[type="range"] {
      width: 100%;
    }
  </style>
</head>
<body>
  <div id="app"></div>
  <div id="canvas-container"></div>
  <script type="module" src="/src/index.js"></script>
</body>
</html>
```

### 3.3 Backend Architecture

**Tech Stack:**
- **Language:** Python 3.11+ (FastAPI) - chosen for simplicity and native Genesis integration
- **Physics Validation:** Genesis Python bindings
- **AI:** Anthropic Python SDK / reqwest for Rust
- **Database:** SQLite (scenes, history)
- **Cache:** LMDB or Redis (scene generation cache)

#### Python/FastAPI Implementation

```python
# backend/main.py

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Literal
import anthropic
import genesis as gs
import json
import hashlib
import lmdb

app = FastAPI()

# CORS for local dev
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],  # Vite dev server
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize clients
ai_client = get_ai_client()  # Configurable AI provider (e.g., via env vars: Claude, Grok, etc.)
cache_env = lmdb.open('./cache_db', map_size=10**9)

# Models

class Vec3(BaseModel):
    x: float
    y: float
    z: float

class Quaternion(BaseModel):
    x: float
    y: float
    z: float
    w: float

class Transform(BaseModel):
    position: Vec3
    rotation: Quaternion
    scale: Vec3

class PhysicsProperties(BaseModel):
    bodyType: Literal["Dynamic", "Static", "Kinematic"]
    mass: float
    friction: float
    restitution: float
    linearVelocity: Optional[Vec3] = None
    angularVelocity: Optional[Vec3] = None

class VisualProperties(BaseModel):
    color: str
    metallic: float
    roughness: float

class PhysicsObject(BaseModel):
    id: str
    type: Literal["Box", "Sphere", "Cylinder", "Capsule"]
    size: Optional[List[float]] = None
    radius: Optional[float] = None
    height: Optional[float] = None
    transform: Transform
    physics: PhysicsProperties
    visual: VisualProperties

class Environment(BaseModel):
    gravity: Vec3
    ground: bool

class Scene(BaseModel):
    objects: List[PhysicsObject]
    environment: Environment

class GenerateRequest(BaseModel):
    text: str

class RefineRequest(BaseModel):
    scene: Scene
    instruction: str

# Scene Generation

SCENE_SCHEMA = """
{
  "objects": [
    {
      "id": "unique_string",
      "type": "Box" | "Sphere" | "Cylinder" | "Capsule",
      "size": [width, height, depth],  // for Box
      "radius": float,  // for Sphere, Cylinder
      "height": float,  // for Cylinder
      "transform": {
        "position": {"x": float, "y": float, "z": float},
        "rotation": {"x": float, "y": float, "z": float, "w": float},
        "scale": {"x": 1.0, "y": 1.0, "z": 1.0}
      },
      "physics": {
        "bodyType": "Dynamic" | "Static" | "Kinematic",
        "mass": float,
        "friction": float,
        "restitution": float
      },
      "visual": {
        "color": "#RRGGBB",
        "metallic": float,
        "roughness": float
      }
    }
  ],
  "environment": {
    "gravity": {"x": 0, "y": -9.81, "z": 0},
    "ground": true
  }
}
"""

GENERATION_PROMPT_TEMPLATE = """You are a physics scene generator. Create a realistic physics scene from this description.

USER DESCRIPTION: "{text}"

Output ONLY valid JSON matching this schema:
{schema}

RULES:
1. Objects must not overlap (check positions and sizes)
2. All objects must be above ground (y â‰¥ 0)
3. Use realistic masses: small objects ~1kg, scale appropriately
4. Default friction: 0.5, restitution: 0.3
5. Max 20 objects for performance
6. Generate unique IDs (box1, sphere1, etc.)
7. Include ground unless explicitly excluded

PHYSICAL INTUITION:
- "Heavy" = 10-100x normal mass
- "Bouncy" = restitution 0.7-0.9
- "Slippery" = friction 0.05-0.15
- "Stack" = align centers vertically, touching
- "Scatter" = randomize in 5x5 area

Output ONLY the JSON, no explanation."""

@app.post("/api/generate")
async def generate_scene(request: GenerateRequest):
    # Check cache first
    cache_key = hashlib.sha256(request.text.encode()).digest()
    
    with cache_env.begin() as txn:
        cached = txn.get(cache_key)
        if cached:
            return json.loads(cached)
    
    # Generate with Claude
    try:
        response = ai_client.messages.create(
            model=os.getenv("AI_MODEL", "default-model"),  # Configurable model
            max_tokens=4000,
            messages=[{
                "role": "user",
                "content": GENERATION_PROMPT_TEMPLATE.format(
                    text=request.text,
                    schema=SCENE_SCHEMA
                )
            }]
        )
        
        scene_json = response.content[0].text
        scene_data = json.loads(scene_json)
        scene = Scene(**scene_data)
        
        # Validate with Genesis
        validation_result = validate_with_genesis(scene)
        
        if not validation_result["valid"]:
            raise HTTPException(
                status_code=400,
                detail=f"Physics validation failed: {validation_result['error']}"
            )
        
        # Cache successful generation
        with cache_env.begin(write=True) as txn:
            txn.put(cache_key, json.dumps(scene_data).encode())
        
        return scene_data
        
    except json.JSONDecodeError:
        raise HTTPException(status_code=500, detail="Failed to parse Claude response")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

def validate_with_genesis(scene: Scene) -> dict:
    """Quick physics validation - simulate 1 second"""
    try:
        gs_scene = gs.Scene(show_viewer=False)
        
        for obj in scene.objects:
            if obj.type == "Box":
                morph = gs.morphs.Box(size=obj.size)
            elif obj.type == "Sphere":
                morph = gs.morphs.Sphere(radius=obj.radius)
            else:
                continue  # Skip unsupported types for now
            
            gs_scene.add_entity(
                morph=morph,
                material=gs.materials.Rigid(
                    rho=obj.physics.mass,
                    restitution=obj.physics.restitution,
                    friction=obj.physics.friction
                ),
                pos=(obj.transform.position.x, obj.transform.position.y, obj.transform.position.z)
            )
        
        gs_scene.build()
        
        # Simulate 60 frames
        for _ in range(60):
            gs_scene.step()
        
        # Check for instability (objects flying off, NaN, etc.)
        # This is simplified - you'd want more robust checks
        
        return {"valid": True, "error": None}
        
    except Exception as e:
        return {"valid": False, "error": str(e)}

@app.post("/api/refine")
async def refine_scene(request: RefineRequest):
    """Modify existing scene based on text instruction"""
    
    prompt = f"""You are modifying a physics scene.

CURRENT SCENE:
{request.scene.json()}

USER INSTRUCTION: "{request.instruction}"

Output the COMPLETE modified scene as JSON (same schema as before).
Apply the requested changes while preserving all other objects.

Output ONLY the JSON."""

    try:
        response = ai_client.messages.create(
            model=os.getenv("AI_MODEL", "default-model"),  # Configurable model
            max_tokens=4000,
            messages=[{
                "role": "user",
                "content": prompt
            }]
        )
        
        refined_json = response.content[0].text
        refined_data = json.loads(refined_json)
        refined_scene = Scene(**refined_data)
        
        return refined_data
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/validate")
async def validate_scene(scene: Scene):
    """Just validate a scene without generating"""
    result = validate_with_genesis(scene)
    return result

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

```python
# requirements.txt

fastapi==0.109.0
uvicorn[standard]==0.27.0
# AI provider SDKs configured via env vars
genesis-world==0.3.7
pydantic==2.6.0
lmdb==1.4.1
```

---

## 4. Data Models

### 4.1 Scene Schema (Shared between Elm & Backend)

```elm
-- Elm types (already shown above)
```

```json
// JSON Wire Format

{
  "objects": [
    {
      "id": "box1",
      "type": "Box",
      "size": [1.0, 1.0, 1.0],
      "transform": {
        "position": {"x": 0.0, "y": 5.0, "z": 0.0},
        "rotation": {"x": 0.0, "y": 0.0, "z": 0.0, "w": 1.0},
        "scale": {"x": 1.0, "y": 1.0, "z": 1.0}
      },
      "physics": {
        "bodyType": "Dynamic",
        "mass": 1.0,
        "friction": 0.5,
        "restitution": 0.3,
        "linearVelocity": null,
        "angularVelocity": null
      },
      "visual": {
        "color": "#ff6b6b",
        "metallic": 0.1,
        "roughness": 0.7
      }
    }
  ],
  "environment": {
    "gravity": {"x": 0.0, "y": -9.81, "z": 0.0},
    "ground": true
  }
}
```

### 4.2 SQLite Schema

```sql
-- scenes table
CREATE TABLE scenes (
    id TEXT PRIMARY KEY,
    name TEXT,
    description TEXT,  -- Original prompt
    scene_json TEXT,   -- Full scene as JSON
    created_at INTEGER,
    updated_at INTEGER
);

CREATE INDEX idx_scenes_created ON scenes(created_at);

-- refinement_history table
CREATE TABLE refinement_history (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    scene_id TEXT,
    instruction TEXT,
    timestamp INTEGER,
    before_json TEXT,
    after_json TEXT,
    FOREIGN KEY (scene_id) REFERENCES scenes(id)
);

CREATE INDEX idx_refinements_scene ON refinement_history(scene_id);
```

---

## 5. Key Features & User Stories

### 5.1 Core Features

**Must Have (MVP):**
1. Text â†’ Scene generation (<10s)
2. Object selection (click to select)
3. Transform controls (translate/rotate/scale via keyboard: G/R/S)
4. Property editing panel (mass, friction, restitution, color)
5. Play/pause/reset simulation
6. Camera orbit controls
7. Export scene as JSON

**Should Have (v1.0):**
8. AI refinement ("make boxes heavier")
9. Undo/redo (Elm makes this trivial)
10. Local scene save/load (browser IndexedDB)
11. Keyboard shortcuts
12. Object presets (bouncy ball, heavy crate, etc.)

**Nice to Have (v1.1+):**
13. Video export (MP4 recording)
14. URDF/MJCF export
15. Custom mesh import (OBJ files)
16. Constraints (hinges, springs)
17. Multiple scene tabs

### 5.2 User Stories

```
As a user, I want to:

1. Describe a scene in plain English
   - So I don't need to learn physics engine APIs
   - Acceptance: Scene generates in <10s

2. Click any object to select it
   - So I can manipulate individual objects
   - Acceptance: Outline appears, properties panel updates

3. Drag objects to reposition them
   - So I can adjust the scene layout
   - Acceptance: Press G, drag with mouse, object moves in real-time

4. Rotate objects with visual handles
   - So I can orient objects precisely
   - Acceptance: Press R, circular handles appear, mouse drag rotates

5. Change object mass via slider
   - So I can test different weight scenarios
   - Acceptance: Slider updates, physics updates on next simulation

6. Press spacebar to simulate
   - So I can quickly preview physics
   - Acceptance: Physics runs at 60 FPS, objects move realistically

7. Reset simulation to initial state
   - So I can try again with same setup
   - Acceptance: All objects return to positions before play

8. Refine scene with text
   - So I can iterate without manual editing
   - Acceptance: "Make all boxes red" changes colors

9. Undo my last change
   - So I can experiment freely
   - Acceptance: Ctrl+Z reverts last action

10. Export scene as JSON
    - So I can use it in other tools
    - Acceptance: Downloads valid JSON matching schema
```

---

## 6. Non-Functional Requirements

### Performance Targets

| Metric | Target |
|--------|--------|
| Text â†’ Scene generation | <10s (p95) |
| Scene refinement | <5s (p95) |
| Initial page load | <2s |
| Physics simulation FPS (20 objects) | 60 FPS |
| UI responsiveness | <16ms (60 FPS) |
| Transform control lag | <100ms |

### Browser Support

- Chrome/Edge 90+
- Firefox 88+
- Safari 14+
- Requires: WebGL 2.0, WebAssembly

### Reliability

- Elm's type system prevents runtime errors in UI logic
- Genesis validation catches unstable physics
- Graceful degradation: template fallback if Claude fails

---

## 7. Development Phases

### Phase 1: Foundation (Week 1-2)

**Elm Setup:**
- [ ] Basic Elm app structure
- [ ] Scene model types
- [ ] UI layout (3-panel)
- [ ] Ports defined

**Three.js/JavaScript:**
- [ ] Three.js renderer setup
- [ ] Rapier WASM integration
- [ ] Basic object rendering (box, sphere, ground)
- [ ] Camera orbit controls
- [ ] Port communication with Elm

**Backend:**
- [ ] FastAPI server setup
- [ ] Claude API integration
- [ ] `/api/generate` endpoint
- [ ] Genesis validation

**Deliverable:** Can type "red box", see it render, orbit camera

### Phase 2: Interaction (Week 3-4)

**Elm:**
- [ ] Object selection logic
- [ ] Property editing panel
- [ ] Transform mode switching (G/R/S)
- [ ] Simulation state management

**Three.js:**
- [ ] Raycasting for selection
- [ ] TransformControls integration
- [ ] Selection highlighting
- [ ] Physics simulation loop

**Deliverable:** Can select, move, rotate objects; press space to simulate

### Phase 3: AI Refinement (Week 5)

**Elm:**
- [ ] Refinement text input
- [ ] Scene diff visualization (what changed)

**Backend:**
- [ ] `/api/refine` endpoint
- [ ] Prompt engineering for modifications
- [ ] Cache layer (LMDB)

**Deliverable:** Can refine scene with "make boxes bigger"

### Phase 4: Polish (Week 6)

**Elm:**
- [ ] Undo/redo system
- [ ] Keyboard shortcuts
- [ ] Local storage (IndexedDB)
- [ ] Error handling & loading states

**Three.js:**
- [ ] Better lighting/shadows
- [ ] Ground grid
- [ ] Visual polish

**Backend:**
- [ ] SQLite scene storage
- [ ] Better error messages

**Deliverable:** Public beta ready

---

## 8. Open Questions

1. **Animation/Interpolation Handling:** JS handles all visual interpolation, Elm tracks discrete state.

2. **Initial States Storage:** Elm stores initial states and sends to JS on reset (Elm as source of truth).

3. **Undo/Redo with Active Simulation:** Undo auto-pauses simulation, reverts state, resets physics; history stored in Elm.

4. **Mobile Support in v1:** No - desktop-first, defer mobile to later versions.

5. **Export Formats Priority:** Defer decision; MVP: JSON only, evaluate others post-MVP.

---

## 9. Success Metrics

**MVP Success:**
- 10 users can generate â†’ edit â†’ simulate â†’ export
- Average iteration time <30 seconds
- <5 crashes/errors per user session

**v1.0 Success:**
- 100 MAU
- Average 5 iterations per session
- 90%+ scene generation success rate
- Users self-report "faster than Unity/Blender"

---

## 10. What We're NOT Building (v1.0)

- Soft body simulation
- Fluid simulation  
- Custom shaders/materials
- Multiplayer/collaboration
- User accounts (MVP is local-only)
- Mobile app
- VR/AR support
- Video export
- Mesh editing
- Animation timeline
- Scripting API

These may come later, but out of scope for MVP.

---

**Next Step:** Start with Phase 1, beginning with Elm app scaffold and basic Three.js integration via ports.
</file>

<file path=".taskmaster/docs/prd-part2.md">
# Physics Sim â†’ Veo 3.1 Pipeline

Perfect! Veo 3.1 is arguably the best model for this. Let me show you the actual integration.

---

## 1. Veo 3.1 API Integration

### 1.1 Setup (Google Vertex AI)

```python
# backend/integrations/veo.py

from google.cloud import aiplatform
from google.oauth2 import service_account
import base64
from typing import Optional, List
import asyncio

class Veo31Client:
    def __init__(
        self, 
        project_id: str,
        location: str = "us-central1",
        credentials_path: Optional[str] = None
    ):
        self.project_id = project_id
        self.location = location
        
        if credentials_path:
            credentials = service_account.Credentials.from_service_account_file(
                credentials_path
            )
            aiplatform.init(
                project=project_id,
                location=location,
                credentials=credentials
            )
        else:
            aiplatform.init(project=project_id, location=location)
        
        self.client = aiplatform.gapic.PredictionServiceClient(
            client_options={"api_endpoint": f"{location}-aiplatform.googleapis.com"}
        )
    
    async def generate_from_reference(
        self,
        prompt: str,
        reference_video_path: Optional[str] = None,
        reference_image_path: Optional[str] = None,
        duration: int = 5,
        resolution: str = "1080p",
        aspect_ratio: str = "16:9",
        motion_strength: float = 0.8,
        style_strength: float = 0.7
    ) -> dict:
        """
        Generate video with Veo 3.1
        
        Args:
            prompt: Text description
            reference_video_path: Path to physics sim video (for motion reference)
            reference_image_path: Path to style reference image
            duration: Duration in seconds (up to 20s with Veo 3.1)
            resolution: "480p", "720p", "1080p"
            motion_strength: 0.0-1.0, how much to preserve reference motion
            style_strength: 0.0-1.0, how much to preserve reference style
        """
        
        # Prepare inputs
        instances = [{
            "prompt": prompt,
            "parameters": {
                "duration": duration,
                "resolution": resolution,
                "aspect_ratio": aspect_ratio,
                "fps": 30,  # Veo 3.1 outputs at 30fps
            }
        }]
        
        # Add reference video if provided
        if reference_video_path:
            with open(reference_video_path, 'rb') as f:
                video_bytes = f.read()
                video_b64 = base64.b64encode(video_bytes).decode('utf-8')
            
            instances[0]["reference_video"] = {
                "video_bytes": video_b64,
                "motion_guidance_strength": motion_strength
            }
        
        # Add reference image if provided (for style)
        if reference_image_path:
            with open(reference_image_path, 'rb') as f:
                image_bytes = f.read()
                image_b64 = base64.b64encode(image_bytes).decode('utf-8')
            
            instances[0]["reference_image"] = {
                "image_bytes": image_b64,
                "style_strength": style_strength
            }
        
        # Call Veo 3.1 API
        endpoint = f"projects/{self.project_id}/locations/{self.location}/publishers/google/models/veo-003"
        
        response = await asyncio.to_thread(
            self.client.predict,
            endpoint=endpoint,
            instances=instances
        )
        
        # Parse response
        result = response.predictions[0]
        
        return {
            "video_url": result.get("gcs_uri"),  # GCS path to output
            "video_bytes": result.get("video_bytes"),  # Or direct bytes
            "generation_id": result.get("generation_id")
        }
    
    async def generate_with_keyframes(
        self,
        prompt: str,
        keyframes: List[dict],
        duration: int = 5
    ) -> dict:
        """
        Generate video using keyframe guidance
        
        Keyframes: List of {"timestamp": float, "image_path": str}
        """
        
        keyframe_inputs = []
        for kf in keyframes:
            with open(kf["image_path"], 'rb') as f:
                img_bytes = f.read()
                img_b64 = base64.b64encode(img_bytes).decode('utf-8')
            
            keyframe_inputs.append({
                "timestamp_seconds": kf["timestamp"],
                "image_bytes": img_b64
            })
        
        instances = [{
            "prompt": prompt,
            "keyframes": keyframe_inputs,
            "parameters": {
                "duration": duration,
                "resolution": "1080p"
            }
        }]
        
        endpoint = f"projects/{self.project_id}/locations/{self.location}/publishers/google/models/veo-003"
        
        response = await asyncio.to_thread(
            self.client.predict,
            endpoint=endpoint,
            instances=instances
        )
        
        result = response.predictions[0]
        return {
            "video_url": result.get("gcs_uri"),
            "video_bytes": result.get("video_bytes")
        }
```

---

## 2. Optimized Reference Generation for Veo

### 2.1 Keyframe Extraction Strategy

**Veo 3.1 works REALLY well with keyframes** - you can give it key poses from your physics sim:

```python
# backend/veo_optimizer.py

class VeoPhysicsOptimizer:
    """Optimize physics sim output specifically for Veo 3.1"""
    
    def __init__(self):
        self.renderer = SceneRenderer(resolution=(1920, 1080))
    
    def extract_keyframes(
        self,
        scene: Scene,
        num_keyframes: int = 5,
        strategy: str = "uniform"  # uniform, critical_moments, velocity_peaks
    ) -> List[dict]:
        """
        Extract key frames from physics simulation
        
        Strategies:
        - uniform: Evenly spaced frames
        - critical_moments: Impact, apex, rest points
        - velocity_peaks: High velocity moments
        """
        
        gs_scene = self._build_genesis_scene(scene)
        
        # Simulate and track
        frames = []
        velocities = []
        
        for frame_idx in range(300):
            gs_scene.step()
            
            # Render frame
            frame = gs_scene.render(rgb=True)
            frames.append(frame)
            
            # Track velocities for critical moments
            max_vel = max(
                entity.get_vel().magnitude() 
                for entity in gs_scene.entities.values()
            )
            velocities.append(max_vel)
        
        # Select keyframes based on strategy
        if strategy == "uniform":
            indices = np.linspace(0, len(frames)-1, num_keyframes, dtype=int)
        
        elif strategy == "critical_moments":
            indices = self._find_critical_moments(velocities, num_keyframes)
        
        elif strategy == "velocity_peaks":
            indices = self._find_velocity_peaks(velocities, num_keyframes)
        
        # Extract and save keyframes
        keyframes = []
        for idx in indices:
            timestamp = idx / 60.0  # 60 fps
            keyframe_path = f"./temp/keyframe_{idx:04d}.png"
            
            # Save as high-quality PNG
            cv2.imwrite(
                keyframe_path,
                cv2.cvtColor(frames[idx], cv2.COLOR_RGB2BGR),
                [cv2.IMWRITE_PNG_COMPRESSION, 0]  # No compression
            )
            
            keyframes.append({
                "timestamp": timestamp,
                "image_path": keyframe_path,
                "frame_index": idx
            })
        
        return keyframes
    
    def _find_critical_moments(self, velocities: List[float], num_keyframes: int) -> List[int]:
        """
        Find critical moments: start, impacts, apex, end
        """
        
        critical_indices = [0]  # Start
        
        # Find impacts (sudden deceleration)
        velocity_changes = np.diff(velocities)
        impact_indices = np.where(velocity_changes < -5.0)[0]  # Threshold
        
        # Find apex (low velocity after high)
        apex_candidates = []
        for i in range(1, len(velocities)-1):
            if velocities[i-1] > 2.0 and velocities[i] < 1.0:
                apex_candidates.append(i)
        
        # Combine and sample
        candidates = sorted(set(list(impact_indices) + apex_candidates))
        
        if len(candidates) > num_keyframes - 2:
            # Sample evenly from candidates
            step = len(candidates) // (num_keyframes - 2)
            critical_indices.extend(candidates[::step][:(num_keyframes-2)])
        else:
            critical_indices.extend(candidates)
        
        critical_indices.append(len(velocities) - 1)  # End
        
        return sorted(critical_indices)[:num_keyframes]
    
    def generate_stylized_reference(
        self,
        scene: Scene,
        output_path: str,
        style: str = "clean_cgi"
    ) -> str:
        """
        Generate reference video optimized for Veo
        
        Styles:
        - clean_cgi: Clean, well-lit CGI render (best for Veo)
        - flat_color: Flat design, simple (good for understanding motion)
        - depth_enhanced: Enhanced depth cues
        """
        
        gs_scene = self._build_genesis_scene(scene)
        
        if style == "clean_cgi":
            # Use good lighting, clean materials
            self._setup_studio_lighting(gs_scene)
        
        frames = []
        for frame_idx in range(300):
            gs_scene.step()
            
            if style == "clean_cgi":
                frame = gs_scene.render(rgb=True)
            
            elif style == "flat_color":
                frame = self._render_flat(gs_scene)
            
            elif style == "depth_enhanced":
                rgb = gs_scene.render(rgb=True)
                depth = gs_scene.render(depth=True)
                frame = self._enhance_with_depth(rgb, depth)
            
            frames.append(frame)
        
        # Encode at high quality (Veo prefers good input)
        video_path = f"{output_path}_reference.mp4"
        self._encode_high_quality(frames, video_path)
        
        return video_path
    
    def _encode_high_quality(self, frames: List[np.ndarray], output_path: str):
        """Encode at high bitrate for Veo input"""
        
        h, w = frames[0].shape[:2]
        
        # Use H.264 high profile, high bitrate
        fourcc = cv2.VideoWriter_fourcc(*'avc1')
        writer = cv2.VideoWriter(
            output_path, 
            fourcc, 
            30,  # Veo prefers 30fps input
            (w, h)
        )
        
        for frame in frames:
            writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))
        
        writer.release()
        
        # Could also use ffmpeg for even better quality
        # subprocess.run([
        #     'ffmpeg', '-i', temp_path, '-c:v', 'libx264',
        #     '-preset', 'slow', '-crf', '18', output_path
        # ])
```

---

## 3. Complete Pipeline

### 3.1 Backend API Endpoint

```python
# backend/main.py

from integrations.veo import Veo31Client
from veo_optimizer import VeoPhysicsOptimizer

veo_client = Veo31Client(
    project_id=os.getenv("GOOGLE_CLOUD_PROJECT"),
    credentials_path="./gcloud-credentials.json"
)

veo_optimizer = VeoPhysicsOptimizer()

@app.post("/api/physics-to-veo")
async def physics_to_veo(request: PhysicsToVeoRequest):
    """
    Physics simulation â†’ Veo 3.1 video generation
    
    Strategies:
    - keyframes: Extract key poses, let Veo interpolate
    - reference_video: Full video as motion reference
    - hybrid: Keyframes + reference video
    """
    
    job_id = generate_uuid()
    
    # 1. Validate scene
    validation = validate_with_genesis(request.scene)
    if not validation["valid"]:
        raise HTTPException(400, detail=validation["error"])
    
    # Choose strategy based on request
    if request.strategy == "keyframes":
        # Extract keyframes from physics
        keyframes = veo_optimizer.extract_keyframes(
            scene=request.scene,
            num_keyframes=request.num_keyframes or 5,
            strategy=request.keyframe_strategy or "critical_moments"
        )
        
        # Generate with keyframes
        result = await veo_client.generate_with_keyframes(
            prompt=request.prompt,
            keyframes=keyframes,
            duration=request.duration or 5
        )
    
    elif request.strategy == "reference_video":
        # Generate full reference video
        reference_video = veo_optimizer.generate_stylized_reference(
            scene=request.scene,
            output_path=f"./temp/{job_id}",
            style=request.reference_style or "clean_cgi"
        )
        
        # Generate with video reference
        result = await veo_client.generate_from_reference(
            prompt=request.prompt,
            reference_video_path=reference_video,
            reference_image_path=request.style_reference,
            duration=request.duration or 5,
            motion_strength=request.motion_strength or 0.85,
            style_strength=request.style_strength or 0.7
        )
    
    elif request.strategy == "hybrid":
        # Generate reference video
        reference_video = veo_optimizer.generate_stylized_reference(
            scene=request.scene,
            output_path=f"./temp/{job_id}",
            style="clean_cgi"
        )
        
        # Extract keyframes for additional guidance
        keyframes = veo_optimizer.extract_keyframes(
            scene=request.scene,
            num_keyframes=3,
            strategy="critical_moments"
        )
        
        # Use both (Veo 3.1 can handle this)
        result = await veo_client.generate_from_reference(
            prompt=request.prompt,
            reference_video_path=reference_video,
            reference_image_path=request.style_reference,
            duration=request.duration,
            motion_strength=0.9  # High for physics accuracy
        )
    
    return {
        "job_id": job_id,
        "status": "complete",
        "video_url": result["video_url"],
        "generation_id": result.get("generation_id")
    }

@app.post("/api/generate-ad-with-veo")
async def generate_ad_with_veo(request: AdGenerationRequest):
    """
    Simplified ad generation with presets
    """
    
    # 1. Generate or load scene
    if request.scene:
        scene = request.scene
    else:
        # Generate from product description
        scene = await generate_scene_from_product(
            product_type=request.product_type,
            action=request.action
        )
    
    # 2. Build prompt with ad-specific details
    prompt = build_ad_prompt(
        base_description=request.description,
        product_type=request.product_type,
        brand_style=request.brand_style,
        shot_type=request.shot_type,
        lighting=request.lighting
    )
    
    # 3. Generate with Veo
    result = await physics_to_veo(PhysicsToVeoRequest(
        scene=scene,
        prompt=prompt,
        strategy="reference_video",  # Full video works best for ads
        reference_style="clean_cgi",
        style_reference=request.brand_reference_image,
        duration=request.duration or 5,
        motion_strength=0.85
    ))
    
    return result

def build_ad_prompt(
    base_description: str,
    product_type: str,
    brand_style: Optional[str] = None,
    shot_type: str = "product_hero",
    lighting: str = "studio"
) -> str:
    """
    Build optimized prompt for Veo 3.1
    
    Veo responds well to:
    - Cinematography terms
    - Specific camera/lens details
    - Lighting descriptions
    - Brand aesthetic cues
    """
    
    shot_descriptions = {
        "product_hero": "Hero product shot, cinematic composition",
        "lifestyle": "Lifestyle shot, natural environment",
        "close_up": "Extreme close-up, macro photography",
        "dynamic": "Dynamic tracking shot, energetic movement"
    }
    
    lighting_descriptions = {
        "studio": "Professional studio lighting, soft key light with rim light",
        "natural": "Natural daylight, soft shadows, golden hour",
        "dramatic": "Dramatic high-contrast lighting, deep shadows",
        "bright": "Bright even lighting, commercial photography"
    }
    
    # Build comprehensive prompt
    prompt_parts = [
        base_description,
        shot_descriptions.get(shot_type, ""),
        lighting_descriptions.get(lighting, ""),
        "Shot on cinema camera, 4K, professional commercial production",
    ]
    
    if brand_style:
        prompt_parts.append(f"In the style of {brand_style} advertising")
    
    prompt_parts.append(f"Premium {product_type} advertisement")
    
    return ". ".join(p for p in prompt_parts if p) + "."
```

### 3.2 Example Usage

```python
# Example 1: Watch ad with keyframes approach

response = await client.post("/api/generate-ad-with-veo", {
    "product_type": "watch",
    "action": "drop",
    "description": "A luxury Swiss watch falling onto a velvet cushion",
    "brand_style": "Rolex",
    "shot_type": "product_hero",
    "lighting": "dramatic",
    "duration": 5,
    "brand_reference_image": "https://example.com/rolex_ref.jpg"
})

# Veo 3.1 will:
# 1. Use physics sim for accurate fall/bounce motion
# 2. Transform into photorealistic Rolex-style imagery
# 3. Apply dramatic lighting
# 4. Output 1080p, 5-second video

# Example 2: Sneaker with reference video

scene = create_scene_with_text("Nike sneaker bouncing on concrete")
adjust_object_property("sneaker", restitution=0.7)

response = await client.post("/api/physics-to-veo", {
    "scene": scene,
    "prompt": """
        A Nike Air Jordan 1 sneaker bouncing on an urban concrete 
        basketball court. Golden hour lighting, authentic street 
        photography style. Shot on Sony FX6, 24mm lens, shallow 
        depth of field. Energetic, athletic aesthetic.
    """,
    "strategy": "reference_video",
    "duration": 5,
    "motion_strength": 0.9,  # Preserve bounce physics
    "style_strength": 0.7
})

# Example 3: Product reveal with multiple takes

base_scene = create_scene_with_text("iPhone sliding across desk")

# Generate 3 style variations with same physics
variations = []

for style in ["minimalist_apple", "dramatic_dark", "bright_studio"]:
    result = await client.post("/api/physics-to-veo", {
        "scene": base_scene,
        "prompt": STYLE_PROMPTS[style],
        "strategy": "reference_video",
        "duration": 4
    })
    variations.append(result)

# A/B test which performs better
```

---

## 4. Prompt Engineering for Veo + Physics

### 4.1 Prompt Templates

```python
# Best practices for Veo 3.1 prompts

VEO_PROMPT_TEMPLATES = {
    "luxury_product": """
        {product_name} falling onto {surface} in slow motion. 
        Professional studio lighting with soft key light and rim lighting. 
        Reflections on polished surfaces. Shot on Arri Alexa, 50mm lens, 
        f/2.8, shallow depth of field. Premium commercial aesthetic. 
        Clean, sophisticated, high-end production quality.
    """,
    
    "athletic_dynamic": """
        {product_name} {action} on {surface}. Dynamic camera movement, 
        handheld cinematography. Natural outdoor lighting, golden hour. 
        Authentic street energy. Shot on RED Komodo, 24mm lens. 
        High-energy commercial style. Motion blur for speed emphasis.
    """,
    
    "tech_reveal": """
        {product_name} {action} across a minimalist surface. 
        Clean studio environment, perfect white backdrop. 
        Soft diffused lighting, no harsh shadows. Shot on cinema 
        camera with macro lens. Apple keynote aesthetic. 
        Precision, innovation, modern design.
    """,
    
    "lifestyle_narrative": """
        {product_name} in a real-world environment. Natural lighting, 
        authentic setting. Documentary-style cinematography. 
        Shot on cinema camera with handheld movement. Warm, 
        approachable, human-centered. Lifestyle commercial feel.
    """
}

def create_veo_prompt(
    template: str,
    product_name: str,
    action: str = "falling",
    surface: str = "surface",
    additional_details: Optional[str] = None
) -> str:
    """Generate optimized Veo prompt"""
    
    prompt = VEO_PROMPT_TEMPLATES[template].format(
        product_name=product_name,
        action=action,
        surface=surface
    )
    
    if additional_details:
        prompt += f" {additional_details}"
    
    return prompt.strip()
```

### 4.2 Veo-Specific Optimization Tips

```python
# What works well with Veo 3.1:

VEO_BEST_PRACTICES = {
    "camera_terms": [
        "Shot on [camera model]",  # Arri Alexa, RED, Sony FX6
        "[focal length] lens",      # 24mm, 50mm, 85mm
        "f/[aperture]",             # f/1.4, f/2.8
        "shallow depth of field",
        "macro lens",
        "wide angle"
    ],
    
    "lighting_terms": [
        "studio lighting",
        "natural daylight",
        "golden hour",
        "soft key light",
        "rim lighting",
        "high-contrast",
        "diffused lighting"
    ],
    
    "motion_terms": [
        "slow motion",
        "smooth tracking shot",
        "handheld",
        "gimbal shot",
        "static shot",
        "dynamic camera movement"
    ],
    
    "style_references": [
        "Apple commercial",
        "Nike advertisement",
        "BMW commercial",
        "perfume advertisement",
        "luxury brand aesthetic"
    ]
}

# What to avoid:
# - Vague descriptions ("nice", "good", "beautiful")
# - Conflicting instructions
# - Too many objects (focus on hero product)
# - Overly complex scenes
```

---

## 5. Production Pipeline

### 5.1 Complete Workflow

```python
# production_pipeline.py

class AdProductionPipeline:
    """End-to-end pipeline for ad creation"""
    
    def __init__(self):
        self.veo_client = Veo31Client(...)
        self.optimizer = VeoPhysicsOptimizer()
    
    async def create_ad(
        self,
        product_brief: dict,
        creative_direction: dict
    ) -> dict:
        """
        Full pipeline from brief to final video
        
        product_brief: {
            "product_type": "watch",
            "product_name": "Rolex Submariner",
            "key_features": ["water resistant", "automatic movement"],
            "action": "falling onto surface"
        }
        
        creative_direction: {
            "brand_style": "luxury",
            "mood": "dramatic",
            "duration": 5,
            "aspect_ratio": "16:9",
            "call_to_action": "Discover perfection"
        }
        """
        
        # Step 1: Generate physics scene
        scene = await self._create_physics_scene(product_brief)
        
        # Step 2: Create prompt
        prompt = self._create_ad_prompt(product_brief, creative_direction)
        
        # Step 3: Generate reference
        reference = self.optimizer.generate_stylized_reference(
            scene=scene,
            output_path="./temp/ref",
            style="clean_cgi"
        )
        
        # Step 4: Generate with Veo
        video_result = await self.veo_client.generate_from_reference(
            prompt=prompt,
            reference_video_path=reference,
            duration=creative_direction["duration"],
            motion_strength=0.85,
            resolution="1080p",
            aspect_ratio=creative_direction["aspect_ratio"]
        )
        
        # Step 5: Add post-production (optional)
        final_video = await self._add_post_production(
            video_url=video_result["video_url"],
            call_to_action=creative_direction.get("call_to_action"),
            brand_assets=creative_direction.get("brand_assets")
        )
        
        return {
            "final_video": final_video,
            "reference_video": reference,
            "prompt_used": prompt,
            "metadata": {
                "product": product_brief,
                "creative": creative_direction
            }
        }
    
    async def _create_physics_scene(self, product_brief: dict) -> Scene:
        """Generate physics scene from product brief"""
        
        action_to_scene = {
            "falling onto surface": "object dropping onto platform",
            "sliding": "object sliding across surface",
            "bouncing": "object bouncing on ground",
            "rotating": "object spinning on turntable"
        }
        
        scene_description = action_to_scene.get(
            product_brief["action"],
            product_brief["action"]
        )
        
        # Use Claude to generate scene
        response = claude_client.messages.create(
            model="claude-sonnet-4-5-20250929",
            messages=[{
                "role": "user",
                "content": f"Generate physics scene JSON for: {scene_description}"
            }]
        )
        
        scene = Scene(**json.loads(response.content[0].text))
        return scene
    
    def _create_ad_prompt(self, product_brief: dict, creative_direction: dict) -> str:
        """Create optimized Veo prompt"""
        
        brand_aesthetics = {
            "luxury": "Premium commercial production, sophisticated lighting, high-end aesthetic",
            "athletic": "Dynamic energy, authentic sports photography, motivational",
            "tech": "Clean, modern, innovative, Apple-style presentation",
            "lifestyle": "Natural, relatable, documentary style, warm tones"
        }
        
        mood_lighting = {
            "dramatic": "High-contrast lighting, dramatic shadows, cinematic",
            "bright": "Bright, even lighting, clean and modern",
            "warm": "Warm golden tones, sunset lighting, inviting",
            "cool": "Cool blue tones, tech aesthetic, futuristic"
        }
        
        prompt = f"""
        {product_brief['product_name']} {product_brief['action']}.
        {brand_aesthetics.get(creative_direction['brand_style'], '')}.
        {mood_lighting.get(creative_direction['mood'], '')}.
        Shot on professional cinema camera, 4K resolution, commercial quality.
        Product photography, advertising aesthetic.
        """.strip()
        
        return " ".join(prompt.split())  # Clean up whitespace
    
    async def _add_post_production(
        self,
        video_url: str,
        call_to_action: Optional[str] = None,
        brand_assets: Optional[dict] = None
    ) -> str:
        """Add overlays, CTA, branding"""
        
        # Download video from GCS
        video_path = await self._download_from_gcs(video_url)
        
        # Use ffmpeg for post-production
        if call_to_action or brand_assets:
            output_path = "./final_output.mp4"
            
            # Add text overlay, logo, etc.
            # This is simplified - you'd use proper video editing
            subprocess.run([
                'ffmpeg', '-i', video_path,
                # ... add text overlay
                # ... add logo
                output_path
            ])
            
            return output_path
        
        return video_path
```

### 5.2 Frontend Integration (Elm)

```elm
-- Add to Model
type alias Model =
    { scene : Scene
    , videoGeneration : VeoGenerationState
    , adBrief : AdBrief
    }

type alias AdBrief =
    { productName : String
    , productType : String
    , brandStyle : String
    , mood : String
    , duration : Int
    , callToAction : Maybe String
    }

type VeoGenerationState
    = NotStarted
    | SimulatingPhysics
    | GeneratingWithVeo { referenceUrl : String, progress : Float }
    | Complete { reference : String, final : String }
    | Failed String

-- View
viewAdCreator : Model -> Html Msg
viewAdCreator model =
    div [ class "ad-creator" ]
        [ div [ class "brief-panel" ]
            [ h2 [] [ text "Ad Brief" ]
            , input 
                [ placeholder "Product Name"
                , value model.adBrief.productName
                , onInput UpdateProductName
                ] []
            , select [ onInput UpdateBrandStyle ]
                [ option [ value "luxury" ] [ text "Luxury" ]
                , option [ value "athletic" ] [ text "Athletic" ]
                , option [ value "tech" ] [ text "Tech" ]
                , option [ value "lifestyle" ] [ text "Lifestyle" ]
                ]
            , select [ onInput UpdateMood ]
                [ option [ value "dramatic" ] [ text "Dramatic" ]
                , option [ value "bright" ] [ text "Bright" ]
                , option [ value "warm" ] [ text "Warm" ]
                , option [ value "cool" ] [ text "Cool" ]
                ]
            , button [ onClick GenerateAd ] [ text "Generate Ad" ]
            ]
        , div [ class "video-output" ]
            [ viewVeoGeneration model.videoGeneration ]
        ]

viewVeoGeneration : VeoGenerationState -> Html Msg
viewVeoGeneration state =
    case state of
        NotStarted ->
            text "Configure your ad and click Generate"
        
        SimulatingPhysics ->
            div []
                [ text "Step 1/2: Simulating physics..."
                , progressBar 0.5
                ]
        
        GeneratingWithVeo { referenceUrl, progress } ->
            div []
                [ text "Step 2/2: Generating with Veo 3.1..."
                , video [ src referenceUrl, autoplay True, muted True, loop True ] []
                , text "(Physics reference)"
                , progressBar progress
                ]
        
        Complete { reference, final } ->
            div [ class "results" ]
                [ div [ class "side-by-side" ]
                    [ div []
                        [ h3 [] [ text "Physics Sim" ]
                        , video [ src reference, controls True ] []
                        ]
                    , div []
                        [ h3 [] [ text "Final Ad (Veo 3.1)" ]
                        , video [ src final, controls True, autoplay True ] []
                        , button [ onClick (DownloadVideo final) ] 
                            [ text "Download Ad" ]
                        ]
                    ]
                ]
        
        Failed error ->
            div [ class "error" ] [ text error ]
```

---

## Summary

**Your Veo 3.1 + Physics Pipeline:**

1. **Design physics in UI** - Get motion exactly right
2. **Generate clean CGI reference** - High quality input for Veo
3. **Send to Veo 3.1 with optimized prompt** - Physics for motion, AI for photorealism
4. **Get broadcast-quality ad video** - 1080p, 5-20 seconds

**Key advantages with Veo 3.1:**
- Best-in-class video quality
- Excellent motion preservation
- Understands cinematic language
- Longer duration support (up to 20s)
- Good style control

**Recommended approach:** Use reference_video strategy with motion_strength=0.85 for ads. Keyframes work great for shorter clips.

Want me to build out any specific part? (e.g., the GCP setup, prompt optimization, or batch ad generation?)
</file>

<file path=".taskmaster/reports/task-complexity-report.json">
{
	"meta": {
		"generatedAt": "2025-11-12T21:12:19.222Z",
		"tasksAnalyzed": 13,
		"totalTasks": 13,
		"analysisCount": 13,
		"thresholdScore": 5,
		"projectName": "Taskmaster",
		"usedResearch": false
	},
	"complexityAnalysis": [
		{
			"taskId": 1,
			"taskTitle": "Set up Elm application structure and basic UI layout",
			"complexityScore": 5,
			"recommendedSubtasks": 2,
			"expansionPrompt": "Break down the task into subtasks focusing on initializing the Elm project structure, implementing the three-panel UI layout, and setting up basic ports for JavaScript communication.",
			"reasoning": "This task involves setting up a new Elm project with UI layout and ports, which requires familiarity with Elm 0.19.1 and Vite. It's moderately complex due to the need to integrate with JavaScript, but straightforward with basic dependencies. Testing involves compilation and basic rendering, adding some effort."
		},
		{
			"taskId": 2,
			"taskTitle": "Implement Elm scene model types and state management",
			"complexityScore": 7,
			"recommendedSubtasks": 3,
			"expansionPrompt": "Break down the task into subtasks focusing on defining Elm types for Scene and related structures, implementing the update function for various messages, and ensuring pure state management with ports.",
			"reasoning": "Core to the application, this requires defining complex types and handling multiple state transitions in Elm. High complexity due to the breadth of messages and state management, with dependencies on task 1. Testing in the Elm debugger adds moderate effort."
		},
		{
			"taskId": 3,
			"taskTitle": "Set up Three.js renderer and basic scene rendering",
			"complexityScore": 8,
			"recommendedSubtasks": 3,
			"expansionPrompt": "Break down the task into subtasks focusing on initializing the Three.js renderer and camera, setting up controls and lights, and implementing scene loading with Rapier physics integration.",
			"reasoning": "Involves integrating Three.js and Rapier, which are technically challenging libraries. High complexity from physics setup and mesh creation. Dependencies on task 1, and testing requires WebGL and physics initialization checks."
		},
		{
			"taskId": 4,
			"taskTitle": "Integrate Rapier physics simulation loop",
			"complexityScore": 8,
			"recommendedSubtasks": 3,
			"expansionPrompt": "Break down the task into subtasks focusing on connecting Rapier to the animation loop, syncing mesh positions with physics bodies, and implementing start/pause/reset commands.",
			"reasoning": "Requires deep integration of physics simulation with rendering and Elm communication. High complexity due to real-time syncing and performance considerations. Depends on task 3, with testing involving FPS monitoring."
		},
		{
			"taskId": 5,
			"taskTitle": "Set up FastAPI backend server with CORS and basic structure",
			"complexityScore": 4,
			"recommendedSubtasks": 1,
			"expansionPrompt": "Break down the task into subtasks focusing on initializing FastAPI with CORS and defining Pydantic models.",
			"reasoning": "Standard FastAPI setup with models. Low to medium complexity as it's boilerplate code. No dependencies, testing is basic server startup and CORS."
		},
		{
			"taskId": 6,
			"taskTitle": "Integrate Claude AI for scene generation",
			"complexityScore": 6,
			"recommendedSubtasks": 2,
			"expansionPrompt": "Break down the task into subtasks focusing on implementing the generate endpoint with Claude API calls and adding LMDB caching.",
			"reasoning": "Involves AI API integration and caching, which adds technical challenges like error handling and performance. Medium-high complexity, depends on task 5. Testing requires API calls and cache verification."
		},
		{
			"taskId": 7,
			"taskTitle": "Add Genesis physics validation to backend",
			"complexityScore": 7,
			"recommendedSubtasks": 2,
			"expansionPrompt": "Break down the task into subtasks focusing on implementing the validate function with Genesis simulation and integrating it into generation and a separate endpoint.",
			"reasoning": "Adds another physics library (Genesis) for validation, increasing complexity with simulation logic. Depends on tasks 5 and 6. Testing involves stability checks on scenes."
		},
		{
			"taskId": 8,
			"taskTitle": "Implement object selection and raycasting in Three.js",
			"complexityScore": 6,
			"recommendedSubtasks": 2,
			"expansionPrompt": "Break down the task into subtasks focusing on adding click event handling with raycasting and implementing object selection with visual feedback.",
			"reasoning": "Requires Three.js raycasting and event handling, with communication to Elm. Medium complexity due to 3D interaction. Depends on tasks 2 and 3. Testing involves click interactions."
		},
		{
			"taskId": 9,
			"taskTitle": "Add transform controls for translate, rotate, and scale",
			"complexityScore": 6,
			"recommendedSubtasks": 2,
			"expansionPrompt": "Break down the task into subtasks focusing on setting up TransformControls with event listeners and implementing mode switching with Elm sync.",
			"reasoning": "Involves Three.js controls and real-time syncing. Medium complexity, depends on task 8. Testing requires mode switching and drag interactions."
		},
		{
			"taskId": 10,
			"taskTitle": "Implement simulation controls and property editing panel",
			"complexityScore": 7,
			"recommendedSubtasks": 3,
			"expansionPrompt": "Break down the task into subtasks focusing on adding UI buttons for controls, implementing the property editing panel, and syncing changes to physics.",
			"reasoning": "Combines UI updates in Elm with physics syncing. High complexity due to multiple dependencies (2,4,9) and behavioral testing. Requires careful state management."
		},
		{
			"taskId": 11,
			"taskTitle": "Add AI refinement feature",
			"complexityScore": 6,
			"recommendedSubtasks": 2,
			"expansionPrompt": "Break down the task into subtasks focusing on implementing the refine endpoint in the backend and adding UI elements in Elm for refinement.",
			"reasoning": "Extends AI integration with refinement logic. Medium complexity, depends on tasks 6 and 10. Testing involves scene modifications."
		},
		{
			"taskId": 12,
			"taskTitle": "Implement undo/redo system and local storage",
			"complexityScore": 7,
			"recommendedSubtasks": 3,
			"expansionPrompt": "Break down the task into subtasks focusing on tracking scene history in Elm, implementing undo/redo functionality, and integrating IndexedDB for save/load.",
			"reasoning": "Involves state history management and browser storage API. High complexity due to persistence and UI integration. Depends on tasks 2 and 11. Testing requires state reversion checks."
		},
		{
			"taskId": 13,
			"taskTitle": "Add keyboard shortcuts and UI polish",
			"complexityScore": 4,
			"recommendedSubtasks": 1,
			"expansionPrompt": "Break down the task into subtasks focusing on implementing keyboard event handling and polishing UI styling and error handling.",
			"reasoning": "Polishing task with low technical challenges, mainly UI enhancements. Low complexity, depends on tasks 10 and 12. Testing is user interaction focused."
		}
	]
}
</file>

<file path=".taskmaster/config.json">
{
  "models": {
    "main": {
      "provider": "xai",
      "modelId": "grok-code-fast-1",
      "maxTokens": 131072,
      "temperature": 0.2
    },
    "research": {
      "provider": "codex-cli",
      "modelId": "gpt-5",
      "maxTokens": 128000,
      "temperature": 0.1
    },
    "fallback": {
      "provider": "anthropic",
      "modelId": "claude-3-7-sonnet-20250219",
      "maxTokens": 120000,
      "temperature": 0.2
    }
  },
  "global": {
    "logLevel": "info",
    "debug": false,
    "defaultNumTasks": 10,
    "defaultSubtasks": 5,
    "defaultPriority": "medium",
    "projectName": "Taskmaster",
    "ollamaBaseURL": "http://localhost:11434/api",
    "bedrockBaseURL": "https://bedrock.us-east-1.amazonaws.com",
    "responseLanguage": "English",
    "enableCodebaseAnalysis": true,
    "defaultTag": "master",
    "azureOpenaiBaseURL": "https://your-endpoint.openai.azure.com/",
    "userId": "1234567890"
  },
  "claudeCode": {},
  "codexCli": {},
  "grokCli": {
    "timeout": 120000,
    "workingDirectory": null,
    "defaultModel": "grok-4-latest"
  }
}
</file>

<file path=".taskmaster/state.json">
{
  "currentTag": "master",
  "lastSwitched": "2025-11-12T21:00:53.531Z",
  "branchTagMapping": {},
  "migrationNoticeShown": true
}
</file>

<file path=".zed/settings.json">
{
	"mcpServers": {
		"task-master-ai": {
			"command": "npx",
			"args": ["-y", "task-master-ai"],
			"env": {
				"ANTHROPIC_API_KEY": "YOUR_ANTHROPIC_API_KEY_HERE",
				"PERPLEXITY_API_KEY": "YOUR_PERPLEXITY_API_KEY_HERE",
				"OPENAI_API_KEY": "YOUR_OPENAI_KEY_HERE",
				"GOOGLE_API_KEY": "YOUR_GOOGLE_KEY_HERE",
				"XAI_API_KEY": "YOUR_XAI_KEY_HERE",
				"OPENROUTER_API_KEY": "YOUR_OPENROUTER_KEY_HERE",
				"MISTRAL_API_KEY": "YOUR_MISTRAL_KEY_HERE",
				"AZURE_OPENAI_API_KEY": "YOUR_AZURE_KEY_HERE",
				"OLLAMA_API_KEY": "YOUR_OLLAMA_API_KEY_HERE"
			}
		}
	}
}
</file>

<file path="package.json">
{
  "name": "physics-simulator",
  "version": "1.0.0",
  "description": "AI-powered physics simulation with Elm frontend",
  "main": "index.js",
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "preview": "vite preview"
  },
  "devDependencies": {
    "vite": "^4.0.0",
    "vite-plugin-elm": "^2.7.0",
    "vite-plugin-top-level-await": "^1.6.0",
    "vite-plugin-wasm": "^3.5.0"
  },
  "dependencies": {
    "@dimforge/rapier3d": "^0.11.2",
    "@types/three": "^0.155.0",
    "three": "^0.155.0"
  },
  "type": "module"
}
</file>

<file path="README.md">
# AI-Powered Physics Simulator

A sophisticated 3D physics simulation platform that uses cutting-edge AI to generate and refine physics scenes through natural language prompts. Built with Elm frontend and FastAPI backend, featuring real-time physics simulation powered by Rapier.js.

## ðŸš€ Features

### AI-Powered Scene Generation
- **Claude Sonnet 4.5**: Latest Anthropic AI model for intelligent scene creation
- **Natural Language Prompts**: Generate complex physics scenes from text descriptions
- **Scene Refinement**: Modify existing scenes with conversational AI
- **Context-Aware**: AI understands physics principles and realistic object interactions

### Real-Time 3D Physics Simulation
- **Rapier Physics Engine**: High-performance, WebAssembly-powered physics
- **Realistic Physics**: Gravity, collisions, friction, and restitution
- **Multiple Object Types**: Boxes, spheres, cylinders with customizable properties
- **Dynamic Interactions**: Objects interact naturally in real-time

### Interactive 3D Editor
- **Transform Controls**: Move, rotate, and scale objects with visual gizmos
- **Property Editing**: Adjust mass, friction, restitution, and visual properties
- **Object Selection**: Click to select objects with visual feedback
- **Real-Time Updates**: See physics changes instantly

### Advanced Features
- **Undo/Redo System**: Full history tracking with keyboard shortcuts
- **Local Storage**: Scenes persist across browser sessions
- **Keyboard Shortcuts**: Space (play/pause), G/R/S (transform modes), Ctrl+Z/Y (undo/redo)
- **Responsive UI**: Modern, clean interface optimized for physics simulation

## ðŸ› ï¸ Technology Stack

### Frontend
- **Elm 0.19.1**: Functional programming language for reliable UIs
- **Three.js**: 3D graphics and rendering
- **Rapier.js**: WebAssembly physics engine
- **Vite**: Fast development server and build tool

### Backend
- **FastAPI**: High-performance Python web framework
- **OpenRouter API**: Access to Claude Sonnet 4.5 via OpenAI-compatible interface
- **Pydantic**: Data validation and serialization
- **Uvicorn**: ASGI server for production deployment

### AI Integration
- **Claude Sonnet 4.5**: Anthropic's most advanced AI model
- **1M Token Context**: Massive context window for complex scene generation
- **OpenRouter**: Multi-provider AI routing with automatic fallbacks

## ðŸ“‹ Prerequisites

- **Python 3.14+** with virtual environment support
- **Node.js 16+** and npm
- **OpenRouter API Key** (for AI scene generation)

## ðŸš€ Quick Start

### 1. Clone and Setup
```bash
git clone <repository-url>
cd physics-simulator
```

### 2. Backend Setup
```bash
cd backend
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 3. Frontend Setup
```bash
# In a separate terminal
npm install
```

### 4. Environment Configuration
Create a `.env` file in the backend directory:
```bash
OPENROUTER_API_KEY=your_openrouter_api_key_here
```

### 5. Start the Application
```bash
# Terminal 1: Backend
cd backend && source venv/bin/activate
OPENROUTER_API_KEY=your_key python main.py

# Terminal 2: Frontend
npm run dev
```

### 6. Open in Browser
Navigate to `http://localhost:5173`

## ðŸŽ® Usage Guide

### Generating Scenes
1. Enter a natural language prompt in the left panel
2. Click "Generate Scene" to create a new physics scene
3. Watch as AI creates realistic objects with proper physics properties

### Example Prompts
- "a red ball bouncing on a wooden table"
- "stack of colorful boxes with a sphere rolling between them"
- "pyramid of spheres on a ramp with a cube at the bottom"

### Editing Scenes
1. **Select Objects**: Click on objects in the 3D view
2. **Transform**: Use G (move), R (rotate), S (scale) keys
3. **Properties**: Adjust physics properties in the right panel
4. **Refine**: Use the "Refine Scene" feature to modify with AI

### Physics Controls
- **Play/Pause**: Spacebar or play button
- **Reset**: Reload the page to reset the scene
- **Undo/Redo**: Ctrl+Z / Ctrl+Y

## ðŸ”§ API Reference

### Backend Endpoints

#### Generate Scene
```http
POST /api/generate
Content-Type: application/json

{
  "prompt": "a bouncing ball and a wooden box"
}
```

#### Refine Scene
```http
POST /api/refine
Content-Type: application/json

{
  "scene": {...},
  "prompt": "make the ball blue"
}
```

### Scene Format
```json
{
  "objects": {
    "object_id": {
      "id": "object_id",
      "transform": {
        "position": {"x": 0.0, "y": 5.0, "z": 0.0},
        "rotation": {"x": 0.0, "y": 0.0, "z": 0.0},
        "scale": {"x": 1.0, "y": 1.0, "z": 1.0}
      },
      "physicsProperties": {
        "mass": 1.0,
        "friction": 0.5,
        "restitution": 0.3
      },
      "visualProperties": {
        "color": "#ff0000",
        "shape": "Box"
      }
    }
  },
  "selectedObject": null
}
```

## ðŸ—ï¸ Architecture

### Frontend Architecture (Elm)
```
Main.elm
â”œâ”€â”€ Model: Scene, UI state, simulation state
â”œâ”€â”€ Update: Message handling and state transitions
â”œâ”€â”€ View: Three-panel layout with canvas integration
â””â”€â”€ Ports: Communication with JavaScript/Three.js
```

### Backend Architecture (FastAPI)
```
main.py
â”œâ”€â”€ AI Client: OpenRouter integration with Claude Sonnet 4.5
â”œâ”€â”€ Scene Generation: AI-powered scene creation
â”œâ”€â”€ Scene Refinement: Conversational scene modification
â””â”€â”€ API Endpoints: RESTful physics scene management
```

### Physics Integration
```
PhysicsRenderer.js (Three.js + Rapier)
â”œâ”€â”€ Scene Management: Object creation and updates
â”œâ”€â”€ Physics Simulation: Real-time physics calculations
â”œâ”€â”€ Transform Controls: Interactive object manipulation
â””â”€â”€ Rendering: WebGL-accelerated 3D graphics
```

## ðŸ”’ Security & Privacy

- **API Key Security**: OpenRouter API key stored server-side only
- **No Data Persistence**: Scenes exist only in browser memory
- **Local Storage**: Optional scene saving in browser localStorage
- **CORS Protection**: Configured for localhost development

## ðŸš€ Deployment

### Development
```bash
# Backend
cd backend && source venv/bin/activate
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# Frontend
npm run dev -- --host 0.0.0.0
```

### Production Build
```bash
# Frontend
npm run build

# Backend (using gunicorn)
gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker
```

## ðŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make changes and test thoroughly
4. Commit changes: `git commit -am 'Add feature'`
5. Push to branch: `git push origin feature-name`
6. Submit a pull request

## ðŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ðŸ™ Acknowledgments

- **Anthropic**: For Claude Sonnet 4.5 AI model
- **OpenRouter**: For AI model access and routing
- **Rapier Physics**: For high-performance WebAssembly physics
- **Three.js**: For 3D graphics and WebGL rendering
- **Elm**: For reliable functional programming frontend

## ðŸ“ž Support

For questions or issues:
- Create an issue on GitHub
- Check the troubleshooting section below

## ðŸ”§ Troubleshooting

### Common Issues

**Backend won't start:**
- Ensure Python virtual environment is activated
- Check OPENROUTER_API_KEY environment variable
- Verify all dependencies are installed: `pip install -r requirements.txt`

**Frontend compilation errors:**
- Run `npm install` to ensure all dependencies
- Check Elm version: `elm --version` (should be 0.19.1)
- Clear Elm cache: `rm -rf elm-stuff && elm install`

**Physics not working:**
- Check browser console for WebAssembly errors
- Ensure modern browser with WebGL support
- Try refreshing the page

**AI generation fails:**
- Verify OpenRouter API key is valid
- Check API quota and billing status
- Ensure internet connection for API calls

### Development Tips

- Use browser developer tools to inspect 3D scene
- Check browser console for Elm runtime errors
- Monitor network tab for API request/response
- Use browser localStorage inspector for saved scenes

---

**Built with â¤ï¸ using cutting-edge AI and modern web technologies**
</file>

<file path="vite.config.js">
import { defineConfig } from 'vite'
import { plugin } from 'vite-plugin-elm'
import wasm from 'vite-plugin-wasm'
import topLevelAwait from 'vite-plugin-top-level-await'

export default defineConfig({
  plugins: [
    wasm(),
    topLevelAwait(),
    plugin()
  ],
  server: {
    port: 5173
  }
})
</file>

<file path=".taskmaster/tasks/tasks.json">
{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Set up Elm application structure and basic UI layout",
        "description": "Initialize the Elm project with the main application structure, including the three-panel layout (left panel for generation, canvas for rendering, right panel for properties), and define basic ports for communication with JavaScript.",
        "details": "Use Elm 0.19.1 to create the Main.elm file with the Browser.element program. Define the Model with Scene, UiState, and SimulationState. Implement the view function to render the app-container with left-panel, canvas-container, and right-panel. Set up ports for sendSceneToThreeJs, sendSelectionToThreeJs, etc. Ensure the HTML includes the canvas-container div. Use Vite for build configuration as specified.",
        "testStrategy": "Verify that the Elm app compiles without errors, the UI renders the three panels correctly in the browser, and ports are defined without runtime issues.",
        "priority": "high",
        "dependencies": [],
        "status": "completed",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Elm project structure",
            "description": "Set up the basic Elm project using Elm 0.19.1, create the Main.elm file with Browser.element program, and define the initial Model structure including Scene, UiState, and SimulationState.",
            "dependencies": [],
            "details": "Use Elm init to create the project, ensure Vite is configured for building. In Main.elm, import necessary modules, define the Model type with placeholders for Scene, UiState, and SimulationState. Set up the main function with Browser.element, including init, update, view, and subscriptions.",
            "status": "completed",
            "testStrategy": "Verify that the Elm project compiles successfully without errors using elm make."
          },
          {
            "id": 2,
            "title": "Implement three-panel UI layout and basic ports",
            "description": "Implement the view function to render the app-container with left-panel, canvas-container, and right-panel, and define ports for communication with JavaScript.",
            "dependencies": [1],
            "details": "In the view function, use Html to create divs for the three panels: left-panel for generation, canvas-container for rendering, and right-panel for properties. Ensure the HTML includes the canvas-container div. Define ports such as sendSceneToThreeJs, sendSelectionToThreeJs, and others as specified. Update the update function to handle any initial messages if needed.",
            "status": "completed",
            "testStrategy": "Compile the Elm app, load it in the browser, and check that the three panels are rendered correctly, and ports are defined without runtime errors."
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement Elm scene model types and state management",
        "description": "Define the Elm types for Scene, PhysicsObject, Transform, PhysicsProperties, etc., and implement the update function to handle messages like UpdateTextInput, GenerateScene, SceneGenerated, ObjectClicked, etc.",
        "details": "Based on the provided Elm code in the PRD, implement the Model, Msg types, and update function. Handle state transitions for text input, scene generation, object selection, transform updates, and simulation toggling. Ensure pure state management without side effects except through ports. Use Dict for objects and track initial states for reset.",
        "testStrategy": "Test Elm compilation, simulate user interactions in the Elm debugger (e.g., update text input, trigger generate), and verify state changes correctly update the model without errors.",
        "priority": "high",
        "dependencies": [1],
        "status": "completed",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Elm Model and Msg types for scene management",
            "description": "Define the core Elm types including Model, Msg, Scene, PhysicsObject, Transform, PhysicsProperties, and related structures using records and unions.",
            "dependencies": [1],
            "details": "Based on the PRD Elm code, create type aliases and union types for Model (containing scene data, selected object, simulation state, etc.), Msg (covering UpdateTextInput, GenerateScene, SceneGenerated, ObjectClicked, etc.), and nested types like Scene with Dict for objects. Ensure types support state tracking for reset functionality.",
            "status": "completed",
            "testStrategy": "Compile the Elm code and verify no type errors; use Elm debugger to inspect initial model structure."
          },
          {
            "id": 2,
            "title": "Implement update function for message handling",
            "description": "Implement the update function to handle various Msg types, managing state transitions for text input, scene generation, object selection, transform updates, and simulation toggling.",
            "dependencies": [1],
            "details": "In the update function, pattern match on Msg variants to update the Model accordingly, ensuring pure functions without side effects. Handle dependencies like updating scene from SceneGenerated, selecting objects via ObjectClicked, and toggling simulation state. Use Dict operations for object management and track initial states for reset.",
            "status": "completed",
            "testStrategy": "Simulate message dispatches in Elm debugger (e.g., send UpdateTextInput, GenerateScene) and verify model updates correctly without errors or side effects."
          },
          {
            "id": 3,
            "title": "Integrate ports for pure state management and external communication",
            "description": "Set up ports for communication with JavaScript (e.g., for scene generation, object clicks, and simulation commands) while maintaining pure Elm state management.",
            "dependencies": [1, 2],
            "details": "Define incoming and outgoing ports for messages like sceneGenerated, objectClicked, and commands to update Three.js. Ensure the update function uses Cmd for port calls and subscriptions for incoming data. Track initial scene states in Model for reset, and avoid direct side effects in update logic.",
            "status": "completed",
            "testStrategy": "Test port integration by triggering Elm actions (e.g., generate scene) and verify JavaScript receives correct data; check that state remains pure by reloading and confirming no unexpected changes."
          }
        ]
      },
      {
        "id": 3,
        "title": "Set up Three.js renderer and basic scene rendering",
        "description": "Initialize Three.js in the JavaScript bridge, set up the renderer, camera, controls, lights, and ground plane, and implement the loadScene method to render objects from Elm data.",
        "details": "In index.js, create the PhysicsRenderer class with setupRenderer, setupCamera, setupControls (OrbitControls), setupLights, and setupPhysics (Rapier world). Implement loadScene to clear existing objects and add new ones using addObject, which creates Three.js meshes and Rapier bodies/colliders for Box, Sphere, Cylinder. Handle geometry creation based on object type and apply materials with visual properties.",
        "testStrategy": "Run the app, verify Three.js renders a basic scene (e.g., ground plane), and check console for no WebGL errors. Manually test camera orbiting and ensure Rapier initializes without issues.",
        "priority": "high",
        "dependencies": [1],
        "status": "completed",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Three.js renderer and camera",
            "description": "Set up the basic Three.js renderer and camera in the PhysicsRenderer class to enable scene rendering.",
            "dependencies": [],
            "details": "In index.js, within the PhysicsRenderer class, implement the setupRenderer method to create a WebGLRenderer, set its size to match the canvas container, and append it to the DOM. Implement setupCamera to create a PerspectiveCamera with appropriate field of view, aspect ratio, near and far planes, and position it suitably for the scene.",
            "status": "completed",
            "testStrategy": "Verify that the renderer initializes without WebGL errors and the camera is positioned correctly by checking the canvas renders a basic view."
          },
          {
            "id": 2,
            "title": "Set up OrbitControls and lighting",
            "description": "Configure OrbitControls for camera manipulation and add basic lighting to the scene for proper object visibility.",
            "dependencies": [1],
            "details": "Implement setupControls to initialize OrbitControls with the camera and renderer DOM element, enabling mouse-based orbiting, zooming, and panning. Implement setupLights to add ambient and directional lights to the scene, positioning them to illuminate objects effectively and include a ground plane mesh for reference.",
            "status": "completed",
            "testStrategy": "Test camera controls by orbiting and zooming in the rendered scene, and ensure lights illuminate objects without shadows or rendering artifacts."
          },
          {
            "id": 3,
            "title": "Implement scene loading with Rapier physics integration",
            "description": "Develop the loadScene method to handle scene updates from Elm, integrating Three.js meshes with Rapier physics bodies and colliders.",
            "dependencies": [2],
            "details": "Implement setupPhysics to initialize the Rapier world with gravity. Create the loadScene method to clear existing meshes and physics bodies, then use addObject to iterate through Elm-provided objects, creating Three.js geometries (BoxGeometry, SphereGeometry, CylinderGeometry) and meshes with materials based on visual properties. For each object, add corresponding Rapier rigid bodies and colliders (cuboid, ball, cylinder) linked to the meshes.",
            "status": "completed",
            "testStrategy": "Load a sample scene via Elm ports, verify objects render correctly with physics applied (e.g., falling under gravity), and check console for no Rapier initialization errors."
          }
        ]
      },
      {
        "id": 4,
        "title": "Integrate Rapier physics simulation loop",
        "description": "Connect Rapier to the animation loop, sync Three.js meshes with physics bodies during simulation, and handle start/pause/reset commands from Elm.",
        "details": "In the animate method, step the Rapier world when simulating, update mesh positions and rotations from body data, and send frame updates to Elm via ports. Implement startSimulation, pauseSimulation, and resetSimulation methods that detach transform controls and reload scene on reset. Ensure 60 FPS simulation with proper timestep.",
        "testStrategy": "Trigger simulation via Elm, observe objects falling realistically under gravity, check that pause stops updates, and reset restores initial positions. Use browser dev tools to monitor FPS and ensure no performance drops.",
        "priority": "high",
        "dependencies": [3],
        "status": "completed",
        "subtasks": [
          {
            "id": 1,
            "title": "Connect Rapier to the animation loop",
            "description": "Modify the animate method to step the Rapier world during simulation, ensuring proper timestep for 60 FPS.",
            "dependencies": [3],
            "details": "In the PhysicsRenderer class, update the animate method to call world.step() when simulation is active, using a fixed timestep like 1/60 to maintain consistent physics updates. Integrate this with the requestAnimationFrame loop to synchronize physics stepping with rendering.",
            "status": "completed",
            "testStrategy": "Monitor FPS in browser dev tools during simulation to ensure it stays at 60 FPS without drops."
          },
          {
            "id": 2,
            "title": "Sync Three.js meshes with physics bodies",
            "description": "Update mesh positions and rotations from Rapier body data during each animation frame.",
            "dependencies": [1],
            "details": "In the animate method, after stepping the world, iterate through all physics bodies, retrieve their positions and rotations, and apply them to the corresponding Three.js meshes using mesh.position.copy(body.translation()) and mesh.quaternion.copy(body.rotation()). Send frame updates to Elm via ports if needed.",
            "status": "completed",
            "testStrategy": "Trigger simulation and observe objects falling realistically under gravity, verifying that meshes follow physics bodies accurately."
          },
          {
            "id": 3,
            "title": "Implement start, pause, and reset simulation commands",
            "description": "Add methods to handle startSimulation, pauseSimulation, and resetSimulation commands from Elm.",
            "dependencies": [2],
            "details": "Implement startSimulation to begin stepping the world and detach transform controls, pauseSimulation to stop stepping, and resetSimulation to reload the scene, restore initial positions, and reattach controls. Ensure these methods are called via Elm ports and handle state transitions properly.",
            "status": "completed",
            "testStrategy": "Send commands from Elm to start, pause, and reset simulation, checking that pause stops updates, reset restores positions, and controls detach/reattach correctly."
          }
        ]
      },
      {
        "id": 5,
        "title": "Set up FastAPI backend server with CORS and basic structure",
        "description": "Initialize the FastAPI application, configure CORS for local development, and define the basic endpoint structure including models for Vec3, Transform, PhysicsObject, Scene, etc.",
        "details": "Create main.py with FastAPI app, add CORSMiddleware for localhost:5173. Define Pydantic models matching the JSON schema. Set up uvicorn for running the server on port 8000. Include placeholders for AI client and cache initialization. Ensure requirements.txt includes fastapi, uvicorn, pydantic.",
        "testStrategy": "Start the server with uvicorn, verify it runs on localhost:8000, test CORS by making a request from the frontend, and check that models validate JSON correctly without errors.",
        "priority": "high",
        "dependencies": [],
        "status": "completed",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize FastAPI application with CORS and define Pydantic models",
            "description": "Set up the FastAPI app in main.py, configure CORSMiddleware for localhost:5173, and define Pydantic models for Vec3, Transform, PhysicsObject, Scene, etc., matching the JSON schema.",
            "dependencies": [],
            "details": "Create main.py with FastAPI instance, add CORSMiddleware allowing origins from localhost:5173. Define Pydantic BaseModel classes for Vec3, Transform, PhysicsObject, Scene, and other required models based on the provided JSON schema. Include placeholders for AI client and cache initialization. Ensure requirements.txt includes fastapi, uvicorn, and pydantic. Set up uvicorn to run the server on port 8000.",
            "status": "completed",
            "testStrategy": "Start the server with uvicorn and verify it runs on localhost:8000. Test CORS by making a cross-origin request from the frontend and confirm models validate JSON inputs without errors."
          }
        ]
      },
      {
        "id": 6,
        "title": "Integrate AI for scene generation",
        "description": "Implement the /api/generate endpoint to use AI API for generating scenes from text prompts, including caching with LMDB.",
        "details": "In main.py, implement generate_scene function with prompt template, call AI API, parse response as Scene, cache successful results using hashlib and lmdb. Handle errors for JSON parsing and API failures. Use environment variables for AI model configuration.",
        "testStrategy": "Send POST requests to /api/generate with sample text (e.g., 'red box'), verify JSON response matches schema, check cache by repeating request (should be faster), and ensure invalid prompts return appropriate errors.",
        "priority": "high",
        "dependencies": [5],
        "status": "completed",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement /api/generate endpoint with AI API integration",
            "description": "Create the /api/generate endpoint in main.py that accepts text prompts, uses a prompt template, calls the AI API, and parses the response into a Scene object. Handle errors for API failures and JSON parsing issues. Configure AI model settings via environment variables.",
            "dependencies": [5],
            "details": "In main.py, define the generate_scene function that constructs a prompt template for scene generation, makes an API call to AI using the configured model (from env vars), parses the JSON response into a Scene object, and returns it. Implement error handling for invalid responses or API errors, raising appropriate exceptions. Ensure the endpoint is a POST route that accepts JSON with a 'prompt' field.",
            "status": "completed",
            "testStrategy": "Send POST requests to /api/generate with valid text prompts (e.g., 'red box'), verify the JSON response conforms to the Scene schema, and check that invalid prompts or API failures return proper error messages."
          },
          {
            "id": 2,
            "title": "Add LMDB caching for generated scenes",
            "description": "Integrate LMDB caching to store successful scene generation results using hashlib for key generation, ensuring repeated requests for the same prompt are served from cache for improved performance.",
            "dependencies": [1],
            "details": "In the generate_scene function, before calling the Claude API, compute a hash of the prompt using hashlib (e.g., SHA256), check if the result exists in an LMDB database. If cached, return the stored Scene; otherwise, proceed with API call and cache the successful result in LMDB. Handle cache misses and ensure thread-safe access if needed. Use environment variables for LMDB path configuration.",
            "status": "completed",
            "testStrategy": "Send identical POST requests to /api/generate, measure response times to verify caching (second request should be faster), and inspect LMDB database to confirm cached entries are stored correctly."
          }
        ]
      },
      {
        "id": 7,
        "title": "Add Genesis physics validation to backend",
        "description": "Implement validate_with_genesis function and integrate it into scene generation and a separate /api/validate endpoint.",
        "details": "Use genesis-world library to create a scene, add entities based on object types, simulate 60 frames, and check for stability. Return validation results. Integrate into generate_scene to reject invalid scenes. Ensure Genesis handles Box and Sphere types primarily.",
        "testStrategy": "Test /api/validate with valid and invalid scenes (e.g., overlapping objects), verify it detects issues like instability, and ensure generate_scene fails gracefully on validation errors.",
        "priority": "medium",
        "dependencies": [5, 6],
        "status": "completed",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement validate_with_genesis function using Genesis library",
            "description": "Create the validate_with_genesis function that uses genesis-world to build a scene, add Box and Sphere entities, simulate 60 frames, and check for stability.",
            "dependencies": [5, 6],
            "details": "In the backend, import genesis-world, define validate_with_genesis(scene_data) that parses object types, creates a Genesis scene, adds rigid bodies for Boxes and Spheres with appropriate properties, runs a 60-frame simulation, and evaluates stability by checking for excessive movement or collisions. Return a validation result object indicating pass/fail with details.",
            "status": "completed",
            "testStrategy": "Unit test the function with mock scene data containing stable and unstable configurations, verifying it correctly identifies stability issues like overlapping objects falling apart."
          },
          {
            "id": 2,
            "title": "Integrate Genesis validation into scene generation and API endpoint",
            "description": "Integrate the validate_with_genesis function into generate_scene to reject invalid scenes and expose it via a new /api/validate endpoint.",
            "dependencies": [1],
            "details": "Modify generate_scene to call validate_with_genesis after scene creation and reject if validation fails, raising an appropriate error. Create a new Flask route /api/validate that accepts scene data via POST, calls validate_with_genesis, and returns the validation results as JSON. Ensure error handling for unsupported object types beyond Box and Sphere.",
            "status": "completed",
            "testStrategy": "Test generate_scene by attempting to generate invalid scenes (e.g., unstable stacks) and confirm rejection. Hit /api/validate with valid/invalid JSON payloads and verify correct pass/fail responses, including error messages for unsupported types."
          }
        ]
      },
      {
        "id": 8,
        "title": "Implement object selection and raycasting in Three.js",
        "description": "Add click event handling for object selection using raycasting, highlight selected objects, and communicate selections back to Elm.",
        "details": "In setupControls, add mouse event listener for clicks, use Raycaster to intersect meshes, send objectClicked event to Elm. Implement selectObject to attach TransformControls to the selected mesh. Update material or add outline for visual feedback.",
        "testStrategy": "Click on rendered objects in the scene, verify selection highlights appear, TransformControls attach, and Elm receives the correct objectId via ports.",
        "priority": "high",
        "dependencies": [2, 3],
        "status": "completed",
        "subtasks": [
          {
            "id": 1,
            "title": "Add click event handling and raycasting for object detection",
            "description": "Implement mouse click event listener in setupControls to detect clicks on the canvas, use Three.js Raycaster to intersect with meshes in the scene, and identify the selected object.",
            "dependencies": [2, 3],
            "details": "In the setupControls method of the PhysicsRenderer class, add an event listener for 'click' events on the renderer.domElement. Calculate mouse position normalized to [-1,1], create a Raycaster, set its origin from the camera and direction towards the mouse position, and intersect with the scene's meshes. Store the intersected object for further processing.",
            "status": "completed",
            "testStrategy": "Click on various objects in the rendered scene and verify that the raycaster correctly detects intersections by logging the selected object ID in the console."
          },
          {
            "id": 2,
            "title": "Implement object selection with visual feedback and Elm communication",
            "description": "Handle the selected object by highlighting it, attaching TransformControls, and sending the selection event to Elm via ports.",
            "dependencies": [1],
            "details": "After detecting the selected object via raycasting, implement a selectObject method that updates the object's material (e.g., change color or add an outline using a post-processing effect or emissive property), attaches TransformControls to the mesh for manipulation, and sends an 'objectClicked' event with the object ID to Elm through the defined ports. Ensure previous selections are deselected properly.",
            "status": "completed",
            "testStrategy": "Click on objects, check that visual highlights appear (e.g., outline or color change), TransformControls attach and allow dragging, and verify Elm receives the correct objectId via port messages in the debugger."
          }
        ]
      },
      {
        "id": 9,
        "title": "Add transform controls for translate, rotate, and scale",
        "description": "Integrate TransformControls for manipulating selected objects, handle mode switching from Elm, and sync changes back to Elm state.",
        "details": "In PhysicsRenderer, set up TransformControls with event listeners for dragging and objectChange. Implement setTransformMode to switch between translate, rotate, scale. On change, send transformUpdate to Elm. Ensure controls detach during simulation.",
        "testStrategy": "Select an object, press keys (G/R/S) to switch modes, drag to transform, verify real-time updates in Elm model and visual changes in Three.js.",
        "priority": "high",
        "dependencies": [8],
        "status": "completed",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up TransformControls with event listeners in PhysicsRenderer",
            "description": "Initialize TransformControls in the Three.js scene, attach it to the selected object, and add event listeners for 'dragging' and 'objectChange' to handle real-time updates.",
            "dependencies": [],
            "details": "In the PhysicsRenderer class, create a new TransformControls instance attached to the camera and scene. Add event listeners: on 'dragging' to prevent other interactions, and on 'objectChange' to capture transform changes. Ensure the controls are properly disposed when not in use.",
            "status": "completed",
            "testStrategy": "Attach controls to a selected object and verify that dragging updates the object's position/rotation/scale visually in Three.js without errors."
          },
          {
            "id": 2,
            "title": "Implement mode switching and Elm synchronization for TransformControls",
            "description": "Add setTransformMode method to switch between translate, rotate, and scale modes based on Elm input, and send transform updates back to Elm via ports.",
            "dependencies": [1],
            "details": "Implement setTransformMode function that sets the controls.mode to 'translate', 'rotate', or 'scale' based on messages from Elm. On 'objectChange' event, send a transformUpdate message to Elm with the updated position, rotation, and scale. Ensure controls detach during simulation to prevent conflicts.",
            "status": "completed",
            "testStrategy": "Switch modes via Elm (e.g., key presses), perform transformations, and check that Elm state updates correctly and reflects in the UI or subsequent actions."
          }
        ]
      },
      {
        "id": 10,
        "title": "Implement simulation controls and property editing panel",
        "description": "Add play/pause/reset buttons in Elm UI, connect to physics commands, and implement the right panel for editing object properties like mass, friction, etc.",
        "details": "In Elm, add buttons for ToggleSimulation and ResetSimulation, update viewBottomBar and viewRightPanel. Handle UpdateObjectProperty messages to modify scene. Sync property changes to Three.js by reloading scene or updating colliders. Ensure sliders update physics properties.",
        "testStrategy": "Edit properties in the panel (e.g., increase mass), simulate, observe different behaviors (e.g., heavier objects fall slower), and test play/pause/reset functionality with button clicks.",
        "priority": "high",
        "dependencies": [2, 4, 9],
        "status": "completed",
        "subtasks": [
          {
            "id": 1,
            "title": "Add UI buttons for simulation controls",
            "description": "Implement play/pause/reset buttons in the Elm UI bottom bar, connecting them to ToggleSimulation and ResetSimulation messages.",
            "dependencies": [2, 4],
            "details": "Update the viewBottomBar function in Elm to include buttons for toggling simulation state and resetting the scene. Ensure buttons dispatch appropriate messages to the update function, and handle state changes for simulation running/paused.",
            "status": "completed",
            "testStrategy": "Click play/pause/reset buttons and verify simulation starts, pauses, or resets correctly, observing physics behavior in the scene."
          },
          {
            "id": 2,
            "title": "Implement property editing panel",
            "description": "Create the right panel in Elm UI for editing object properties such as mass, friction, and other physics attributes using sliders and inputs.",
            "dependencies": [2],
            "details": "Update the viewRightPanel function to display editable properties for selected objects. Handle UpdateObjectProperty messages in the update function to modify the scene model. Ensure the panel only shows when an object is selected and updates dynamically.",
            "status": "completed",
            "testStrategy": "Select an object, adjust properties like mass via sliders, and confirm the UI reflects changes without errors."
          },
          {
            "id": 3,
            "title": "Sync property changes to physics engine",
            "description": "Ensure changes from the property panel are synced to Three.js and Rapier physics by reloading the scene or updating colliders.",
            "dependencies": [9, 1, 2],
            "details": "In the Elm update function, after handling UpdateObjectProperty, use ports to send updated scene data to JavaScript. In the PhysicsRenderer, implement methods to update existing colliders or reload the scene with new properties. Handle cases where simulation is running or paused.",
            "status": "completed",
            "testStrategy": "Edit properties (e.g., increase mass), simulate, and observe if physics behavior changes correctly, such as objects falling at different speeds."
          }
        ]
      },
      {
        "id": 11,
        "title": "Add AI refinement feature",
        "description": "Implement /api/refine endpoint in backend and UI in Elm for refining scenes with text instructions.",
        "details": "In main.py, create refine_scene function with prompt to modify existing scene JSON. In Elm, add refine textarea and button, handle RefineScene message, update scene on response. Cache refinements if possible.",
        "testStrategy": "Generate a scene, enter refinement text (e.g., 'make boxes red'), verify scene updates correctly, and check backend returns modified JSON without errors.",
        "priority": "medium",
        "dependencies": [6, 10],
        "status": "completed",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement /api/refine endpoint in backend",
            "description": "Create the refine_scene function in main.py that takes a prompt to modify an existing scene JSON and returns the refined scene.",
            "dependencies": [6, 10],
            "details": "In main.py, define a refine_scene function that accepts a scene JSON and a text prompt, uses AI to refine the scene based on the prompt, and returns the modified JSON. Ensure the endpoint handles caching of refinements if possible to optimize performance.",
            "status": "completed",
            "testStrategy": "Send a POST request to /api/refine with a sample scene JSON and a refinement prompt (e.g., 'make boxes red'), verify the response is a valid modified JSON without errors, and check that the scene updates correctly."
          },
          {
            "id": 2,
            "title": "Add UI elements in Elm for scene refinement",
            "description": "Integrate a textarea and button in the Elm UI to allow users to input refinement instructions and trigger the refinement process.",
            "dependencies": [6, 10],
            "details": "In Elm, add a refine textarea and button to the UI, implement a RefineScene message to handle user input, send the refinement request to the backend, and update the scene state upon receiving the response. Include handling for loading states and potential errors.",
            "status": "completed",
            "testStrategy": "In the Elm UI, enter refinement text (e.g., 'make boxes red') in the textarea, click the refine button, verify the scene updates visually, and ensure no errors occur during the process."
          }
        ]
      },
      {
        "id": 12,
        "title": "Implement undo/redo system and local storage",
        "description": "Add undo/redo functionality in Elm for scene changes and integrate IndexedDB for saving/loading scenes.",
        "details": "In Elm, track history of scene states, implement undo/redo messages. Use ports to save/load scenes to/from localStorage or IndexedDB. Add buttons or shortcuts for undo/redo.",
        "testStrategy": "Make changes to scene (e.g., move object, refine), use undo to revert, verify state restores, and test save/load by refreshing page and reloading scene.",
        "priority": "medium",
        "dependencies": [2, 11],
        "status": "completed",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement scene history tracking in Elm model",
            "description": "Extend the Elm model to maintain a history stack of scene states, allowing for undo and redo operations by storing previous and future states.",
            "dependencies": [2],
            "details": "In the Elm code, add a history field to the Model type, such as a list or stack of Scene states. Update the update function to push current state to history on scene changes (e.g., object moves, property edits). Ensure history is limited to prevent memory issues, perhaps keeping the last 50 states.",
            "status": "completed",
            "testStrategy": "Simulate scene changes in Elm debugger, verify history list updates correctly, and check that states are stored accurately for reversion."
          },
          {
            "id": 2,
            "title": "Add undo/redo messages and UI controls",
            "description": "Implement Msg types for Undo and Redo, handle them in the update function to revert or advance scene states, and add buttons or keyboard shortcuts in the Elm UI.",
            "dependencies": [1],
            "details": "Define Undo and Redo messages in the Msg type. In the update function, on Undo, pop from history and set model to previous state; on Redo, advance forward. Add buttons to the UI (e.g., in viewBottomBar) and handle keyboard events (Ctrl+Z, Ctrl+Y). Ensure UI updates reflect current undo/redo availability.",
            "status": "completed",
            "testStrategy": "Click undo/redo buttons or use shortcuts after making scene changes, verify the scene reverts or advances correctly, and check that buttons disable when no history is available."
          },
          {
            "id": 3,
            "title": "Integrate IndexedDB for saving and loading scenes",
            "description": "Use Elm ports to communicate with JavaScript for saving the current scene to IndexedDB and loading it back, ensuring persistence across sessions.",
            "dependencies": [2, 11],
            "details": "Set up ports in Elm for sending save/load commands. In JavaScript, implement IndexedDB operations to store scene data as JSON and retrieve it. Handle save on scene changes or user action, and load on app initialization or user request. Ensure data includes scene objects, properties, and history if needed.",
            "status": "completed",
            "testStrategy": "Make scene changes, trigger save, refresh the page, and verify the scene loads correctly from IndexedDB. Test loading invalid data handles errors gracefully."
          }
        ]
      },
      {
        "id": 13,
        "title": "Add keyboard shortcuts and UI polish",
        "description": "Implement keyboard event handling for shortcuts (e.g., Space for play, R for rotate mode) and polish the UI with better styling, loading states, and error handling.",
        "details": "In Elm subscriptions, add onKeyDown decoder. Handle KeyPressed messages for shortcuts. Update CSS in index.html for better appearance, add loading indicators for generation.",
        "testStrategy": "Test shortcuts (e.g., press Space to toggle simulation, R to enter rotate mode), verify UI responsiveness, and check error messages display correctly on failures.",
        "priority": "low",
        "dependencies": [10, 12],
        "status": "completed",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Keyboard Shortcuts and UI Polish",
            "description": "Add keyboard event handling for shortcuts like Space for play and R for rotate mode, and enhance UI with better styling, loading states, and error handling.",
            "dependencies": [10, 12],
            "details": "In Elm subscriptions, add an onKeyDown decoder to capture key presses. Handle KeyPressed messages to trigger actions such as toggling simulation or entering rotate mode. Update CSS in index.html for improved appearance, including better button styles and layout. Add loading indicators for scene generation processes and implement error handling to display messages on failures, ensuring the UI remains responsive.",
            "status": "completed",
            "testStrategy": "Test keyboard shortcuts by pressing Space to toggle simulation and R to enter rotate mode, verify UI responsiveness with loading indicators during generation, and check that error messages display correctly on failures."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-11-12T21:11:39.802Z",
      "updated": "2025-11-12T23:30:00.000Z",
      "description": "Tasks for master context"
    }
  }
}
          {
            "id": 2,
            "title": "Implement update function for message handling",
            "description": "Implement the update function to handle various Msg types, managing state transitions for text input, scene generation, object selection, transform updates, and simulation toggling.",
            "dependencies": [
              1
            ],
            "details": "In the update function, pattern match on Msg variants to update the Model accordingly, ensuring pure functions without side effects. Handle dependencies like updating scene from SceneGenerated, selecting objects via ObjectClicked, and toggling simulation state. Use Dict operations for object management and track initial states for reset.",
            "status": "pending",
            "testStrategy": "Simulate message dispatches in Elm debugger (e.g., send UpdateTextInput, GenerateScene) and verify model updates correctly without errors or side effects."
          },
          {
            "id": 3,
            "title": "Integrate ports for pure state management and external communication",
            "description": "Set up ports for communication with JavaScript (e.g., for scene generation, object clicks, and simulation commands) while maintaining pure Elm state management.",
            "dependencies": [
              1,
              2
            ],
            "details": "Define incoming and outgoing ports for messages like sceneGenerated, objectClicked, and commands to update Three.js. Ensure the update function uses Cmd for port calls and subscriptions for incoming data. Track initial scene states in Model for reset, and avoid direct side effects in update logic.",
            "status": "pending",
            "testStrategy": "Test port integration by triggering Elm actions (e.g., generate scene) and verifying JavaScript receives correct data; check that state remains pure by reloading and confirming no unexpected changes."
          }
        ]
      },
      {
         "id": 3,
         "title": "Set up Three.js renderer and basic scene rendering",
         "description": "Initialize Three.js in the JavaScript bridge, set up the renderer, camera, controls, lights, and ground plane, and implement the loadScene method to render objects from Elm data.",
         "details": "In index.js, create the PhysicsRenderer class with setupRenderer, setupCamera, setupControls (OrbitControls), setupLights, and setupPhysics (Rapier world). Implement loadScene to clear existing objects and add new ones using addObject, which creates Three.js meshes and Rapier bodies/colliders for Box, Sphere, Cylinder. Handle geometry creation based on object type and apply materials with visual properties.",
         "testStrategy": "Run the app, verify Three.js renders a basic scene (e.g., ground plane), and check console for no WebGL errors. Manually test camera orbiting and ensure Rapier initializes without issues.",
         "priority": "high",
         "dependencies": [
           1
         ],
         "status": "completed",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Three.js renderer and camera",
            "description": "Set up the basic Three.js renderer and camera in the PhysicsRenderer class to enable scene rendering.",
            "dependencies": [],
            "details": "In index.js, within the PhysicsRenderer class, implement the setupRenderer method to create a WebGLRenderer, set its size to match the canvas container, and append it to the DOM. Implement setupCamera to create a PerspectiveCamera with appropriate field of view, aspect ratio, near and far planes, and position it suitably for the scene.",
            "status": "pending",
            "testStrategy": "Verify that the renderer initializes without WebGL errors and the camera is positioned correctly by checking the canvas renders a basic view."
          },
          {
            "id": 2,
            "title": "Set up OrbitControls and lighting",
            "description": "Configure OrbitControls for camera manipulation and add basic lighting to the scene for proper object visibility.",
            "dependencies": [
              1
            ],
            "details": "Implement setupControls to initialize OrbitControls with the camera and renderer DOM element, enabling mouse-based orbiting, zooming, and panning. Implement setupLights to add ambient and directional lights to the scene, positioning them to illuminate objects effectively and include a ground plane mesh for reference.",
            "status": "pending",
            "testStrategy": "Test camera controls by orbiting and zooming in the rendered scene, and ensure lights illuminate objects without shadows or rendering artifacts."
          },
          {
            "id": 3,
            "title": "Implement scene loading with Rapier physics integration",
            "description": "Develop the loadScene method to handle scene updates from Elm, integrating Three.js meshes with Rapier physics bodies and colliders.",
            "dependencies": [
              2
            ],
            "details": "Implement setupPhysics to initialize the Rapier world with gravity. Create the loadScene method to clear existing meshes and physics bodies, then use addObject to iterate through Elm-provided objects, creating Three.js geometries (BoxGeometry, SphereGeometry, CylinderGeometry) and meshes with materials based on visual properties. For each object, add corresponding Rapier rigid bodies and colliders (cuboid, ball, cylinder) linked to the meshes.",
            "status": "pending",
            "testStrategy": "Load a sample scene via Elm ports, verify objects render correctly with physics applied (e.g., falling under gravity), and check console for no Rapier initialization errors."
          }
        ]
      },
      {
         "id": 4,
         "title": "Integrate Rapier physics simulation loop",
         "description": "Connect Rapier to the animation loop, sync Three.js meshes with physics bodies during simulation, and handle start/pause/reset commands from Elm.",
         "details": "In the animate method, step the Rapier world when simulating, update mesh positions and rotations from body data, and send frame updates to Elm via ports. Implement startSimulation, pauseSimulation, and resetSimulation methods that detach transform controls and reload scene on reset. Ensure 60 FPS simulation with proper timestep.",
         "testStrategy": "Trigger simulation via Elm, observe objects falling realistically under gravity, check that pause stops updates, and reset restores initial positions. Use browser dev tools to monitor FPS and ensure no performance drops.",
         "priority": "high",
         "dependencies": [
           3
         ],
         "status": "completed",
        "subtasks": [
          {
            "id": 1,
            "title": "Connect Rapier to the animation loop",
            "description": "Modify the animate method to step the Rapier world during simulation, ensuring proper timestep for 60 FPS.",
            "dependencies": [
              3
            ],
            "details": "In the PhysicsRenderer class, update the animate method to call world.step() when simulation is active, using a fixed timestep like 1/60 to maintain consistent physics updates. Integrate this with the requestAnimationFrame loop to synchronize physics stepping with rendering.",
            "status": "pending",
            "testStrategy": "Monitor FPS in browser dev tools during simulation to ensure it stays at 60 FPS without drops."
          },
          {
            "id": 2,
            "title": "Sync Three.js meshes with physics bodies",
            "description": "Update mesh positions and rotations from Rapier body data during each animation frame.",
            "dependencies": [
              1
            ],
            "details": "In the animate method, after stepping the world, iterate through all physics bodies, retrieve their positions and rotations, and apply them to the corresponding Three.js meshes using mesh.position.copy(body.translation()) and mesh.quaternion.copy(body.rotation()). Send frame updates to Elm via ports if needed.",
            "status": "pending",
            "testStrategy": "Trigger simulation and observe objects falling realistically under gravity, verifying that meshes follow physics bodies accurately."
          },
          {
            "id": 3,
            "title": "Implement start, pause, and reset simulation commands",
            "description": "Add methods to handle startSimulation, pauseSimulation, and resetSimulation commands from Elm.",
            "dependencies": [
              2
            ],
            "details": "Implement startSimulation to begin stepping the world and detach transform controls, pauseSimulation to stop stepping, and resetSimulation to reload the scene, restore initial positions, and reattach controls. Ensure these methods are called via Elm ports and handle state transitions properly.",
            "status": "pending",
            "testStrategy": "Send commands from Elm to start, pause, and reset simulation, checking that pause stops updates, reset restores positions, and controls detach/reattach correctly."
          }
        ]
      },
      {
         "id": 5,
         "title": "Set up FastAPI backend server with CORS and basic structure",
         "description": "Initialize the FastAPI application, configure CORS for local development, and define the basic endpoint structure including models for Vec3, Transform, PhysicsObject, Scene, etc.",
         "details": "Create main.py with FastAPI app, add CORSMiddleware for localhost:5173. Define Pydantic models matching the JSON schema. Set up uvicorn for running the server on port 8000. Include placeholders for AI client and cache initialization. Ensure requirements.txt includes fastapi, uvicorn, pydantic.",
         "testStrategy": "Start the server with uvicorn, verify it runs on localhost:8000, test CORS by making a request from the frontend, and check that models validate JSON correctly without errors.",
         "priority": "high",
         "dependencies": [],
         "status": "completed",
         "subtasks": [
           {
             "id": 1,
             "title": "Initialize FastAPI application with CORS and define Pydantic models",
             "description": "Set up the FastAPI app in main.py, configure CORSMiddleware for localhost:5173, and define Pydantic models for Vec3, Transform, PhysicsObject, Scene, etc., matching the JSON schema.",
             "dependencies": [],
             "details": "Create main.py with FastAPI instance, add CORSMiddleware allowing origins from localhost:5173. Define Pydantic BaseModel classes for Vec3, Transform, PhysicsObject, Scene, and other required models based on the provided JSON schema. Include placeholders for AI client and cache initialization. Ensure requirements.txt includes fastapi, uvicorn, and pydantic. Set up uvicorn to run the server on port 8000.",
             "status": "completed",
             "testStrategy": "Start the server with uvicorn and verify it runs on localhost:8000. Test CORS by making a cross-origin request from the frontend and confirm models validate JSON inputs without errors."
           }
         ]
       },
       {
         "id": 6,
         "title": "Integrate AI for scene generation",
         "description": "Implement the /api/generate endpoint to use AI API for generating scenes from text prompts, including caching with LMDB.",
         "details": "In main.py, implement generate_scene function with prompt template, call AI API, parse response as Scene, cache successful results using hashlib and lmdb. Handle errors for JSON parsing and API failures. Use environment variables for AI model configuration.",
         "testStrategy": "Send POST requests to /api/generate with sample text (e.g., 'red box'), verify JSON response matches schema, check cache by repeating request (should be faster), and ensure invalid prompts return appropriate errors.",
         "priority": "high",
         "dependencies": [
           5
         ],
         "status": "completed",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement /api/generate endpoint with AI API integration",
            "description": "Create the /api/generate endpoint in main.py that accepts text prompts, uses a prompt template, calls the AI API, and parses the response into a Scene object. Handle errors for API failures and JSON parsing issues. Configure AI model settings via environment variables.",
            "dependencies": [
              5
            ],
            "details": "In main.py, define the generate_scene function that constructs a prompt template for scene generation, makes an API call to AI using the configured model (from env vars), parses the JSON response into a Scene object, and returns it. Implement error handling for invalid responses or API errors, raising appropriate exceptions. Ensure the endpoint is a POST route that accepts JSON with a 'prompt' field.",
            "status": "pending",
            "testStrategy": "Send POST requests to /api/generate with valid text prompts (e.g., 'red box'), verify the JSON response conforms to the Scene schema, and check that invalid prompts or API failures return proper error messages."
          },
          {
            "id": 2,
            "title": "Add LMDB caching for generated scenes",
            "description": "Integrate LMDB caching to store successful scene generation results using hashlib for key generation, ensuring repeated requests for the same prompt are served from cache for improved performance.",
            "dependencies": [
              1
            ],
            "details": "In the generate_scene function, before calling the Claude API, compute a hash of the prompt using hashlib (e.g., SHA256), check if the result exists in an LMDB database. If cached, return the stored Scene; otherwise, proceed with API call and cache the successful result in LMDB. Handle cache misses and ensure thread-safe access if needed. Use environment variables for LMDB path configuration.",
            "status": "pending",
            "testStrategy": "Send identical POST requests to /api/generate, measure response times to verify caching (second request should be faster), and inspect LMDB database to confirm cached entries are stored correctly."
          }
        ]
      },
      {
        "id": 7,
        "title": "Add Genesis physics validation to backend",
        "description": "Implement validate_with_genesis function and integrate it into scene generation and a separate /api/validate endpoint.",
        "details": "Use genesis-world library to create a scene, add entities based on object types, simulate 60 frames, and check for stability. Return validation results. Integrate into generate_scene to reject invalid scenes. Ensure Genesis handles Box and Sphere types primarily.",
        "testStrategy": "Test /api/validate with valid and invalid scenes (e.g., overlapping objects), verify it detects issues like instability, and ensure generate_scene fails gracefully on validation errors.",
        "priority": "medium",
        "dependencies": [
          5,
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement validate_with_genesis function using Genesis library",
            "description": "Create the validate_with_genesis function that uses genesis-world to build a scene, add Box and Sphere entities, simulate 60 frames, and check for stability.",
            "dependencies": [
              5,
              6
            ],
            "details": "In the backend, import genesis-world, define validate_with_genesis(scene_data) that parses object types, creates a Genesis scene, adds rigid bodies for Boxes and Spheres with appropriate properties, runs a 60-frame simulation, and evaluates stability by checking for excessive movement or collisions. Return a validation result object indicating pass/fail with details.",
            "status": "pending",
            "testStrategy": "Unit test the function with mock scene data containing stable and unstable configurations, verifying it correctly identifies stability issues like overlapping objects falling apart."
          },
          {
            "id": 2,
            "title": "Integrate Genesis validation into scene generation and API endpoint",
            "description": "Integrate the validate_with_genesis function into generate_scene to reject invalid scenes and expose it via a new /api/validate endpoint.",
            "dependencies": [
              1
            ],
            "details": "Modify generate_scene to call validate_with_genesis after scene creation and reject if validation fails, raising an appropriate error. Create a new Flask route /api/validate that accepts scene data via POST, calls validate_with_genesis, and returns the validation results as JSON. Ensure error handling for unsupported object types beyond Box and Sphere.",
            "status": "pending",
            "testStrategy": "Test generate_scene by attempting to generate invalid scenes (e.g., unstable stacks) and confirm rejection. Hit /api/validate with valid/invalid JSON payloads and verify correct pass/fail responses, including error messages for unsupported types."
          }
        ]
      },
      {
         "id": 8,
         "title": "Implement object selection and raycasting in Three.js",
         "description": "Add click event handling for object selection using raycasting, highlight selected objects, and communicate selections back to Elm.",
         "details": "In setupControls, add mouse event listener for clicks, use Raycaster to intersect meshes, send objectClicked event to Elm. Implement selectObject to attach TransformControls to the selected mesh. Update material or add outline for visual feedback.",
         "testStrategy": "Click on rendered objects in the scene, verify selection highlights appear, TransformControls attach, and Elm receives the correct objectId via ports.",
         "priority": "high",
         "dependencies": [
           2,
           3
         ],
         "status": "completed",
        "subtasks": [
          {
            "id": 1,
            "title": "Add click event handling and raycasting for object detection",
            "description": "Implement mouse click event listener in setupControls to detect clicks on the canvas, use Three.js Raycaster to intersect with meshes in the scene, and identify the selected object.",
            "dependencies": [
              2,
              3
            ],
            "details": "In the setupControls method of the PhysicsRenderer class, add an event listener for 'click' events on the renderer.domElement. Calculate mouse position normalized to [-1,1], create a Raycaster, set its origin from the camera and direction towards the mouse position, and intersect with the scene's meshes. Store the intersected object for further processing.",
            "status": "pending",
            "testStrategy": "Click on various objects in the rendered scene and verify that the raycaster correctly detects intersections by logging the selected object ID in the console."
          },
          {
            "id": 2,
            "title": "Implement object selection with visual feedback and Elm communication",
            "description": "Handle the selected object by highlighting it, attaching TransformControls, and sending the selection event to Elm via ports.",
            "dependencies": [
              1
            ],
            "details": "After detecting the selected object via raycasting, implement a selectObject method that updates the object's material (e.g., change color or add an outline using a post-processing effect or emissive property), attaches TransformControls to the mesh for manipulation, and sends an 'objectClicked' event with the object ID to Elm through the defined ports. Ensure previous selections are deselected properly.",
            "status": "pending",
            "testStrategy": "Click on objects, check that visual highlights appear (e.g., outline or color change), TransformControls attach and allow dragging, and verify Elm receives the correct objectId via port messages in the debugger."
          }
        ]
      },
      {
         "id": 9,
         "title": "Add transform controls for translate, rotate, and scale",
         "description": "Integrate TransformControls for manipulating selected objects, handle mode switching from Elm, and sync changes back to Elm state.",
         "details": "In PhysicsRenderer, set up TransformControls with event listeners for dragging and objectChange. Implement setTransformMode to switch between translate, rotate, scale. On change, send transformUpdate to Elm. Ensure controls detach during simulation.",
         "testStrategy": "Select an object, press keys (G/R/S) to switch modes, drag to transform, verify real-time updates in Elm model and visual changes in Three.js.",
         "priority": "high",
         "dependencies": [
           8
         ],
         "status": "completed",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up TransformControls with event listeners in PhysicsRenderer",
            "description": "Initialize TransformControls in the Three.js scene, attach it to the selected object, and add event listeners for 'dragging' and 'objectChange' to handle real-time updates.",
            "dependencies": [],
            "details": "In the PhysicsRenderer class, create a new TransformControls instance attached to the camera and scene. Add event listeners: on 'dragging' to prevent other interactions, and on 'objectChange' to capture transform changes. Ensure the controls are properly disposed when not in use.",
            "status": "pending",
            "testStrategy": "Attach controls to a selected object and verify that dragging updates the object's position/rotation/scale visually in Three.js without errors."
          },
          {
            "id": 2,
            "title": "Implement mode switching and Elm synchronization for TransformControls",
            "description": "Add setTransformMode method to switch between translate, rotate, and scale modes based on Elm input, and send transform updates back to Elm via ports.",
            "dependencies": [
              1
            ],
            "details": "Implement setTransformMode function that sets the controls.mode to 'translate', 'rotate', or 'scale' based on messages from Elm. On 'objectChange' event, send a transformUpdate message to Elm with the updated position, rotation, and scale. Ensure controls detach during simulation to prevent conflicts.",
            "status": "pending",
            "testStrategy": "Switch modes via Elm (e.g., key presses), perform transformations, and check that Elm state updates correctly and reflects in the UI or subsequent actions."
          }
        ]
      },
      {
        "id": 10,
        "title": "Implement simulation controls and property editing panel",
        "description": "Add play/pause/reset buttons in Elm UI, connect to physics commands, and implement the right panel for editing object properties like mass, friction, etc.",
        "details": "In Elm, add buttons for ToggleSimulation and ResetSimulation, update viewBottomBar and viewRightPanel. Handle UpdateObjectProperty messages to modify scene. Sync property changes to Three.js by reloading scene or updating colliders. Ensure sliders update physics properties.",
        "testStrategy": "Edit properties in the panel (e.g., increase mass), simulate, observe different behaviors (e.g., heavier objects fall slower), and test play/pause/reset functionality with button clicks.",
        "priority": "high",
        "dependencies": [
          2,
          4,
          9
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Add UI buttons for simulation controls",
            "description": "Implement play/pause/reset buttons in the Elm UI bottom bar, connecting them to ToggleSimulation and ResetSimulation messages.",
            "dependencies": [
              2,
              4
            ],
            "details": "Update the viewBottomBar function in Elm to include buttons for toggling simulation state and resetting the scene. Ensure buttons dispatch appropriate messages to the update function, and handle state changes for simulation running/paused.",
            "status": "pending",
            "testStrategy": "Click play/pause/reset buttons and verify simulation starts, pauses, or resets correctly, observing physics behavior in the scene."
          },
          {
            "id": 2,
            "title": "Implement property editing panel",
            "description": "Create the right panel in Elm UI for editing object properties such as mass, friction, and other physics attributes using sliders and inputs.",
            "dependencies": [
              2
            ],
            "details": "Update the viewRightPanel function to display editable properties for selected objects. Handle UpdateObjectProperty messages in the update function to modify the scene model. Ensure the panel only shows when an object is selected and updates dynamically.",
            "status": "pending",
            "testStrategy": "Select an object, adjust properties like mass via sliders, and confirm the UI reflects changes without errors."
          },
          {
            "id": 3,
            "title": "Sync property changes to physics engine",
            "description": "Ensure changes from the property panel are synced to Three.js and Rapier physics by reloading the scene or updating colliders.",
            "dependencies": [
              9,
              1,
              2
            ],
            "details": "In the Elm update function, after handling UpdateObjectProperty, use ports to send updated scene data to JavaScript. In the PhysicsRenderer, implement methods to update existing colliders or reload the scene with new properties. Handle cases where simulation is running or paused.",
            "status": "pending",
            "testStrategy": "Edit properties (e.g., increase mass), simulate, and observe if physics behavior changes correctly, such as objects falling at different speeds."
          }
        ]
      },
      {
        "id": 11,
        "title": "Add AI refinement feature",
        "description": "Implement /api/refine endpoint in backend and UI in Elm for refining scenes with text instructions.",
        "details": "In main.py, create refine_scene function with prompt to modify existing scene JSON. In Elm, add refine textarea and button, handle RefineScene message, update scene on response. Cache refinements if possible.",
        "testStrategy": "Generate a scene, enter refinement text (e.g., 'make boxes red'), verify scene updates correctly, and check backend returns modified JSON without errors.",
        "priority": "medium",
        "dependencies": [
          6,
          10
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement /api/refine endpoint in backend",
            "description": "Create the refine_scene function in main.py that takes a prompt to modify an existing scene JSON and returns the refined scene.",
            "dependencies": [
              6,
              10
            ],
            "details": "In main.py, define a refine_scene function that accepts a scene JSON and a text prompt, uses AI to refine the scene based on the prompt, and returns the modified JSON. Ensure the endpoint handles caching of refinements if possible to optimize performance.",
            "status": "pending",
            "testStrategy": "Send a POST request to /api/refine with a sample scene JSON and a refinement prompt (e.g., 'make boxes red'), verify the response is a valid modified JSON without errors, and check that the scene updates correctly."
          },
          {
            "id": 2,
            "title": "Add UI elements in Elm for scene refinement",
            "description": "Integrate a textarea and button in the Elm UI to allow users to input refinement instructions and trigger the refinement process.",
            "dependencies": [
              6,
              10
            ],
            "details": "In Elm, add a refine textarea and button to the UI, implement a RefineScene message to handle user input, send the refinement request to the backend, and update the scene state upon receiving the response. Include handling for loading states and potential errors.",
            "status": "pending",
            "testStrategy": "In the Elm UI, enter refinement text (e.g., 'make boxes red') in the textarea, click the refine button, verify the scene updates visually, and ensure no errors occur during the process."
          }
        ]
      },
      {
        "id": 12,
        "title": "Implement undo/redo system and local storage",
        "description": "Add undo/redo functionality in Elm for scene changes and integrate IndexedDB for saving/loading scenes.",
        "details": "In Elm, track history of scene states, implement undo/redo messages. Use ports to save/load scenes to/from localStorage or IndexedDB. Add buttons or shortcuts for undo/redo.",
        "testStrategy": "Make changes to scene (e.g., move object, refine), use undo to revert, verify state restores, and test save/load by refreshing page and reloading scene.",
        "priority": "medium",
        "dependencies": [
          2,
          11
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement scene history tracking in Elm model",
            "description": "Extend the Elm model to maintain a history stack of scene states, allowing for undo and redo operations by storing previous and future states.",
            "dependencies": [
              2
            ],
            "details": "In the Elm code, add a history field to the Model type, such as a list or stack of Scene states. Update the update function to push current state to history on scene changes (e.g., object moves, property edits). Ensure history is limited to prevent memory issues, perhaps keeping the last 50 states.",
            "status": "pending",
            "testStrategy": "Simulate scene changes in Elm debugger, verify history list updates correctly, and check that states are stored accurately for reversion."
          },
          {
            "id": 2,
            "title": "Add undo/redo messages and UI controls",
            "description": "Implement Msg types for Undo and Redo, handle them in the update function to revert or advance scene states, and add buttons or keyboard shortcuts in the Elm UI.",
            "dependencies": [
              1
            ],
            "details": "Define Undo and Redo messages in the Msg type. In the update function, on Undo, pop from history and set model to previous state; on Redo, advance forward. Add buttons to the UI (e.g., in viewBottomBar) and handle keyboard events (Ctrl+Z, Ctrl+Y). Ensure UI updates reflect current undo/redo availability.",
            "status": "pending",
            "testStrategy": "Click undo/redo buttons or use shortcuts after making scene changes, verify the scene reverts or advances correctly, and check that buttons disable when no history is available."
          },
          {
            "id": 3,
            "title": "Integrate IndexedDB for saving and loading scenes",
            "description": "Use Elm ports to communicate with JavaScript for saving the current scene to IndexedDB and loading it back, ensuring persistence across sessions.",
            "dependencies": [
              2,
              11
            ],
            "details": "Set up ports in Elm for sending save/load commands. In JavaScript, implement IndexedDB operations to store scene data as JSON and retrieve it. Handle save on scene changes or user action, and load on app initialization or user request. Ensure data includes scene objects, properties, and history if needed.",
            "status": "pending",
            "testStrategy": "Make scene changes, trigger save, refresh the page, and verify the scene loads correctly from IndexedDB. Test loading invalid data handles errors gracefully."
          }
        ]
      },
      {
        "id": 13,
        "title": "Add keyboard shortcuts and UI polish",
        "description": "Implement keyboard event handling for shortcuts (e.g., Space for play, R for rotate mode) and polish the UI with better styling, loading states, and error handling.",
        "details": "In Elm subscriptions, add onKeyDown decoder. Handle KeyPressed messages for shortcuts. Update CSS in index.html for better appearance, add loading indicators for generation.",
        "testStrategy": "Test shortcuts (e.g., press Space to toggle simulation, R to enter rotate mode), verify UI responsiveness, and check error messages display correctly on failures.",
        "priority": "low",
        "dependencies": [
          10,
          12
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Keyboard Shortcuts and UI Polish",
            "description": "Add keyboard event handling for shortcuts like Space for play and R for rotate mode, and enhance UI with better styling, loading states, and error handling.",
            "dependencies": [
              10,
              12
            ],
            "details": "In Elm subscriptions, add an onKeyDown decoder to capture key presses. Handle KeyPressed messages to trigger actions such as toggling simulation or entering rotate mode. Update CSS in index.html for improved appearance, including better button styles and layout. Add loading indicators for scene generation processes and implement error handling to display messages on failures, ensuring the UI remains responsive.",
            "status": "pending",
            "testStrategy": "Test keyboard shortcuts by pressing Space to toggle simulation and R to enter rotate mode, verify UI responsiveness with loading indicators during generation, and check that error messages display correctly on failures."
          }
        ]
      }
    ],
    "metadata": {
       "created": "2025-11-12T21:11:39.802Z",
       "updated": "2025-11-12T23:30:00.000Z",
       "description": "Tasks for master context"
     }
  }
}
</file>

<file path="src/PhysicsRenderer.js">
import * as THREE from 'three';
import { OrbitControls } from 'three/examples/jsm/controls/OrbitControls.js';
import { TransformControls } from 'three/examples/jsm/controls/TransformControls.js';
import RAPIER from '@dimforge/rapier3d';

class PhysicsRenderer {
    constructor(canvasContainer) {
        this.canvasContainer = canvasContainer;
        this.scene = null;
        this.camera = null;
        this.renderer = null;
        this.controls = null;
        this.world = null;
        this.objects = new Map();
        this.animationId = null;
        this.isInitialized = false;
        this.raycaster = new THREE.Raycaster();
        this.mouse = new THREE.Vector2();
        this.selectedObjectId = null;
        this.transformControls = null;
        this.currentTransformMode = 'translate';
    }

    async init() {
        if (this.isInitialized) return;

        try {
            // Initialize Rapier physics - default export is already initialized
            const gravity = { x: 0, y: -9.81, z: 0 };
            this.world = new RAPIER.World(gravity);

            // Initialize Three.js
            this.setupRenderer();
            this.setupCamera();
            this.setupControls();
            this.setupLights();
            this.setupGround();

            this.isInitialized = true;
            console.log('PhysicsRenderer initialized successfully');
        } catch (error) {
            console.error('Failed to initialize PhysicsRenderer:', error);
        }
    }

    setupRenderer() {
        this.scene = new THREE.Scene();

        this.renderer = new THREE.WebGLRenderer({ antialias: true });
        this.renderer.setSize(this.canvasContainer.clientWidth, this.canvasContainer.clientHeight);
        this.renderer.setClearColor(0x000000);
        this.renderer.shadowMap.enabled = true;
        this.renderer.shadowMap.type = THREE.PCFSoftShadowMap;
        this.canvasContainer.appendChild(this.renderer.domElement);

        // Handle window resize
        window.addEventListener('resize', () => {
            this.camera.aspect = this.canvasContainer.clientWidth / this.canvasContainer.clientHeight;
            this.camera.updateProjectionMatrix();
            this.renderer.setSize(this.canvasContainer.clientWidth, this.canvasContainer.clientHeight);
        });
    }

    setupCamera() {
        const aspect = this.canvasContainer.clientWidth / this.canvasContainer.clientHeight;
        this.camera = new THREE.PerspectiveCamera(75, aspect, 0.1, 1000);
        this.camera.position.set(5, 5, 5);
        this.camera.lookAt(0, 0, 0);
    }

    setupControls() {
        this.controls = new OrbitControls(this.camera, this.renderer.domElement);
        this.controls.enableDamping = true;
        this.controls.dampingFactor = 0.05;
        this.controls.enableZoom = true;
        this.controls.enablePan = true;

        // Initialize transform controls
        this.transformControls = new TransformControls(this.camera, this.renderer.domElement);
        this.transformControls.setMode(this.currentTransformMode);
        this.transformControls.setSize(0.8);
        this.transformControls.addEventListener('objectChange', () => this.onTransformChange());

        // Disable orbit controls when dragging transform controls
        this.transformControls.addEventListener('dragging-changed', (event) => {
            this.controls.enabled = !event.value;
        });

        this.scene.add(this.transformControls);

        // Add mouse event listeners for object selection
        this.renderer.domElement.addEventListener('click', (event) => this.onMouseClick(event));
        this.renderer.domElement.addEventListener('mousemove', (event) => this.onMouseMove(event));
    }

    setupLights() {
        // Ambient light
        const ambientLight = new THREE.AmbientLight(0x404040, 0.4);
        this.scene.add(ambientLight);

        // Directional light
        const directionalLight = new THREE.DirectionalLight(0xffffff, 0.8);
        directionalLight.position.set(10, 10, 5);
        directionalLight.castShadow = true;
        directionalLight.shadow.mapSize.width = 2048;
        directionalLight.shadow.mapSize.height = 2048;
        this.scene.add(directionalLight);
    }

    setupGround() {
        // Create ground plane
        const groundGeometry = new THREE.PlaneGeometry(20, 20);
        const groundMaterial = new THREE.MeshLambertMaterial({ color: 0x888888 });
        const ground = new THREE.Mesh(groundGeometry, groundMaterial);
        ground.rotation.x = -Math.PI / 2;
        ground.receiveShadow = true;
        ground.userData = { selectable: false }; // Mark ground as non-selectable
        this.scene.add(ground);

        // Add ground collider
        const groundColliderDesc = RAPIER.ColliderDesc.cuboid(10, 0.1, 10);
        this.world.createCollider(groundColliderDesc);
    }

    loadScene(sceneData) {
        if (!this.isInitialized) return;

        // Store initial scene for reset functionality
        if (!this.initialScene) {
            this.initialScene = JSON.parse(JSON.stringify(sceneData));
        }

        // Clear existing objects
        this.clearScene();

        // Parse scene data and create objects
        if (sceneData && sceneData.objects) {
            Object.values(sceneData.objects).forEach(objData => {
                this.addObject(objData);
            });
        }
    }

    addObject(objData) {
        const { id, transform, physicsProperties, visualProperties } = objData;

        // Create Three.js mesh
        const geometry = this.createGeometry(visualProperties.shape);
        const material = new THREE.MeshLambertMaterial({
            color: new THREE.Color(visualProperties.color)
        });
        const mesh = new THREE.Mesh(geometry, material);
        mesh.castShadow = true;
        mesh.receiveShadow = true;
        mesh.userData = { id, selectable: true }; // Mark mesh as selectable

        // Set transform
        mesh.position.set(transform.position.x, transform.position.y, transform.position.z);
        mesh.rotation.set(transform.rotation.x, transform.rotation.y, transform.rotation.z);
        mesh.scale.set(transform.scale.x, transform.scale.y, transform.scale.z);

        this.scene.add(mesh);

        // Create Rapier rigid body and collider
        const rigidBodyDesc = RAPIER.RigidBodyDesc.dynamic()
            .setTranslation(transform.position.x, transform.position.y, transform.position.z)
            .setRotation({ x: transform.rotation.x, y: transform.rotation.y, z: transform.rotation.z, w: 1 });

        const rigidBody = this.world.createRigidBody(rigidBodyDesc);
        rigidBody.setAdditionalMass(physicsProperties.mass);

        let colliderDesc;
        switch (visualProperties.shape) {
            case 'Box':
                colliderDesc = RAPIER.ColliderDesc.cuboid(
                    transform.scale.x / 2,
                    transform.scale.y / 2,
                    transform.scale.z / 2
                );
                break;
            case 'Sphere':
                colliderDesc = RAPIER.ColliderDesc.ball(transform.scale.x / 2);
                break;
            case 'Cylinder':
                colliderDesc = RAPIER.ColliderDesc.cylinder(transform.scale.y / 2, transform.scale.x / 2);
                break;
            default:
                colliderDesc = RAPIER.ColliderDesc.cuboid(0.5, 0.5, 0.5);
        }

        colliderDesc.setFriction(physicsProperties.friction);
        colliderDesc.setRestitution(physicsProperties.restitution);

        const collider = this.world.createCollider(colliderDesc, rigidBody);

        // Store object data
        this.objects.set(id, {
            mesh,
            rigidBody,
            collider,
            originalData: objData
        });
    }

    createGeometry(shape) {
        switch (shape) {
            case 'Box':
                return new THREE.BoxGeometry(1, 1, 1);
            case 'Sphere':
                return new THREE.SphereGeometry(0.5, 16, 16);
            case 'Cylinder':
                return new THREE.CylinderGeometry(0.5, 0.5, 1, 16);
            default:
                return new THREE.BoxGeometry(1, 1, 1);
        }
    }

    clearScene() {
        // Remove all rigid bodies and meshes
        this.objects.forEach(obj => {
            if (obj.rigidBody && this.world) {
                this.world.removeRigidBody(obj.rigidBody);
            }
            if (obj.mesh) {
                this.scene.remove(obj.mesh);
            }
        });

        // Clear objects map
        this.objects.clear();
    }

    startSimulation() {
        if (!this.animationId) {
            // Detach transform controls during simulation to prevent conflicts
            if (this.transformControls) {
                this.transformControls.detach();
            }
            this.animate();
        }
    }

    pauseSimulation() {
        if (this.animationId) {
            cancelAnimationFrame(this.animationId);
            this.animationId = null;
            // Re-attach transform controls when simulation is paused
            if (this.selectedObjectId && this.transformControls) {
                const selectedObject = this.objects.get(this.selectedObjectId);
                if (selectedObject) {
                    this.transformControls.attach(selectedObject.mesh);
                }
            }
        }
    }

    resetSimulation() {
        this.pauseSimulation();
        this.loadScene(this.initialScene);
    }

    animate = () => {
        this.animationId = requestAnimationFrame(this.animate);

        // Update physics
        if (!this.world) return;
        this.world.step();

        // Sync Three.js meshes with physics bodies
        this.objects.forEach(obj => {
            const position = obj.rigidBody.translation();
            const rotation = obj.rigidBody.rotation();

            obj.mesh.position.set(position.x, position.y, position.z);
            obj.mesh.quaternion.set(rotation.x, rotation.y, rotation.z, rotation.w);
        });

        // Update controls
        this.controls.update();

        // Render
        this.renderer.render(this.scene, this.camera);
    }

    onMouseClick(event) {
        if (!this.isInitialized) return;

        // Update mouse position
        this.updateMousePosition(event);

        // Perform raycasting
        this.raycaster.setFromCamera(this.mouse, this.camera);
        const intersects = this.raycaster.intersectObjects(this.scene.children, false);

        // Find the first selectable object
        let selectedObject = null;
        for (const intersect of intersects) {
            if (intersect.object.userData.selectable) {
                selectedObject = intersect.object;
                break;
            }
        }

        // Update selection
        this.selectObject(selectedObject ? selectedObject.userData.id : null);
    }

    onMouseMove(event) {
        if (!this.isInitialized) return;
        this.updateMousePosition(event);
    }

    updateMousePosition(event) {
        const rect = this.renderer.domElement.getBoundingClientRect();
        this.mouse.x = ((event.clientX - rect.left) / rect.width) * 2 - 1;
        this.mouse.y = -((event.clientY - rect.top) / rect.height) * 2 + 1;
    }

    selectObject(objectId) {
        // Clear previous selection
        if (this.selectedObjectId) {
            const prevObject = this.objects.get(this.selectedObjectId);
            if (prevObject) {
                // Reset material color
                prevObject.mesh.material.color.setHex(prevObject.originalData.visualProperties.color.replace('#', '0x'));
                // Detach transform controls
                this.transformControls.detach();
            }
        }

        this.selectedObjectId = objectId;

        // Apply selection highlight and attach transform controls
        if (objectId) {
            const selectedObject = this.objects.get(objectId);
            if (selectedObject) {
                // Highlight selected object (make it brighter)
                const originalColor = new THREE.Color(selectedObject.originalData.visualProperties.color);
                selectedObject.mesh.material.color.setRGB(
                    Math.min(originalColor.r * 1.3, 1),
                    Math.min(originalColor.g * 1.3, 1),
                    Math.min(originalColor.b * 1.3, 1)
                );
                // Attach transform controls (only if not simulating)
                if (!this.animationId) {
                    this.transformControls.attach(selectedObject.mesh);
                }
            }
        }

        // Notify Elm about selection change
        if (window.elmApp && window.elmApp.ports.sendSelectionToElm) {
            window.elmApp.ports.sendSelectionToElm.send(objectId);
        }
    }

    getSelectedObjectId() {
        return this.selectedObjectId;
    }

    setTransformMode(mode) {
        this.currentTransformMode = mode;
        if (this.transformControls) {
            this.transformControls.setMode(mode);
        }
    }

    onTransformChange() {
        if (!this.selectedObjectId) return;

        const selectedObject = this.objects.get(this.selectedObjectId);
        if (!selectedObject) return;

        const mesh = selectedObject.mesh;
        const rigidBody = selectedObject.rigidBody;

        // Update physics body position and rotation
        const position = mesh.position;
        const quaternion = mesh.quaternion;

        rigidBody.setTranslation({ x: position.x, y: position.y, z: position.z }, true);
        rigidBody.setRotation({ x: quaternion.x, y: quaternion.y, z: quaternion.z, w: quaternion.w }, true);

        // Notify Elm about transform change
        const transform = {
            position: { x: position.x, y: position.y, z: position.z },
            rotation: { x: mesh.rotation.x, y: mesh.rotation.y, z: mesh.rotation.z },
            scale: { x: mesh.scale.x, y: mesh.scale.y, z: mesh.scale.z }
        };

        if (window.elmApp && window.elmApp.ports.sendTransformUpdateToElm) {
            window.elmApp.ports.sendTransformUpdateToElm.send({
                objectId: this.selectedObjectId,
                transform: transform
            });
        }
    }

    destroy() {
        this.pauseSimulation();
        if (this.transformControls) {
            this.scene.remove(this.transformControls);
        }
        if (this.renderer) {
            // Remove event listeners
            this.renderer.domElement.removeEventListener('click', this.onMouseClick);
            this.renderer.domElement.removeEventListener('mousemove', this.onMouseMove);

            this.canvasContainer.removeChild(this.renderer.domElement);
            this.renderer.dispose();
        }
    }
}



export default PhysicsRenderer;
</file>

<file path=".gitignore">
# Dependencies
node_modules/

# Elm build artifacts
elm-stuff/

# Vite build cache
.vite/

# Environment variables
.env

# OS generated files
.DS_Store
Thumbs.db

# IDE files
.vscode/
.idea/

# Logs
*.log
npm-debug.log*

# Runtime data
pids
*.pid
*.seed

# Coverage directory used by tools like istanbul
coverage/

# Build outputs
dist/
build/
</file>

<file path="elm.json">
{
    "type": "application",
    "source-directories": [
        "src"
    ],
    "elm-version": "0.19.1",
    "dependencies": {
        "direct": {
            "elm/browser": "1.0.2",
            "elm/core": "1.0.5",
            "elm/html": "1.0.0",
            "elm/http": "2.0.0",
            "elm/json": "1.1.3",
            "elm/url": "1.0.0"
        },
        "indirect": {
            "elm/bytes": "1.0.8",
            "elm/file": "1.0.5",
            "elm/time": "1.0.0",
            "elm/virtual-dom": "1.0.2"
        }
    },
    "test-dependencies": {
        "direct": {},
        "indirect": {}
    }
}
</file>

<file path="index.html">
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Physics Simulator</title>
    <style>
        body {
            margin: 0;
            padding: 0;
            font-family: Arial, sans-serif;
            background-color: #f5f5f5;
        }

        .app-container {
            display: flex;
            height: 100vh;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }

        .left-panel {
            width: 320px;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: 24px;
            border-right: 2px solid #dee2e6;
            overflow-y: auto;
            box-shadow: 2px 0 8px rgba(0,0,0,0.1);
        }

        .canvas-container {
            flex: 1;
            position: relative;
            background: linear-gradient(135deg, #1a1a1a 0%, #2d2d2d 100%);
            border: 1px solid #333;
        }

        .right-panel {
            width: 320px;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: 24px;
            border-left: 2px solid #dee2e6;
            overflow-y: auto;
            box-shadow: -2px 0 8px rgba(0,0,0,0.1);
        }

        .bottom-bar {
            position: fixed;
            bottom: 0;
            left: 0;
            right: 0;
            background: linear-gradient(135deg, #ffffff 0%, #f8f9fa 100%);
            border-top: 2px solid #dee2e6;
            padding: 16px 24px;
            display: flex;
            justify-content: space-between;
            align-items: center;
            box-shadow: 0 -4px 12px rgba(0,0,0,0.1);
            backdrop-filter: blur(10px);
        }

        .simulation-controls {
            display: flex;
            gap: 10px;
        }

        .transform-controls {
            display: flex;
            gap: 10px;
        }

        .history-controls {
            display: flex;
            gap: 10px;
        }

        textarea {
            width: 100%;
            height: 100px;
            margin: 10px 0;
            padding: 8px;
            border: 1px solid #ccc;
            border-radius: 4px;
            resize: vertical;
            font-family: inherit;
        }

        button {
            background: linear-gradient(135deg, #007bff 0%, #0056b3 100%);
            color: white;
            border: none;
            padding: 10px 16px;
            border-radius: 6px;
            cursor: pointer;
            font-weight: 500;
            transition: all 0.2s ease;
            box-shadow: 0 2px 4px rgba(0,123,255,0.2);
        }

        button:hover:not(:disabled) {
            background: linear-gradient(135deg, #0056b3 0%, #004085 100%);
            transform: translateY(-1px);
            box-shadow: 0 4px 8px rgba(0,123,255,0.3);
        }

        button:active:not(:disabled) {
            transform: translateY(0);
        }

        button:disabled {
            background: #6c757d;
            cursor: not-allowed;
            box-shadow: none;
            transform: none;
        }

        button.active {
            background: linear-gradient(135deg, #28a745 0%, #1e7e34 100%);
            box-shadow: 0 2px 4px rgba(40,167,69,0.2);
        }

        .error {
            background: linear-gradient(135deg, #f8d7da 0%, #f5c6cb 100%);
            color: #721c24;
            padding: 12px 16px;
            border-radius: 8px;
            margin-top: 16px;
            display: flex;
            justify-content: space-between;
            align-items: center;
            border: 1px solid #f5c6cb;
            box-shadow: 0 2px 4px rgba(247,215,218,0.3);
        }

        .loading {
            display: inline-block;
            width: 16px;
            height: 16px;
            border: 2px solid #f3f3f3;
            border-top: 2px solid #007bff;
            border-radius: 50%;
            animation: spin 1s linear infinite;
            margin-right: 8px;
        }

        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }

        .loading-text {
            text-align: center;
            padding: 40px;
            color: #7f8c8d;
            font-size: 18px;
            font-weight: 500;
        }

        .property-section {
            margin: 20px 0;
            padding: 15px;
            border: 1px solid #eee;
            border-radius: 4px;
        }

        .vec3-input, .float-input {
            margin: 10px 0;
        }

        .vec3-input div:first-child, .float-input div:first-child {
            display: block;
            margin-bottom: 5px;
            font-weight: bold;
        }

        .input-row {
            display: flex;
            gap: 5px;
        }

        .input-row input, .float-input input {
            flex: 1;
            padding: 5px;
            border: 1px solid #ccc;
            border-radius: 3px;
        }

        h2 {
            margin-top: 0;
            color: #333;
        }

        h3 {
            margin-top: 0;
            color: #555;
        }

        h4 {
            margin-bottom: 10px;
            color: #666;
        }

        .tabs {
            display: flex;
            background: #2c3e50;
            border-bottom: 3px solid #34495e;
            padding: 0;
            box-shadow: 0 2px 8px rgba(0,0,0,0.2);
        }

        .tabs a {
            padding: 16px 32px;
            background: transparent;
            color: #ecf0f1;
            text-decoration: none;
            font-weight: 600;
            font-size: 14px;
            border: none;
            border-radius: 0;
            transition: all 0.3s ease;
            position: relative;
            box-shadow: none;
        }

        .tabs a:hover {
            background: #34495e;
            color: #fff;
            transform: none;
        }

        .tabs a.active {
            background: #3498db;
            color: white;
            box-shadow: inset 0 -3px 0 #2980b9;
        }

        .video-page {
            padding: 40px;
            max-width: 1200px;
            margin: 0 auto;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            min-height: calc(100vh - 50px);
        }

        .video-page h1 {
            color: #2c3e50;
            margin-bottom: 30px;
            font-size: 32px;
        }

        .search-section {
            display: flex;
            gap: 16px;
            margin-bottom: 30px;
        }

        .search-section input {
            flex: 1;
            padding: 12px 16px;
            border: 2px solid #dee2e6;
            border-radius: 8px;
            font-size: 16px;
        }

        .models-list {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }

        .model-option {
            background: white;
            border: 2px solid #dee2e6;
            border-radius: 12px;
            padding: 20px;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        .model-option:hover {
            border-color: #3498db;
            transform: translateY(-4px);
            box-shadow: 0 6px 16px rgba(52,152,219,0.2);
        }

        .model-option h3 {
            color: #2c3e50;
            margin: 0 0 10px 0;
            font-size: 20px;
        }

        .model-option p {
            color: #7f8c8d;
            margin: 0;
            font-size: 14px;
            line-height: 1.6;
        }

        .selected-model {
            background: white;
            border: 2px solid #3498db;
            border-radius: 12px;
            padding: 30px;
            margin-bottom: 30px;
            box-shadow: 0 4px 16px rgba(52,152,219,0.15);
        }

        .selected-model h2 {
            color: #2c3e50;
            margin-top: 0;
        }

        .video-output {
            margin-top: 30px;
            background: white;
            border-radius: 12px;
            padding: 20px;
            box-shadow: 0 4px 16px rgba(0,0,0,0.1);
        }

        .video-output video {
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        .parameters-form {
            display: flex;
            flex-direction: column;
            gap: 20px;
            margin: 20px 0;
        }

        .parameter-field {
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .parameter-field label {
            font-weight: 600;
            color: #2c3e50;
            font-size: 14px;
            text-transform: capitalize;
        }

        .parameter-input {
            width: 100%;
            padding: 12px 16px;
            border: 2px solid #dee2e6;
            border-radius: 8px;
            font-size: 14px;
            font-family: inherit;
            transition: border-color 0.3s ease;
        }

        .parameter-input:focus {
            outline: none;
            border-color: #3498db;
        }

        .parameter-input:disabled {
            background: #f8f9fa;
            cursor: not-allowed;
        }

        .parameter-field textarea.parameter-input {
            min-height: 100px;
            resize: vertical;
        }

        /* Collection buttons */
        .collection-buttons {
            display: flex;
            gap: 12px;
            margin-bottom: 24px;
        }

        .collection-button {
            flex: 1;
            padding: 12px 24px;
            background: white;
            color: #2c3e50;
            border: 2px solid #dee2e6;
            border-radius: 8px;
            font-weight: 600;
            font-size: 14px;
            transition: all 0.3s ease;
        }

        .collection-button:hover:not(:disabled) {
            background: #f8f9fa;
            border-color: #3498db;
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(52,152,219,0.2);
        }

        .collection-button.active {
            background: linear-gradient(135deg, #3498db 0%, #2980b9 100%);
            color: white;
            border-color: #3498db;
            box-shadow: 0 2px 8px rgba(52,152,219,0.3);
        }

        /* Parameters grid layout */
        .parameters-form-grid {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 12px 16px;
            margin: 20px 0;
        }

        @media (max-width: 768px) {
            .parameters-form-grid {
                grid-template-columns: 1fr;
            }
        }

        .parameter-field {
            display: flex;
            flex-direction: column;
            gap: 4px;
        }

        /* Make prompt field span both columns */
        .parameter-field:has(.parameter-textarea) {
            grid-column: 1 / -1;
        }

        .parameter-label {
            font-weight: 600;
            color: #2c3e50;
            font-size: 12px;
            display: flex;
            align-items: baseline;
            gap: 4px;
            margin-bottom: 2px;
        }

        .parameter-hint {
            font-weight: 400;
            color: #7f8c8d;
            font-size: 10px;
            font-style: italic;
            display: block;
            margin-top: 2px;
            line-height: 1.3;
        }

        .parameter-input,
        .parameter-select {
            width: 100%;
            padding: 6px 10px;
            border: 1px solid #dee2e6;
            border-radius: 4px;
            font-size: 13px;
            font-family: inherit;
            transition: border-color 0.2s ease;
            background: white;
            box-sizing: border-box;
        }

        .parameter-select {
            cursor: pointer;
            height: 32px;
        }

        .parameter-input:focus,
        .parameter-select:focus {
            outline: none;
            border-color: #3498db;
            box-shadow: 0 0 0 3px rgba(52, 152, 219, 0.1);
        }

        .parameter-input:disabled,
        .parameter-select:disabled {
            background: #f8f9fa;
            cursor: not-allowed;
            opacity: 0.7;
        }

        /* Textarea specific styling */
        .parameter-textarea {
            min-height: 70px;
            resize: vertical;
            line-height: 1.4;
        }

        /* Regular input height */
        input.parameter-input {
            height: 32px;
        }

        .generate-button {
            margin-top: 20px;
            width: 100%;
            padding: 12px 24px;
            font-size: 15px;
            font-weight: 600;
        }

        /* Image upload styling */
        .image-upload-container {
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .parameter-file-input {
            padding: 6px;
            border: 1px dashed #dee2e6;
            border-radius: 4px;
            font-size: 12px;
            cursor: pointer;
            background: #f8f9fa;
        }

        .parameter-file-input:hover {
            border-color: #3498db;
            background: white;
        }

        .parameter-file-input::-webkit-file-upload-button {
            padding: 4px 12px;
            border: 1px solid #dee2e6;
            border-radius: 3px;
            background: white;
            cursor: pointer;
            font-size: 12px;
            margin-right: 8px;
        }

        .parameter-file-input::-webkit-file-upload-button:hover {
            background: #3498db;
            color: white;
            border-color: #3498db;
        }
    </style>
</head>
<body>
    <div id="elm-app"></div>
    <script type="module">
        import { Elm } from './src/Main.elm';
        import PhysicsRenderer from './src/PhysicsRenderer.js';

        // Wrap initialization in async function
        (async () => {
            // Initialize Elm app
            const app = Elm.Main.init({
                node: document.getElementById('elm-app')
            });

            // Make Elm app globally accessible for Three.js callbacks
            window.elmApp = app;

            // Wait for Elm to render the DOM
            await new Promise(resolve => setTimeout(resolve, 100));

            // Initialize Three.js renderer only if canvas container exists
            let renderer = null;
            const initRenderer = async () => {
                const canvasContainer = document.getElementById('canvas-container');
                if (canvasContainer && !renderer) {
                    renderer = new PhysicsRenderer(canvasContainer);
                    await renderer.init();
                    console.log('Three.js renderer initialized');
                }
            };

            // Try to initialize renderer immediately
            await initRenderer();

            // Re-initialize renderer when URL changes (in case we navigate to physics page)
            const observer = new MutationObserver(() => {
                if (!renderer) {
                    initRenderer();
                }
            });
            observer.observe(document.body, { childList: true, subtree: true });

        // Set up Elm ports
        app.ports.sendSceneToThreeJs.subscribe((sceneData) => {
            console.log('Received scene data from Elm:', sceneData);
            if (renderer) {
                renderer.loadScene(sceneData);
            }
        });

        app.ports.sendSelectionToThreeJs.subscribe((objectId) => {
            console.log('Selected object:', objectId);
            if (renderer) {
                renderer.selectObject(objectId);
            }
        });

        app.ports.sendSimulationCommand.subscribe((command) => {
            console.log('Simulation command:', command);
            if (renderer) {
                switch (command) {
                    case 'start':
                        renderer.startSimulation();
                        break;
                    case 'pause':
                        renderer.pauseSimulation();
                        break;
                    case 'reset':
                        renderer.resetSimulation();
                        break;
                }
            }
        });

        app.ports.sendTransformModeToThreeJs.subscribe((mode) => {
            console.log('Transform mode:', mode);
            if (renderer) {
                renderer.setTransformMode(mode);
            }
        });

        // IndexedDB for scene storage
        const DB_NAME = 'PhysicsSimulatorDB';
        const DB_VERSION = 1;
        const SCENE_STORE = 'scenes';

        function openDB() {
            return new Promise((resolve, reject) => {
                const request = indexedDB.open(DB_NAME, DB_VERSION);
                request.onerror = () => reject(request.error);
                request.onsuccess = () => resolve(request.result);

                request.onupgradeneeded = (event) => {
                    const db = event.target.result;
                    if (!db.objectStoreNames.contains(SCENE_STORE)) {
                        db.createObjectStore(SCENE_STORE);
                    }
                };
            });
        }

        app.ports.saveSceneToStorage.subscribe((sceneData) => {
            console.log('Saving scene to IndexedDB');
            openDB().then(db => {
                const transaction = db.transaction([SCENE_STORE], 'readwrite');
                const store = transaction.objectStore(SCENE_STORE);
                store.put(sceneData, 'currentScene');
                transaction.oncomplete = () => console.log('Scene saved successfully');
                transaction.onerror = () => console.error('Failed to save scene');
            }).catch(error => console.error('IndexedDB error:', error));
        });

        app.ports.loadSceneFromStorage.subscribe(() => {
            console.log('Loading scene from IndexedDB');
            openDB().then(db => {
                const transaction = db.transaction([SCENE_STORE], 'readonly');
                const store = transaction.objectStore(SCENE_STORE);
                const request = store.get('currentScene');

                request.onsuccess = () => {
                    if (request.result) {
                        console.log('Scene loaded from IndexedDB');
                        if (app.ports.sceneLoadedFromStorage) {
                            app.ports.sceneLoadedFromStorage.send(request.result);
                        }
                    } else {
                        console.log('No saved scene found');
                    }
                };

                request.onerror = () => console.error('Failed to load scene');
            }).catch(error => console.error('IndexedDB error:', error));
        });

        console.log('Elm and Three.js integration initialized');
        })(); // Close async IIFE
    </script>
</body>
</html>
</file>

<file path="backend/requirements.txt">
fastapi>=0.100.0
uvicorn[standard]>=0.23.0
pydantic>=2.0.0
python-multipart>=0.0.6
requests>=2.31.0
python-dotenv>=1.0.0
replicate>=0.25.0
genesis-world==0.3.7
anthropic>=0.18.0
</file>

<file path="src/Main.elm">
port module Main exposing (main)

import Browser
import Browser.Events
import Browser.Navigation as Nav
import Dict exposing (Dict)
import Html exposing (..)
import Html.Attributes exposing (..)
import Html.Events exposing (..)
import Http
import Json.Decode as Decode
import Json.Encode as Encode
import Route exposing (Route)
import Url exposing (Url)
import Video
import VideoGallery


-- MAIN


main : Program () Model Msg
main =
    Browser.application
        { init = init
        , update = update
        , view = view
        , subscriptions = subscriptions
        , onUrlChange = UrlChanged
        , onUrlRequest = LinkClicked
        }


-- MODEL


type alias Model =
    { key : Nav.Key
    , url : Url
    , route : Maybe Route
    , scene : Scene
    , uiState : UiState
    , simulationState : SimulationState
    , initialScene : Maybe Scene
    , history : List Scene
    , future : List Scene
    , ctrlPressed : Bool
    , videoModel : Video.Model
    , galleryModel : VideoGallery.Model
    }


type alias Scene =
    { objects : Dict String PhysicsObject
    , selectedObject : Maybe String
    }


type alias UiState =
    { textInput : String
    , isGenerating : Bool
    , errorMessage : Maybe String
    , refineInput : String
    , isRefining : Bool
    }


type alias SimulationState =
    { isRunning : Bool
    , transformMode : TransformMode
    }


type TransformMode
    = Translate
    | Rotate
    | Scale


type alias PhysicsObject =
    { id : String
    , transform : Transform
    , physicsProperties : PhysicsProperties
    , visualProperties : VisualProperties
    , description : Maybe String
    }


type alias Transform =
    { position : Vec3
    , rotation : Vec3
    , scale : Vec3
    }


type alias Vec3 =
    { x : Float
    , y : Float
    , z : Float
    }


type alias PhysicsProperties =
    { mass : Float
    , friction : Float
    , restitution : Float
    }


type alias VisualProperties =
    { color : String
    , shape : Shape
    }


type Shape
    = Box
    | Sphere
    | Cylinder


init : () -> Url -> Nav.Key -> ( Model, Cmd Msg )
init _ url key =
    let
        ( videoModel, videoCmd ) =
            Video.init

        ( galleryModel, galleryCmd ) =
            VideoGallery.init

        route =
            Route.fromUrl url
    in
    ( { key = key
      , url = url
      , route = route
      , scene = { objects = Dict.empty, selectedObject = Nothing }
      , uiState = { textInput = "", isGenerating = False, errorMessage = Nothing, refineInput = "", isRefining = False }
      , simulationState = { isRunning = False, transformMode = Translate }
      , initialScene = Nothing
      , history = []
      , future = []
      , ctrlPressed = False
      , videoModel = videoModel
      , galleryModel = galleryModel
      }
    , Cmd.batch
        [ Cmd.map VideoMsg videoCmd
        , Cmd.map GalleryMsg galleryCmd
        ]
    )


-- UPDATE


type Msg
    = NoOp
    | LinkClicked Browser.UrlRequest
    | UrlChanged Url
    | UpdateTextInput String
    | GenerateScene
    | SceneGenerated Encode.Value
    | SceneGeneratedResult (Result Http.Error Scene)
    | ObjectClicked String
    | UpdateObjectTransform String Transform
    | UpdateObjectProperty String String Float
    | UpdateObjectDescription String String
    | ToggleSimulation
    | ResetSimulation
    | SetTransformMode TransformMode
    | UpdateRefineInput String
    | RefineScene
    | SceneRefined (Result Http.Error Scene)
    | Undo
    | Redo
    | SaveScene
    | LoadScene
    | SceneLoadedFromStorage Encode.Value
    | KeyDown String
    | KeyUp String
    | ClearError
    | SelectionChanged (Maybe String)
    | TransformUpdated { objectId : String, transform : Transform }
    | VideoMsg Video.Msg


update : Msg -> Model -> ( Model, Cmd Msg )
update msg model =
    case msg of
        NoOp ->
            ( model, Cmd.none )

        LinkClicked urlRequest ->
            case urlRequest of
                Browser.Internal url ->
                    ( model, Nav.pushUrl model.key (Url.toString url) )

                Browser.External href ->
                    ( model, Nav.load href )

        UrlChanged url ->
            ( { model | url = url, route = Route.fromUrl url }
            , Cmd.none
            )

        UpdateTextInput text ->
            let
                uiState =
                    model.uiState
            in
            ( { model | uiState = { uiState | textInput = text } }, Cmd.none )

        GenerateScene ->
            let
                uiState =
                    model.uiState
            in
            ( { model
                | uiState =
                    { uiState
                        | isGenerating = True
                        , errorMessage = Nothing
                    }
              }
            , generateSceneRequest model.uiState.textInput
            )

        SceneGenerated sceneJson ->
            case Decode.decodeValue sceneDecoder sceneJson of
                Ok newScene ->
                    let
                        uiState =
                            model.uiState
                    in
                    ( { model
                        | scene = newScene
                        , initialScene = Just newScene
                        , uiState =
                            { uiState
                                | isGenerating = False
                                , textInput = ""
                            }
                      }
                    , Cmd.none
                    )

                Err error ->
                    let
                        uiState =
                            model.uiState
                    in
                    ( { model
                        | uiState =
                            { uiState
                                | isGenerating = False
                                , errorMessage = Just (Decode.errorToString error)
                            }
                      }
                    , Cmd.none
                    )

        ObjectClicked objectId ->
            let
                scene =
                    model.scene
            in
            ( { model | scene = { scene | selectedObject = Just objectId } }
            , sendSelectionToThreeJs objectId
            )

        UpdateObjectTransform objectId newTransform ->
            let
                scene =
                    model.scene

                updateObject obj =
                    if obj.id == objectId then
                        { obj | transform = newTransform }
                    else
                        obj
            in
            ( { model
                | scene =
                    { scene
                        | objects = Dict.map (\_ obj -> updateObject obj) scene.objects
                    }
              }
            , Cmd.none
            )

        UpdateObjectProperty objectId propertyName value ->
            let
                scene =
                    model.scene

                updateObject obj =
                    if obj.id == objectId then
                        { obj | physicsProperties = updatePhysicsProperty obj.physicsProperties propertyName value }
                    else
                        obj

                updatePhysicsProperty props propName propValue =
                    case propName of
                        "mass" ->
                            { props | mass = propValue }

                        "friction" ->
                            { props | friction = propValue }

                        "restitution" ->
                            { props | restitution = propValue }

                        _ ->
                            props

                updatedScene =
                    { scene
                        | objects = Dict.map (\_ obj -> updateObject obj) scene.objects
                    }

                modelWithHistory =
                    saveToHistory model
            in
            ( { modelWithHistory | scene = updatedScene }
            , sendSceneToThreeJs (sceneEncoder updatedScene)
            )

        UpdateObjectDescription objectId desc ->
            let
                scene =
                    model.scene

                updateObject obj =
                    if obj.id == objectId then
                        { obj | description = if String.isEmpty desc then Nothing else Just desc }
                    else
                        obj

                updatedScene =
                    { scene
                        | objects = Dict.map (\_ obj -> updateObject obj) scene.objects
                    }

                modelWithHistory =
                    saveToHistory model
            in
            ( { modelWithHistory | scene = updatedScene }
            , sendSceneToThreeJs (sceneEncoder updatedScene)
            )

        ToggleSimulation ->
            let
                simulationState =
                    model.simulationState

                newIsRunning =
                    not simulationState.isRunning

                command =
                    if newIsRunning then
                        "start"
                    else
                        "pause"
            in
            ( { model
                | simulationState =
                    { simulationState
                        | isRunning = newIsRunning
                    }
              }
            , sendSimulationCommand command
            )

        ResetSimulation ->
            case model.initialScene of
                Just initial ->
                    ( { model | scene = initial }, sendSimulationCommand "reset" )

                Nothing ->
                    ( model, Cmd.none )

        SetTransformMode mode ->
            let
                simulationState =
                    model.simulationState

                modeString =
                    case mode of
                        Translate ->
                            "translate"

                        Rotate ->
                            "rotate"

                        Scale ->
                            "scale"
            in
            ( { model
                | simulationState =
                    { simulationState | transformMode = mode }
              }
            , sendTransformModeToThreeJs modeString
            )

        ClearError ->
            let
                uiState =
                    model.uiState
            in
            ( { model | uiState = { uiState | errorMessage = Nothing } }, Cmd.none )

        SceneGeneratedResult result ->
            case result of
                Ok scene ->
                    let
                        uiState =
                            model.uiState
                        modelWithHistory =
                            saveToHistory model
                    in
                    ( { modelWithHistory
                        | scene = scene
                        , initialScene = Just scene
                        , uiState =
                            { uiState
                                | isGenerating = False
                                , textInput = ""
                            }
                      }
                    , sendSceneToThreeJs (sceneEncoder scene)
                    )

                Err error ->
                    let
                        uiState =
                            model.uiState

                        errorMessage =
                            case error of
                                Http.BadUrl url ->
                                    "Bad URL: " ++ url

                                Http.Timeout ->
                                    "Request timed out"

                                Http.NetworkError ->
                                    "Network error"

                                Http.BadStatus status ->
                                    "Server error: " ++ String.fromInt status

                                Http.BadBody body ->
                                    "Invalid response: " ++ body
                    in
                    ( { model
                        | uiState =
                            { uiState
                                | isGenerating = False
                                , errorMessage = Just errorMessage
                            }
                      }
                    , Cmd.none
                    )

        UpdateRefineInput newInput ->
            let
                uiState =
                    model.uiState
            in
            ( { model | uiState = { uiState | refineInput = newInput } }, Cmd.none )

        RefineScene ->
            let
                uiState =
                    model.uiState
            in
            ( { model | uiState = { uiState | isRefining = True, errorMessage = Nothing } }
            , refineSceneRequest model.scene model.uiState.refineInput
            )

        SceneRefined result ->
            case result of
                Ok scene ->
                    let
                        uiState =
                            model.uiState
                        modelWithHistory =
                            saveToHistory model
                    in
                    ( { modelWithHistory
                        | scene = scene
                        , uiState =
                            { uiState
                                | isRefining = False
                                , refineInput = ""
                            }
                      }
                    , sendSceneToThreeJs (sceneEncoder scene)
                    )

                Err error ->
                    let
                        uiState =
                            model.uiState

                        errorMessage =
                            case error of
                                Http.BadUrl url ->
                                    "Bad URL: " ++ url

                                Http.Timeout ->
                                    "Request timed out"

                                Http.NetworkError ->
                                    "Network error"

                                Http.BadStatus status ->
                                    "Server error: " ++ String.fromInt status

                                Http.BadBody body ->
                                    "Invalid response: " ++ body
                    in
                    ( { model
                        | uiState =
                            { uiState
                                | isRefining = False
                                , errorMessage = Just errorMessage
                            }
                      }
                    , Cmd.none
                    )

        Undo ->
            case model.history of
                previousScene :: restHistory ->
                    ( { model
                        | scene = previousScene
                        , history = restHistory
                        , future = model.scene :: model.future
                      }
                    , sendSceneToThreeJs (sceneEncoder previousScene)
                    )

                [] ->
                    ( model, Cmd.none )

        Redo ->
            case model.future of
                nextScene :: restFuture ->
                    ( { model
                        | scene = nextScene
                        , history = model.scene :: model.history
                        , future = restFuture
                      }
                    , sendSceneToThreeJs (sceneEncoder nextScene)
                    )

                [] ->
                    ( model, Cmd.none )

        SaveScene ->
            ( model, saveSceneToStorage (sceneEncoder model.scene) )

        LoadScene ->
            ( model, loadSceneFromStorage () )

        SceneLoadedFromStorage sceneValue ->
            case Decode.decodeValue sceneDecoder sceneValue of
                Ok loadedScene ->
                    let
                        modelWithHistory =
                            saveToHistory model
                    in
                    ( { modelWithHistory
                        | scene = loadedScene
                        , initialScene = Just loadedScene
                      }
                    , sendSceneToThreeJs (sceneEncoder loadedScene)
                    )

                Err _ ->
                    ( model, Cmd.none )

        KeyDown key ->
            case key of
                "Control" ->
                    ( { model | ctrlPressed = True }, Cmd.none )

                " " ->
                    -- Space: toggle simulation
                    update (ToggleSimulation) model

                "g" ->
                    -- G: translate mode
                    update (SetTransformMode Translate) model

                "r" ->
                    -- R: rotate mode
                    update (SetTransformMode Rotate) model

                "s" ->
                    -- S: scale mode
                    update (SetTransformMode Scale) model

                "z" ->
                    -- Ctrl+Z: undo
                    if model.ctrlPressed then
                        update Undo model
                    else
                        ( model, Cmd.none )

                "y" ->
                    -- Ctrl+Y: redo
                    if model.ctrlPressed then
                        update Redo model
                    else
                        ( model, Cmd.none )

                _ ->
                    ( model, Cmd.none )

        KeyUp key ->
            case key of
                "Control" ->
                    ( { model | ctrlPressed = False }, Cmd.none )

                _ ->
                    ( model, Cmd.none )

        SelectionChanged maybeObjectId ->
            let
                scene =
                    model.scene
            in
            ( { model | scene = { scene | selectedObject = maybeObjectId } }, Cmd.none )

        TransformUpdated { objectId, transform } ->
            let
                scene =
                    model.scene

                updateObject obj =
                    if obj.id == objectId then
                        { obj | transform = transform }
                    else
                        obj
            in
            ( { model
                | scene =
                    { scene
                        | objects = Dict.map (\_ obj -> updateObject obj) scene.objects
                    }
              }
            , Cmd.none
            )

        VideoMsg videoMsg ->
            let
                ( updatedVideoModel, videoCmd ) =
                    Video.update videoMsg model.videoModel
            in
            ( { model | videoModel = updatedVideoModel }, Cmd.map VideoMsg videoCmd )


-- HISTORY MANAGEMENT


saveToHistory : Model -> Model
saveToHistory model =
    { model
        | history = model.scene :: List.take 49 model.history  -- Keep last 50 states
        , future = []  -- Clear future when new change is made
    }


-- VIEW


view : Model -> Browser.Document Msg
view model =
    { title = "Physics Simulator & Video Models"
    , body =
        [ div []
            [ viewTabs model
            , case model.route of
                Just Route.Physics ->
                    div [ class "app-container" ]
                        [ viewLeftPanel model
                        , viewCanvasContainer
                        , viewRightPanel model
                        , viewBottomBar model
                        ]

                Just Route.Videos ->
                    Video.view model.videoModel
                        |> Html.map VideoMsg

                Nothing ->
                    div [ class "app-container" ]
                        [ viewLeftPanel model
                        , viewCanvasContainer
                        , viewRightPanel model
                        , viewBottomBar model
                        ]
            ]
        ]
    }


viewTabs : Model -> Html Msg
viewTabs model =
    div [ class "tabs" ]
        [ a
            [ href "/physics"
            , class (if model.route == Just Route.Physics then "active" else "")
            ]
            [ text "Physics Simulator" ]
        , a
            [ href "/videos"
            , class (if model.route == Just Route.Videos then "active" else "")
            ]
            [ text "Video Models" ]
        ]


viewBottomBar : Model -> Html Msg
viewBottomBar model =
    div [ class "bottom-bar" ]
        [ div [ class "simulation-controls" ]
            [ button
                [ onClick ToggleSimulation
                , class (if model.simulationState.isRunning then "active" else "")
                ]
                [ text (if model.simulationState.isRunning then "Pause" else "Play") ]
            , button [ onClick ResetSimulation ] [ text "Reset" ]
            ]
        , div [ class "transform-controls" ]
            [ button
                [ onClick (SetTransformMode Translate)
                , class (if model.simulationState.transformMode == Translate then "active" else "")
                ]
                [ text "Move (G)" ]
            , button
                [ onClick (SetTransformMode Rotate)
                , class (if model.simulationState.transformMode == Rotate then "active" else "")
                ]
                [ text "Rotate (R)" ]
            , button
                [ onClick (SetTransformMode Scale)
                , class (if model.simulationState.transformMode == Scale then "active" else "")
                ]
                [ text "Scale (S)" ]
            ]
        , div [ class "history-controls" ]
            [ button
                [ onClick Undo
                , disabled (List.isEmpty model.history)
                ]
                [ text "Undo" ]
            , button
                [ onClick Redo
                , disabled (List.isEmpty model.future)
                ]
                [ text "Redo" ]
            , button [ onClick SaveScene ] [ text "Save" ]
            , button [ onClick LoadScene ] [ text "Load" ]
            ]
        ]


viewLeftPanel : Model -> Html Msg
viewLeftPanel model =
    div [ class "left-panel" ]
        [ h2 [] [ text "Generation" ]
        , textarea
            [ placeholder "Describe a scene to generate..."
            , value model.uiState.textInput
            , onInput UpdateTextInput
            , disabled model.uiState.isGenerating
            ]
            []
        , button
            [ onClick GenerateScene
            , disabled (String.isEmpty (String.trim model.uiState.textInput) || model.uiState.isGenerating)
            ]
            [ if model.uiState.isGenerating then
                span [ class "loading" ] []
              else
                text ""
            , text (if model.uiState.isGenerating then "Generating..." else "Generate Scene")
            ]
        , case model.uiState.errorMessage of
            Just error ->
                div [ class "error" ]
                    [ text error
                    , button [ onClick ClearError ] [ text "Ã—" ]
                    ]

            Nothing ->
                text ""
        , h2 [] [ text "Refinement" ]
        , textarea
            [ placeholder "Describe how to modify the current scene..."
            , value model.uiState.refineInput
            , onInput UpdateRefineInput
            , disabled (Dict.isEmpty model.scene.objects || model.uiState.isRefining)
            ]
            []
        , button
            [ onClick RefineScene
            , disabled (String.isEmpty (String.trim model.uiState.refineInput) || Dict.isEmpty model.scene.objects || model.uiState.isRefining)
            ]
            [ if model.uiState.isRefining then
                span [ class "loading" ] []
              else
                text ""
            , text (if model.uiState.isRefining then "Refining..." else "Refine Scene")
            ]
        ]


viewCanvasContainer : Html Msg
viewCanvasContainer =
    div [ id "canvas-container", class "canvas-container" ]
        []


viewRightPanel : Model -> Html Msg
viewRightPanel model =
    div [ class "right-panel" ]
        [ h2 [] [ text "Properties" ]
        , case model.scene.selectedObject of
            Just objectId ->
                case Dict.get objectId model.scene.objects of
                    Just object ->
                        viewObjectProperties object

                    Nothing ->
                        text "Object not found"

            Nothing ->
                text "No object selected"
        ]


viewObjectProperties : PhysicsObject -> Html Msg
viewObjectProperties object =
    div []
        [ h3 [] [ text ("Object: " ++ object.id) ]
        , div [ class "property-section" ]
            [ h4 [] [ text "Transform" ]
            , viewVec3Input "Position" object.transform.position (\vec -> UpdateObjectTransform object.id { position = vec, rotation = object.transform.rotation, scale = object.transform.scale })
            , viewVec3Input "Rotation" object.transform.rotation (\vec -> UpdateObjectTransform object.id { position = object.transform.position, rotation = vec, scale = object.transform.scale })
            , viewVec3Input "Scale" object.transform.scale (\vec -> UpdateObjectTransform object.id { position = object.transform.position, rotation = object.transform.rotation, scale = vec })
            ]
        , div [ class "property-section" ]
            [ h4 [] [ text "Physics" ]
            , viewFloatInput "Mass" object.physicsProperties.mass (\val -> UpdateObjectProperty object.id "mass" val)
            , viewFloatInput "Friction" object.physicsProperties.friction (\val -> UpdateObjectProperty object.id "friction" val)
            , viewFloatInput "Restitution" object.physicsProperties.restitution (\val -> UpdateObjectProperty object.id "restitution" val)
            ]
        , div [ class "property-section" ]
            [ h4 [] [ text "Visual" ]
            , div [] [ text ("Color: " ++ object.visualProperties.color) ]
            , div [] [ text ("Shape: " ++ shapeToString object.visualProperties.shape) ]
            ]
        , div [ class "property-section" ]
            [ h4 [] [ text "Description (for Genesis)" ]
            , div [ class "description-help" ]
                [ text "Describe what this object should look like (e.g., 'blue corvette', 'wooden table')" ]
            , textarea
                [ class "description-input"
                , placeholder "e.g., blue corvette, light pole, wooden coffee table..."
                , Html.Attributes.value (Maybe.withDefault "" object.description)
                , onInput (\desc -> UpdateObjectDescription object.id desc)
                , rows 3
                ]
                []
            ]
        ]


viewVec3Input : String -> Vec3 -> (Vec3 -> Msg) -> Html Msg
viewVec3Input labelText vec3 msgConstructor =
    div [ class "vec3-input" ]
        [ div [] [ text labelText ]
        , div [ class "input-row" ]
            [ input
                [ type_ "number"
                , step "0.1"
                , Html.Attributes.value (String.fromFloat vec3.x)
                , onInput (\x -> msgConstructor { vec3 | x = Maybe.withDefault vec3.x (String.toFloat x) })
                ]
                []
            , input
                [ type_ "number"
                , step "0.1"
                , Html.Attributes.value (String.fromFloat vec3.y)
                , onInput (\y -> msgConstructor { vec3 | y = Maybe.withDefault vec3.y (String.toFloat y) })
                ]
                []
            , input
                [ type_ "number"
                , step "0.1"
                , Html.Attributes.value (String.fromFloat vec3.z)
                , onInput (\z -> msgConstructor { vec3 | z = Maybe.withDefault vec3.z (String.toFloat z) })
                ]
                []
            ]
        ]


viewFloatInput : String -> Float -> (Float -> Msg) -> Html Msg
viewFloatInput labelText value msgConstructor =
    div [ class "float-input" ]
        [ div [] [ text labelText ]
        , input
            [ type_ "number"
            , step "0.1"
            , Html.Attributes.value (String.fromFloat value)
            , onInput (\val -> msgConstructor (Maybe.withDefault value (String.toFloat val)))
            ]
            []
        ]


shapeToString : Shape -> String
shapeToString shape =
    case shape of
        Box ->
            "Box"

        Sphere ->
            "Sphere"

        Cylinder ->
            "Cylinder"


-- SUBSCRIPTIONS


subscriptions : Model -> Sub Msg
subscriptions model =
    Sub.batch
        [ sendSelectionToElm SelectionChanged
        , sendTransformUpdateToElm TransformUpdated
        , sceneLoadedFromStorage SceneLoadedFromStorage
        , Browser.Events.onKeyDown (Decode.map KeyDown keyDecoder)
        , Browser.Events.onKeyUp (Decode.map KeyUp keyDecoder)
        ]


keyDecoder : Decode.Decoder String
keyDecoder =
    Decode.field "key" Decode.string


-- PORTS


port sendSceneToThreeJs : Encode.Value -> Cmd msg


port sendSelectionToThreeJs : String -> Cmd msg


port sendSimulationCommand : String -> Cmd msg


port sendSelectionToElm : (Maybe String -> msg) -> Sub msg


port sendTransformModeToThreeJs : String -> Cmd msg


port sendTransformUpdateToElm : ({ objectId : String, transform : Transform } -> msg) -> Sub msg


port saveSceneToStorage : Encode.Value -> Cmd msg


port loadSceneFromStorage : () -> Cmd msg


port sceneLoadedFromStorage : (Encode.Value -> msg) -> Sub msg


-- DECODERS


sceneDecoder : Decode.Decoder Scene
sceneDecoder =
    Decode.map2 Scene
        (Decode.field "objects" (Decode.dict physicsObjectDecoder))
        (Decode.maybe (Decode.field "selectedObject" Decode.string))


physicsObjectDecoder : Decode.Decoder PhysicsObject
physicsObjectDecoder =
    Decode.map5 PhysicsObject
        (Decode.field "id" Decode.string)
        (Decode.field "transform" transformDecoder)
        (Decode.field "physicsProperties" physicsPropertiesDecoder)
        (Decode.field "visualProperties" visualPropertiesDecoder)
        (Decode.maybe (Decode.field "description" Decode.string))


transformDecoder : Decode.Decoder Transform
transformDecoder =
    Decode.map3 Transform
        (Decode.field "position" vec3Decoder)
        (Decode.field "rotation" vec3Decoder)
        (Decode.field "scale" vec3Decoder)


vec3Decoder : Decode.Decoder Vec3
vec3Decoder =
    Decode.map3 Vec3
        (Decode.field "x" Decode.float)
        (Decode.field "y" Decode.float)
        (Decode.field "z" Decode.float)


physicsPropertiesDecoder : Decode.Decoder PhysicsProperties
physicsPropertiesDecoder =
    Decode.map3 PhysicsProperties
        (Decode.field "mass" Decode.float)
        (Decode.field "friction" Decode.float)
        (Decode.field "restitution" Decode.float)


visualPropertiesDecoder : Decode.Decoder VisualProperties
visualPropertiesDecoder =
    Decode.map2 VisualProperties
        (Decode.field "color" Decode.string)
        (Decode.field "shape" shapeDecoder)


shapeDecoder : Decode.Decoder Shape
shapeDecoder =
    Decode.string
        |> Decode.andThen
            (\shapeStr ->
                case shapeStr of
                    "Box" ->
                        Decode.succeed Box

                    "Sphere" ->
                        Decode.succeed Sphere

                    "Cylinder" ->
                        Decode.succeed Cylinder

                    _ ->
                        Decode.fail ("Unknown shape: " ++ shapeStr)
            )


-- HTTP REQUESTS


generateSceneRequest : String -> Cmd Msg
generateSceneRequest prompt =
    Http.post
        { url = "http://127.0.0.1:8000/api/generate"
        , body = Http.jsonBody (Encode.object [ ( "prompt", Encode.string prompt ) ])
        , expect = Http.expectJson SceneGeneratedResult sceneDecoder
        }


refineSceneRequest : Scene -> String -> Cmd Msg
refineSceneRequest scene prompt =
    Http.post
        { url = "http://127.0.0.1:8000/api/refine"
        , body = Http.jsonBody (Encode.object [ ( "scene", sceneEncoder scene ), ( "prompt", Encode.string prompt ) ])
        , expect = Http.expectJson SceneRefined sceneDecoder
        }


-- ENCODERS


sceneEncoder : Scene -> Encode.Value
sceneEncoder scene =
    Encode.object
        [ ( "objects", Encode.dict identity physicsObjectEncoder scene.objects )
        , ( "selectedObject", maybeEncoder Encode.string scene.selectedObject )
        ]


physicsObjectEncoder : PhysicsObject -> Encode.Value
physicsObjectEncoder obj =
    let
        baseFields =
            [ ( "id", Encode.string obj.id )
            , ( "transform", transformEncoder obj.transform )
            , ( "physicsProperties", physicsPropertiesEncoder obj.physicsProperties )
            , ( "visualProperties", visualPropertiesEncoder obj.visualProperties )
            ]

        descriptionField =
            case obj.description of
                Just desc ->
                    [ ( "description", Encode.string desc ) ]

                Nothing ->
                    []
    in
    Encode.object (baseFields ++ descriptionField)


transformEncoder : Transform -> Encode.Value
transformEncoder transform =
    Encode.object
        [ ( "position", vec3Encoder transform.position )
        , ( "rotation", vec3Encoder transform.rotation )
        , ( "scale", vec3Encoder transform.scale )
        ]


vec3Encoder : Vec3 -> Encode.Value
vec3Encoder vec3 =
    Encode.object
        [ ( "x", Encode.float vec3.x )
        , ( "y", Encode.float vec3.y )
        , ( "z", Encode.float vec3.z )
        ]


physicsPropertiesEncoder : PhysicsProperties -> Encode.Value
physicsPropertiesEncoder props =
    Encode.object
        [ ( "mass", Encode.float props.mass )
        , ( "friction", Encode.float props.friction )
        , ( "restitution", Encode.float props.restitution )
        ]


visualPropertiesEncoder : VisualProperties -> Encode.Value
visualPropertiesEncoder props =
    Encode.object
        [ ( "color", Encode.string props.color )
        , ( "shape", shapeEncoder props.shape )
        ]


shapeEncoder : Shape -> Encode.Value
shapeEncoder shape =
    Encode.string <|
        case shape of
            Box ->
                "Box"

            Sphere ->
                "Sphere"

            Cylinder ->
                "Cylinder"


maybeEncoder : (a -> Encode.Value) -> Maybe a -> Encode.Value
maybeEncoder encoder maybeValue =
    case maybeValue of
        Just value ->
            encoder value

        Nothing ->
            Encode.null
</file>

<file path="backend/main.py">
from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import Dict, Optional, List
import uvicorn
import os
import hashlib
import json
import requests
from dotenv import load_dotenv
from pathlib import Path

# Note: replicate package has Python 3.14 compatibility issues
# We only use HTTP API calls via requests library
replicate = None
REPLICATE_AVAILABLE = False

from database import (
    save_generated_scene,
    get_scene_by_id,
    list_scenes,
    get_scene_count,
    get_models_list,
    delete_scene
)

# Load environment variables from .env file in parent directory
# Try loading .env from backend directory, then parent directory
if not load_dotenv('.env'):
    load_dotenv('../.env')
# import genesis as gs  # Using geometric validation instead

app = FastAPI(title="Physics Simulator API", version="1.0.0")

# CORS middleware (for development)
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",
        "http://127.0.0.1:5173",
        "http://localhost:5175",  # Alternative Vite port
        "http://127.0.0.1:5175"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Check if static files exist (production mode)
STATIC_DIR = Path(__file__).parent.parent / "static"
if STATIC_DIR.exists() and STATIC_DIR.is_dir():
    # Mount static files
    app.mount("/assets", StaticFiles(directory=str(STATIC_DIR / "assets")), name="assets")

# Pydantic models
class Vec3(BaseModel):
    x: float
    y: float
    z: float

class Transform(BaseModel):
    position: Vec3
    rotation: Vec3
    scale: Vec3

class PhysicsProperties(BaseModel):
    mass: float
    friction: float
    restitution: float

class VisualProperties(BaseModel):
    color: str
    shape: str  # "Box", "Sphere", "Cylinder"

class PhysicsObject(BaseModel):
    id: str
    transform: Transform
    physicsProperties: PhysicsProperties
    visualProperties: VisualProperties
    description: Optional[str] = None  # Text description for LLM semantic augmentation

class Scene(BaseModel):
    objects: Dict[str, PhysicsObject]
    selectedObject: Optional[str] = None

class GenerateRequest(BaseModel):
    prompt: str

class ValidationResult(BaseModel):
    valid: bool
    message: str
    details: Optional[Dict] = None

# AI client initialization
replicate_api_key = os.getenv("REPLICATE_API_KEY")
if replicate_api_key:
    ai_client = {
        "api_key": replicate_api_key,
        "base_url": "https://api.replicate.com/v1"
    }
    replicate_client = replicate.Client(api_token=replicate_api_key) if REPLICATE_AVAILABLE and replicate else None
    print("AI client initialized with Replicate")
else:
    ai_client = None
    replicate_client = None
    print("Warning: Using demo scene generation (REPLICATE_API_KEY not set)")

# Demo video models for fallback
DEMO_VIDEO_MODELS = [
    {
        "id": "demo/video-1",
        "name": "Demo Text-to-Video",
        "description": "Generates video from text prompt (demo mode)",
        "input_schema": None
    },
    {
        "id": "demo/video-2",
        "name": "Demo Image-to-Video",
        "description": "Generates video from image and prompt (demo mode)",
        "input_schema": None
    }
]

# Simple in-memory cache (replace with LMDB later)
scene_cache = {}

@app.get("/health")
async def health_check():
    return {"status": "healthy"}

@app.get("/api")
async def api_root():
    return {"message": "Physics Simulator API", "status": "running"}

def generate_scene(prompt: str) -> Scene:
    """Generate a physics scene from a text prompt using AI."""
    print(f"ai_client is None: {ai_client is None}")
    # For demo purposes, return a simple test scene if AI client is not configured
    if not ai_client:
        print("Warning: Using demo scene generation (AI client not configured)")
        return create_demo_scene(prompt)

    # Check cache first
    cache_key = hashlib.sha256(prompt.encode()).hexdigest()
    if cache_key in scene_cache:
        try:
            return Scene.parse_raw(scene_cache[cache_key])
        except Exception:
            pass  # Cache corrupted, regenerate

    # Create prompt template
    system_prompt = """You are a physics scene generator. Create realistic 3D physics scenes based on text descriptions.

Generate scenes with 2-8 objects that can interact physically. Each object should have:
- Realistic physics properties (mass, friction, restitution)
- Appropriate visual properties (color, shape)
- Sensible initial positions and orientations

Supported shapes: "Box", "Sphere", "Cylinder"
Colors should be hex codes like "#ff0000" for red

Return ONLY valid JSON matching this schema:
{
  "objects": {
    "object_id": {
      "id": "object_id",
      "transform": {
        "position": {"x": float, "y": float, "z": float},
        "rotation": {"x": float, "y": float, "z": float},
        "scale": {"x": float, "y": float, "z": float}
      },
      "physicsProperties": {
        "mass": float,
        "friction": float,
        "restitution": float
      },
      "visualProperties": {
        "color": "hex_color",
        "shape": "Box|Sphere|Cylinder"
      }
    }
  }
}

Make scenes physically realistic and interesting to simulate."""

    user_prompt = f"Generate a physics scene for: {prompt}"

    try:
        # Use Claude via Replicate HTTP API
        headers = {
            "Authorization": f"Bearer {ai_client['api_key']}",
            "Content-Type": "application/json"
        }

        payload = {
            "input": {
                "prompt": f"{system_prompt}\n\n{user_prompt}",
                "max_tokens": 2000,
                "temperature": 0.7
            }
        }

        response = requests.post(
            "https://api.replicate.com/v1/models/anthropic/claude-3.5-sonnet/predictions",
            headers=headers,
            json=payload,
            timeout=60
        )
        # Log the response for debugging
        print(f"Replicate API response status: {response.status_code}")
        if response.status_code != 200 and response.status_code != 201:
            print(f"Replicate API error response: {response.text}")

        response.raise_for_status()

        result = response.json()

        # Wait for the prediction to complete
        prediction_url = result.get("urls", {}).get("get")
        if not prediction_url:
            print(f"Error: No prediction URL in response: {result}")
            raise HTTPException(status_code=500, detail="No prediction URL returned")

        print(f"Polling prediction at: {prediction_url}")

        # Poll for completion
        import time
        max_attempts = 120  # Increased timeout for Claude
        for attempt in range(max_attempts):
            pred_response = requests.get(prediction_url, headers=headers)
            pred_response.raise_for_status()
            pred_data = pred_response.json()

            status = pred_data.get("status")
            print(f"Attempt {attempt + 1}/{max_attempts}: Status = {status}")

            if status == "succeeded":
                output = pred_data.get("output")
                print(f"Raw output type: {type(output)}")
                print(f"Raw output: {output}")

                if isinstance(output, list):
                    scene_json = "".join(output).strip()
                elif isinstance(output, str):
                    scene_json = output.strip()
                else:
                    print(f"Unexpected output type: {type(output)}, value: {output}")
                    raise HTTPException(status_code=500, detail=f"Unexpected output format: {type(output)}")

                print(f"Scene JSON (first 200 chars): {scene_json[:200]}")
                break
            elif status in ["failed", "canceled"]:
                error = pred_data.get("error", "Unknown error")
                print(f"Prediction failed: {error}")
                raise HTTPException(status_code=500, detail=f"Prediction failed: {error}")

            time.sleep(2)  # Poll every 2 seconds
        else:
            print(f"Prediction timed out after {max_attempts} attempts")
            raise HTTPException(status_code=500, detail="Prediction timed out")

        # Clean up JSON response (remove markdown code blocks if present)
        if scene_json.startswith("```json"):
            scene_json = scene_json[7:]
        if scene_json.endswith("```"):
            scene_json = scene_json[:-3]
        scene_json = scene_json.strip()

        # Parse and validate the scene
        scene_data = json.loads(scene_json)
        scene = Scene(**scene_data)

        # Skip validation for now - it's too strict
        # validation = validate_with_genesis(scene)
        # if not validation.valid:
        #     raise HTTPException(
        #         status_code=400,
        #         detail=f"Generated scene is not stable: {validation.message}"
        #     )

        # Cache the result
        scene_cache[cache_key] = scene.json()

        return scene

    except json.JSONDecodeError as e:
        raise HTTPException(status_code=500, detail=f"Invalid JSON response from AI: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"AI generation failed: {str(e)}")

def create_demo_scene(prompt: str) -> Scene:
    """Create a demo scene for testing when AI is not available."""
    # Create a simple demo scene with a few objects
    objects = {
        "box1": PhysicsObject(
            id="box1",
            transform=Transform(
                position=Vec3(x=0, y=5, z=0),
                rotation=Vec3(x=0, y=0, z=0),
                scale=Vec3(x=1, y=1, z=1)
            ),
            physicsProperties=PhysicsProperties(
                mass=1.0,
                friction=0.5,
                restitution=0.3
            ),
            visualProperties=VisualProperties(
                color="#ff0000",
                shape="Box"
            )
        ),
        "sphere1": PhysicsObject(
            id="sphere1",
            transform=Transform(
                position=Vec3(x=2, y=8, z=0),
                rotation=Vec3(x=0, y=0, z=0),
                scale=Vec3(x=1, y=1, z=1)
            ),
            physicsProperties=PhysicsProperties(
                mass=0.5,
                friction=0.2,
                restitution=0.8
            ),
            visualProperties=VisualProperties(
                color="#0000ff",
                shape="Sphere"
            )
        ),
        "ground": PhysicsObject(
            id="ground",
            transform=Transform(
                position=Vec3(x=0, y=-0.5, z=0),
                rotation=Vec3(x=0, y=0, z=0),
                scale=Vec3(x=10, y=1, z=10)
            ),
            physicsProperties=PhysicsProperties(
                mass=0.0,  # Static ground
                friction=0.8,
                restitution=0.1
            ),
            visualProperties=VisualProperties(
                color="#888888",
                shape="Box"
            )
        )
    }

    return Scene(objects=objects)

def validate_with_genesis(scene: Scene) -> ValidationResult:
    """Validate scene stability using geometric analysis."""
    try:
        # Check for unsupported shapes
        for obj_id, obj in scene.objects.items():
            if obj.visualProperties.shape not in ["Box", "Sphere"]:
                return ValidationResult(
                    valid=False,
                    message=f"Unsupported shape: {obj.visualProperties.shape}",
                    details={"unsupported_shape": obj.visualProperties.shape}
                )

        # Check for overlapping objects (simple geometric validation)
        overlapping_pairs = []
        objects_list = list(scene.objects.items())

        for i, (id1, obj1) in enumerate(objects_list):
            for j, (id2, obj2) in enumerate(objects_list[i+1:], i+1):
                # Skip ground objects (mass = 0)
                if obj1.physicsProperties.mass == 0 or obj2.physicsProperties.mass == 0:
                    continue

                # Calculate distance between centers
                dx = obj1.transform.position.x - obj2.transform.position.x
                dy = obj1.transform.position.y - obj2.transform.position.y
                dz = obj1.transform.position.z - obj2.transform.position.z
                distance = (dx**2 + dy**2 + dz**2)**0.5

                # Calculate minimum separation needed
                if obj1.visualProperties.shape == "Box" and obj2.visualProperties.shape == "Box":
                    # Box-box collision: check if bounding boxes overlap
                    min_sep_x = (obj1.transform.scale.x + obj2.transform.scale.x) / 2
                    min_sep_y = (obj1.transform.scale.y + obj2.transform.scale.y) / 2
                    min_sep_z = (obj1.transform.scale.z + obj2.transform.scale.z) / 2

                    if (abs(dx) < min_sep_x and abs(dy) < min_sep_y and abs(dz) < min_sep_z):
                        overlapping_pairs.append((id1, id2))

                elif obj1.visualProperties.shape == "Sphere" and obj2.visualProperties.shape == "Sphere":
                    # Sphere-sphere collision
                    min_distance = (obj1.transform.scale.x + obj2.transform.scale.x) / 2  # Assume uniform scale
                    if distance < min_distance:
                        overlapping_pairs.append((id1, id2))

                else:
                    # Mixed sphere-box: approximate with sphere radius
                    sphere_obj = obj1 if obj1.visualProperties.shape == "Sphere" else obj2
                    box_obj = obj2 if obj1.visualProperties.shape == "Sphere" else obj1

                    sphere_radius = sphere_obj.transform.scale.x / 2
                    box_half_size = max(box_obj.transform.scale.x, box_obj.transform.scale.y, box_obj.transform.scale.z) / 2

                    min_distance = sphere_radius + box_half_size
                    if distance < min_distance:
                        overlapping_pairs.append((id1, id2))

        # Check for objects too high (likely to fall unstably)
        high_objects = []
        for obj_id, obj in scene.objects.items():
            if obj.physicsProperties.mass > 0 and obj.transform.position.y > 5.0:
                high_objects.append(obj_id)

        # Validate results
        issues = []
        if overlapping_pairs:
            issues.append(f"Overlapping objects: {overlapping_pairs}")
        if high_objects:
            issues.append(f"Objects too high (unstable): {high_objects}")

        if issues:
            return ValidationResult(
                valid=False,
                message="Scene has stability issues: " + "; ".join(issues),
                details={
                    "overlapping_pairs": overlapping_pairs,
                    "high_objects": high_objects,
                    "max_height_threshold": 5.0
                }
            )
        else:
            return ValidationResult(
                valid=True,
                message="Scene appears geometrically stable",
                details={"checked_objects": len(scene.objects)}
            )

    except Exception as e:
        return ValidationResult(
            valid=False,
            message=f"Validation failed: {str(e)}",
            details={"error": str(e)}
        )

@app.post("/api/generate")
async def api_generate_scene(request: GenerateRequest):
    """Generate a physics scene from a text prompt."""
    try:
        scene = generate_scene(request.prompt)
        scene_dict = scene.dict()

        # Save to database
        scene_id = save_generated_scene(
            prompt=request.prompt,
            scene_data=scene_dict,
            model="claude-3.5-sonnet",
            metadata={"source": "generate"}
        )

        # Add scene_id to response
        scene_dict["_id"] = scene_id

        return scene_dict
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Scene generation failed: {str(e)}")

@app.post("/api/validate")
async def api_validate_scene(scene: Scene):
    """Validate a physics scene for stability using Genesis simulation."""
    try:
        result = validate_with_genesis(scene)
        return result.dict()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Scene validation failed: {str(e)}")

class RefineRequest(BaseModel):
    scene: Scene
    prompt: str

class VideoModel(BaseModel):
    id: str
    name: str
    description: str
    input_schema: Optional[Dict] = None

class RunVideoRequest(BaseModel):
    model_id: str
    input: Dict[str, str]  # For now, simple dict; extend for files later
    collection: Optional[str] = None

class GenesisRenderRequest(BaseModel):
    scene: Scene
    duration: float = 5.0
    fps: int = 60
    resolution: tuple[int, int] = (1920, 1080)
    quality: str = "high"  # "draft", "high", "ultra"
    camera_config: Optional[Dict] = None
    scene_context: Optional[str] = None

def refine_scene(scene: Scene, prompt: str) -> Scene:
    """Refine an existing physics scene based on a text prompt using AI."""
    print(f"Refining scene with prompt: {prompt}")
    # For demo purposes, return the original scene if AI client is not configured
    if not ai_client:
        print("Warning: Using demo scene refinement (AI client not configured)")
        return scene

    # Create cache key from scene and prompt
    scene_str = scene.json()
    cache_key = hashlib.sha256(f"{scene_str}:{prompt}".encode()).hexdigest()

    if cache_key in scene_cache:
        try:
            return Scene.parse_raw(scene_cache[cache_key])
        except Exception:
            pass  # Cache corrupted, regenerate

    # Create prompt template for refinement
    system_prompt = """You are a physics scene refiner. Modify existing 3D physics scenes based on text instructions.

Given an existing scene JSON and a refinement prompt, modify the scene accordingly. You can:
- Change object colors, positions, scales, rotations
- Add new objects
- Remove objects
- Modify physics properties (mass, friction, restitution)
- Change object shapes

Return ONLY valid JSON matching the scene schema. Preserve the structure and only make the requested changes.

Scene schema:
{
  "objects": {
    "object_id": {
      "id": "object_id",
      "transform": {
        "position": {"x": float, "y": float, "z": float},
        "rotation": {"x": float, "y": float, "z": float},
        "scale": {"x": float, "y": float, "z": float}
      },
      "physicsProperties": {
        "mass": float,
        "friction": float,
        "restitution": float
      },
      "visualProperties": {
        "color": "hex_color",
        "shape": "Box|Sphere|Cylinder"
      }
    }
  }
}

Make minimal, targeted changes based on the prompt."""

    user_prompt = f"Original scene: {scene_str}\n\nRefinement request: {prompt}\n\nReturn the modified scene JSON:"

    try:
        # Use Claude via Replicate HTTP API
        headers = {
            "Authorization": f"Bearer {ai_client['api_key']}",
            "Content-Type": "application/json"
        }

        payload = {
            "input": {
                "prompt": f"{system_prompt}\n\n{user_prompt}",
                "max_tokens": 2000,
                "temperature": 0.7
            }
        }

        response = requests.post(
            "https://api.replicate.com/v1/models/anthropic/claude-3.5-sonnet/predictions",
            headers=headers,
            json=payload,
            timeout=60
        )
        # Log the response for debugging
        print(f"Replicate API response status: {response.status_code}")
        if response.status_code != 200 and response.status_code != 201:
            print(f"Replicate API error response: {response.text}")

        response.raise_for_status()

        result = response.json()

        # Wait for the prediction to complete
        prediction_url = result.get("urls", {}).get("get")
        if not prediction_url:
            raise HTTPException(status_code=500, detail="No prediction URL returned")

        # Poll for completion
        import time
        max_attempts = 120  # Increased timeout for Claude
        for attempt in range(max_attempts):
            pred_response = requests.get(prediction_url, headers=headers)
            pred_response.raise_for_status()
            pred_data = pred_response.json()

            status = pred_data.get("status")
            print(f"Refine attempt {attempt + 1}/{max_attempts}: Status = {status}")

            if status == "succeeded":
                output = pred_data.get("output")
                print(f"Raw output type: {type(output)}")

                if isinstance(output, list):
                    refined_scene_json = "".join(output).strip()
                elif isinstance(output, str):
                    refined_scene_json = output.strip()
                else:
                    print(f"Unexpected output type: {type(output)}, value: {output}")
                    raise HTTPException(status_code=500, detail=f"Unexpected output format: {type(output)}")

                print(f"Refined scene JSON (first 200 chars): {refined_scene_json[:200]}")
                break
            elif status in ["failed", "canceled"]:
                error = pred_data.get("error", "Unknown error")
                print(f"Prediction failed: {error}")
                raise HTTPException(status_code=500, detail=f"Prediction failed: {error}")

            time.sleep(2)  # Poll every 2 seconds
        else:
            print(f"Prediction timed out after {max_attempts} attempts")
            raise HTTPException(status_code=500, detail="Prediction timed out")

        # Clean up JSON response
        if refined_scene_json.startswith("```json"):
            refined_scene_json = refined_scene_json[7:]
        if refined_scene_json.endswith("```"):
            refined_scene_json = refined_scene_json[:-3]
        refined_scene_json = refined_scene_json.strip()

        # Parse and validate the refined scene
        refined_scene_data = json.loads(refined_scene_json)
        refined_scene = Scene(**refined_scene_data)

        # Skip validation for now - it's too strict
        # validation = validate_with_genesis(refined_scene)
        # if not validation.valid:
        #     raise HTTPException(
        #         status_code=400,
        #         detail=f"Refined scene is not stable: {validation.message}"
        #     )

        # Cache the result
        scene_cache[cache_key] = refined_scene.json()

        return refined_scene

    except json.JSONDecodeError as e:
        raise HTTPException(status_code=500, detail=f"Invalid JSON response from AI: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Scene refinement failed: {str(e)}")

@app.post("/api/refine")
async def api_refine_scene(request: RefineRequest):
    """Refine an existing physics scene based on a text prompt."""
    try:
        refined_scene = refine_scene(request.scene, request.prompt)
        return refined_scene.dict()
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Scene refinement failed: {str(e)}")

# Scene history endpoints
@app.get("/api/scenes")
async def api_list_scenes(
    limit: int = Query(50, ge=1, le=100),
    offset: int = Query(0, ge=0),
    model: Optional[str] = Query(None)
):
    """List generated scenes with pagination and optional model filter."""
    try:
        scenes = list_scenes(limit=limit, offset=offset, model=model)
        total = get_scene_count(model=model)
        return {
            "scenes": scenes,
            "total": total,
            "limit": limit,
            "offset": offset
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list scenes: {str(e)}")

@app.get("/api/scenes/{scene_id}")
async def api_get_scene(scene_id: int):
    """Get a specific scene by ID."""
    try:
        scene = get_scene_by_id(scene_id)
        if not scene:
            raise HTTPException(status_code=404, detail="Scene not found")
        return scene
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get scene: {str(e)}")

@app.delete("/api/scenes/{scene_id}")
async def api_delete_scene(scene_id: int):
    """Delete a scene by ID."""
    try:
        deleted = delete_scene(scene_id)
        if not deleted:
            raise HTTPException(status_code=404, detail="Scene not found")
        return {"success": True, "message": "Scene deleted"}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete scene: {str(e)}")

@app.get("/api/models")
async def api_get_models():
    """Get list of models that have generated scenes."""
    try:
        models = get_models_list()
        return {"models": models}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get models: {str(e)}")

@app.get("/api/replicate-models")
async def api_get_replicate_models(
    query: Optional[str] = Query(None, description="Search query"),
    cursor: Optional[str] = Query(None, description="Pagination cursor")
):
    """Get list of available models from Replicate."""
    try:
        if not ai_client:
            return {"results": DEMO_VIDEO_MODELS, "next": None}

        headers = {
            "Authorization": f"Bearer {ai_client['api_key']}",
            "Content-Type": "application/json"
        }

        # Build URL with query params
        params = []
        if cursor:
            params.append(f"cursor={cursor}")
        if query:
            params.append(f"query={query}")

        # Use Replicate's models API
        url = "https://api.replicate.com/v1/models"
        if params:
            url += "?" + "&".join(params)

        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()

        # Format the response
        models = []
        results = data.get("results", [])

        for model_data in results:
            models.append({
                "owner": model_data.get("owner"),
                "name": model_data.get("name"),
                "description": model_data.get("description"),
                "url": model_data.get("url"),
                "cover_image_url": model_data.get("cover_image_url"),
                "latest_version": model_data.get("latest_version", {}).get("id") if model_data.get("latest_version") else None,
                "run_count": model_data.get("run_count", 0),
            })

        return {
            "results": models,
            "next": data.get("next")
        }

    except Exception as e:
        print(f"Error fetching models from Replicate: {str(e)}")
        import traceback
        traceback.print_exc()
        # Fallback to demo models
        return {"results": DEMO_VIDEO_MODELS, "next": None}

@app.get("/api/video-models")
async def api_get_video_models(
    collection: Optional[str] = Query("text-to-video", description="Collection slug: text-to-video, image-to-video, etc.")
):
    """Get video generation models from Replicate collections API."""
    try:
        if not ai_client:
            # Fallback to demo models if no API key
            return {"models": [model for model in DEMO_VIDEO_MODELS]}

        headers = {
            "Authorization": f"Bearer {ai_client['api_key']}",
            "Content-Type": "application/json"
        }

        # Use collections API with the specified collection slug
        url = f"https://api.replicate.com/v1/collections/{collection}"
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()

        # Format the models from the collection
        models = []
        for model_data in data.get("models", []):
            model_id = f"{model_data.get('owner')}/{model_data.get('name')}"
            models.append({
                "id": model_id,
                "name": model_data.get("name", ""),
                "owner": model_data.get("owner", ""),
                "description": model_data.get("description"),
                "cover_image_url": model_data.get("cover_image_url"),
                "latest_version": model_data.get("latest_version", {}).get("id") if model_data.get("latest_version") else None,
                "run_count": model_data.get("run_count", 0),
                "input_schema": None  # Will be fetched when model is selected
            })

        return {"models": models}
    except Exception as e:
        print(f"Error fetching video models from collection '{collection}': {str(e)}")
        import traceback
        traceback.print_exc()
        # Fallback to demo models
        return {"models": [model for model in DEMO_VIDEO_MODELS]}

@app.get("/api/video-models/{model_owner}/{model_name}/schema")
async def api_get_model_schema(model_owner: str, model_name: str):
    """Get the input schema for a specific model."""
    try:
        if not ai_client:
            return {"input_schema": {"prompt": {"type": "string"}}}

        headers = {
            "Authorization": f"Bearer {ai_client['api_key']}",
            "Content-Type": "application/json"
        }

        # Fetch model details including schema
        url = f"https://api.replicate.com/v1/models/{model_owner}/{model_name}"
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()

        # Extract input schema from latest version
        latest_version = data.get("latest_version", {})
        openapi_schema = latest_version.get("openapi_schema", {})
        input_schema = openapi_schema.get("components", {}).get("schemas", {}).get("Input", {})

        # Extract properties and required fields
        properties = input_schema.get("properties", {})
        required_fields = input_schema.get("required", [])

        # Also extract default values from the schema if they exist
        # OpenAPI schemas can have default values at the property level
        return {
            "input_schema": properties,
            "required": required_fields
        }
    except Exception as e:
        print(f"Error fetching model schema: {str(e)}")
        import traceback
        traceback.print_exc()
        return {"input_schema": {"prompt": {"type": "string"}}}

@app.post("/api/run-video-model")
async def api_run_video_model(request: RunVideoRequest):
    """Run a video generation model on Replicate."""
    try:
        if not ai_client:
            # Demo response
            return {"output": ["https://example.com/demo-video.mp4"], "status": "demo"}

        headers = {
            "Authorization": f"Bearer {ai_client['api_key']}",
            "Content-Type": "application/json"
        }

        # Convert parameter types based on the schema
        # Fetch the schema to know which parameters should be integers/numbers
        converted_input = {}
        for key, value in request.input.items():
            if isinstance(value, str):
                # Try to convert to int
                try:
                    converted_input[key] = int(value)
                    continue
                except ValueError:
                    pass

                # Try to convert to float
                try:
                    converted_input[key] = float(value)
                    continue
                except ValueError:
                    pass

                # Keep as string
                converted_input[key] = value
            else:
                converted_input[key] = value

        # Create prediction using HTTP API
        payload = {
            "input": converted_input
        }

        # Debug: log the payload to see what types we're sending
        print(f"DEBUG: Sending to Replicate API:")
        print(f"  Model: {request.model_id}")
        print(f"  Input types: {[(k, type(v).__name__, v) for k, v in converted_input.items()]}")

        # Use the model predictions endpoint
        url = f"https://api.replicate.com/v1/models/{request.model_id}/predictions"
        response = requests.post(url, headers=headers, json=payload, timeout=60)

        # Log the detailed error if request fails
        if response.status_code != 201:
            error_detail = response.text
            print(f"Replicate API Error ({response.status_code}): {error_detail}")

            # Parse error detail if it's JSON
            try:
                error_json = response.json()
                error_msg = error_json.get("detail", error_detail)
            except:
                error_msg = error_detail

            raise HTTPException(status_code=400, detail=f"Replicate API error: {error_msg}")

        result = response.json()

        # Get the prediction URL
        prediction_url = result.get("urls", {}).get("get")
        if not prediction_url:
            return {"output": [], "status": "error", "error": "No prediction URL returned"}

        # Poll for completion (simplified for now)
        import time
        max_attempts = 60
        for attempt in range(max_attempts):
            pred_response = requests.get(prediction_url, headers=headers)
            pred_response.raise_for_status()
            pred_data = pred_response.json()

            status = pred_data.get("status")

            if status == "succeeded":
                output = pred_data.get("output", [])
                if isinstance(output, str):
                    output = [output]

                # Save to database
                video_url = output[0] if output else ""
                prompt = request.input.get("prompt", "")

                try:
                    from database import save_generated_video
                    video_id = save_generated_video(
                        prompt=prompt,
                        video_url=video_url,
                        model_id=request.model_id,
                        parameters=request.input,
                        collection=request.collection
                    )
                    return {"output": output, "status": "succeeded", "video_id": video_id}
                except Exception as db_error:
                    print(f"Failed to save to database: {db_error}")
                    return {"output": output, "status": "succeeded"}
            elif status in ["failed", "canceled"]:
                error = pred_data.get("error", "Unknown error")
                return {"output": [], "status": status, "error": error}

            time.sleep(2)

        return {"output": [], "status": "timeout", "error": "Prediction timed out"}

    except HTTPException:
        # Re-raise HTTPException so FastAPI returns proper error status
        raise
    except Exception as e:
        print(f"Error running video model: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Internal error: {str(e)}")

@app.get("/api/videos")
async def api_list_videos(
    limit: int = Query(50, ge=1, le=100),
    offset: int = Query(0, ge=0),
    model_id: Optional[str] = Query(None),
    collection: Optional[str] = Query(None)
):
    """List generated videos from the database."""
    from database import list_videos
    videos = list_videos(limit=limit, offset=offset, model_id=model_id, collection=collection)
    return {"videos": videos}

@app.post("/api/genesis/render")
async def api_genesis_render(request: GenesisRenderRequest):
    """
    Render a scene using Genesis photorealistic ray-tracer with LLM semantic augmentation.

    This endpoint:
    1. Takes scene data with simple shapes and text descriptions
    2. Uses LLM to augment objects with photorealistic properties
    3. Renders using Genesis ray-tracer
    4. Returns path to rendered video
    """
    try:
        from genesis_renderer import create_renderer

        # Convert scene to dict with description field
        scene_data = request.scene.dict()

        # Ensure each object has a description field (can be empty)
        for obj_id, obj in scene_data.get("objects", {}).items():
            if "description" not in obj:
                obj["description"] = ""

        # Create renderer with specified quality
        renderer = create_renderer(
            quality=request.quality,
            output_dir="./backend/DATA/genesis_videos"
        )

        # Render the scene
        video_path = await renderer.render_scene(
            scene_data=scene_data,
            duration=request.duration,
            fps=request.fps,
            resolution=request.resolution,
            camera_config=request.camera_config,
            scene_context=request.scene_context
        )

        # Clean up
        renderer.cleanup()

        # Return video URL (relative to backend)
        video_url = video_path.replace("./backend/DATA/", "/data/")

        return {
            "success": True,
            "video_path": video_path,
            "video_url": video_url,
            "quality": request.quality,
            "duration": request.duration,
            "fps": request.fps
        }

    except ImportError as e:
        raise HTTPException(
            status_code=503,
            detail=f"Genesis not available. Install with: pip install genesis-world==0.3.7. Error: {str(e)}"
        )
    except Exception as e:
        print(f"Genesis rendering error: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"Genesis rendering failed: {str(e)}"
        )

# Serve rendered videos
from fastapi.staticfiles import StaticFiles
GENESIS_VIDEO_DIR = Path(__file__).parent / "DATA" / "genesis_videos"
if GENESIS_VIDEO_DIR.exists():
    app.mount("/data/genesis_videos", StaticFiles(directory=str(GENESIS_VIDEO_DIR)), name="genesis_videos")

# Serve frontend (must be last to catch all other routes)
@app.get("/{full_path:path}")
async def serve_frontend(full_path: str):
    """Serve the frontend application for all non-API routes."""
    # Check if we're in production mode with static files
    if STATIC_DIR.exists() and STATIC_DIR.is_dir():
        index_file = STATIC_DIR / "index.html"
        if index_file.exists():
            return FileResponse(str(index_file))

    # Fallback for development or if static files don't exist
    return {"message": "Frontend not built. Run 'npm run build' to build the frontend."}

if __name__ == "__main__":
    print("Starting Physics Simulator API server...")
    uvicorn.run(
        "main:app",
        host="127.0.0.1",
        port=8000,
        reload=True
    )
</file>

</files>
